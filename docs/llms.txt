# Confluent Cloud Flink SQL Documentation

> Comprehensive documentation for Apache Flink SQL dialect used in Confluent Cloud. This includes SQL syntax, functions, operators, and best practices for stream processing with Flink SQL in Confluent Cloud.

## Overview

This documentation covers:
- Flink SQL syntax and semantics
- Built-in functions and operators
- Stream processing concepts
- Confluent Cloud specific features
- Best practices and examples

## Core Documentation

- [Query Tableflow Tables with Confluent Cloud for Apache Flink | Confluent Documentation](https://docs.confluent.io/cloud/current/topics/tableflow/how-to-guides/query-engines/query-with-flink.html): Query Tableflow Tables with Flink in Confluent Cloud for Apache Flink®¶ Confluent Cloud for Apache Flink® supports snapshot queries that read data from a Tableflow-enabled topic at a specific point in...
- [Process Data with Confluent Cloud for Apache Flink | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/index.html): Process Data with Confluent Cloud for Apache Flink¶ Overview Get Started Overview Quick Start with Cloud Console Quick Start with SQL Shell in Confluent CLI Quick Start with Java Table API Quick Start...
- [Billing in Confluent Cloud for Apache Flink | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/concepts/flink-billing.html): Billing on Confluent Cloud for Apache Flink¶ Confluent Cloud for Apache Flink® is a serverless stream-processing platform with usage-based pricing, where you are charged only for the duration that you...
- [Comparing Apache Flink with Confluent Cloud for Apache Flink | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/concepts/comparison-with-apache-flink.html): Comparing Apache Flink with Confluent Cloud for Apache Flink¶ Confluent Cloud for Apache Flink® supports many of the capabilities of Apache Flink® and provides additional features. Also, Confluent Clo...
- [Private Networking with Confluent Cloud for Apache Flink | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/concepts/flink-private-networking.html): Private Networking with Confluent Cloud for Apache Flink¶ Confluent Cloud for Apache Flink® supports private networking on AWS, Azure, and Google Cloud. This feature enables Flink to securely read and...
- [Grant Role-Based Access for Flink SQL Statements in Confluent Cloud for Apache Flink | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/operate-and-deploy/flink-rbac.html): Grant Role-Based Access in Confluent Cloud for Apache Flink¶ When deploying Flink SQL statements in production, you must configure appropriate access controls for different types of users and workload...
- [Deploy a Flink SQL Statement in Confluent Cloud for Apache Flink | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/operate-and-deploy/deploy-flink-sql-statement.html): Deploy a Flink SQL Statement Using CI/CD and Confluent Cloud for Apache Flink¶ GitHub Actions is a powerful feature on GitHub that enables automating your software development workflows. If your sourc...
- [Generate an API key for Programmatic Access to Confluent Cloud for Apache Flink | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/operate-and-deploy/generate-api-key-for-flink.html): Generate an API Key for Access in Confluent Cloud for Apache Flink¶ To manage Flink workloads programmatically in Confluent Cloud for Apache Flink®, you need an API key that’s specific to Flink. You c...
- [Confluent Documentation | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/concepts/flink-compute-pools.html): Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A f...
- [Stream Processing Concepts in Confluent Cloud for Apache Flink | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/concepts/overview.html): Stream Processing Concepts in Confluent Cloud for Apache Flink¶ Apache Flink® SQL, a high-level API powered by Confluent Cloud for Apache Flink, offers a simple and easy way to leverage the power of s...
- [Confluent Documentation | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/concepts/flink-sql-api-keys.html): Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A f...
- [Confluent Documentation | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/concepts/flink-sql-statements.html): Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A f...
- [Confluent Documentation | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/concepts/flink-sql-user-defined-functions.html): Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A f...
- [Confluent Documentation | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/concepts/flink-kafka-table-mapping.html): Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A f...
- [Confluent Documentation | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/concepts/flink-sql-limits.html): Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A f...
- [Confluent Documentation | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/concepts/flink-sql-serialization.html): Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A f...
- [Confluent Documentation | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/concepts/flink-sql-security.html): Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A f...
- [Confluent Documentation | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/concepts/flink-sql-troubleshooting.html): Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A f...
- [Confluent Documentation | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/concepts/flink-sql-faq.html): Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A f...
- [Get Started with Confluent Cloud for Apache Flink | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/get-started/overview.html): Get Started with Confluent Cloud for Apache Flink¶ Welcome to Confluent Cloud for Apache Flink®. This section guides you through the steps to get your queries running using the Confluent Cloud Console...
- [Confluent Documentation | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/get-started/quick-start-sql.html): Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A f...
- [Java Table API Quick Start on Confluent Cloud for Apache Flink | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/get-started/quick-start-java-table-api.html): Java Table API Quick Start on Confluent Cloud for Apache Flink¶ Confluent Cloud for Apache Flink® supports programming applications with the Table API. Confluent provides a plugin for running applicat...
- [Python Table API Quick Start on Confluent Cloud for Apache Flink | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/get-started/quick-start-python-table-api.html): Python Table API Quick Start on Confluent Cloud for Apache Flink¶ Confluent Cloud for Apache Flink® supports programming applications with the Table API. Confluent provides a plugin for running applic...
- [Confluent Documentation | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/get-started/quick-start-confluent-cli.html): Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A f...
- [Confluent Documentation | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/get-started/quick-start-rest-api.html): Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A f...
- [Confluent Documentation | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/get-started/flink-sql-vs-code.html): Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A f...
- [Confluent Documentation | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/get-started/flink-sql-cloud-console.html): Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A f...
- [Confluent Documentation | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/get-started/flink-sql-confluent-cli.html): Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A f...
- [Confluent Documentation | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/get-started/flink-sql-rest-api.html): Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A f...
- [Confluent Documentation | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/get-started/flink-sql-notebooks.html): Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A f...
- [Confluent Documentation | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/get-started/flink-sql-terraform.html): Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A f...
- [Confluent Documentation | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/get-started/flink-sql-compute-pools.html): Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A f...
- [Confluent Documentation | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/get-started/flink-sql-statements.html): Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A f...
- [Confluent Documentation | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/get-started/flink-sql-user-defined-functions.html): Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A f...
- [Confluent Documentation | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/get-started/flink-sql-security.html): Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A f...
- [Confluent Documentation | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/get-started/flink-sql-private-networking.html): Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A f...
- [Confluent Documentation | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/get-started/flink-sql-rbac.html): Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A f...
- [Confluent Documentation | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/get-started/flink-sql-troubleshooting.html): Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A f...
- [Confluent Documentation | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/get-started/flink-sql-faq.html): Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A f...
- [Get Help with Confluent Cloud for Apache Flink | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/get-help.html): Get Help with Confluent Cloud for Apache Flink¶ You can request support in the Confluent Support Portal. You can access the portal directly, or you can navigate to it from the Confluent Cloud Console
- [How-to Guides for Developing Flink Applications on Confluent Cloud for Apache Flink | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/how-to-guides/overview.html): How-to Guides for Confluent Cloud for Apache Flink¶ Discover how Confluent Cloud for Apache Flink® can help you accomplish common processing tasks such as joins and aggregations. This section provides...
- [Confluent Documentation | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/how-to-guides/create-flink-sql-table.html): Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A f...
- [Confluent Documentation | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/how-to-guides/query-flink-sql-table.html): Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A f...
- [Confluent Documentation | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/how-to-guides/insert-into-flink-sql-table.html): Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A f...
- [Confluent Documentation | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/how-to-guides/windowing-aggregations.html): Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A f...
- [Confluent Documentation | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/how-to-guides/joins.html): Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A f...
- [Confluent Documentation | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/how-to-guides/time-processing.html): Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A f...
- [Confluent Documentation | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/how-to-guides/data-types.html): Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A f...
- [Confluent Documentation | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/how-to-guides/upsert-into-from-select-statement.html): Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A f...
- [Operate and Deploy Flink SQL Statements with Confluent Cloud for Apache Flink | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/operate-and-deploy/overview.html): Operate and Deploy Flink Statements with Confluent Cloud for Apache Flink¶ Confluent provides tools for operating Confluent Cloud for Apache Flink® in the Cloud Console, the Confluent CLI, the Conflue...
- [Confluent Documentation | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/operate-and-deploy/flink-sql-compute-pools.html): Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A f...
- [Confluent Documentation | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/operate-and-deploy/flink-sql-statements.html): Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A f...
- [Confluent Documentation | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/operate-and-deploy/flink-sql-user-defined-functions.html): Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A f...
- [Confluent Documentation | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/operate-and-deploy/flink-sql-security.html): Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A f...
- [Confluent Documentation | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/operate-and-deploy/flink-sql-private-networking.html): Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A f...
- [Confluent Documentation | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/operate-and-deploy/flink-sql-rbac.html): Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A f...
- [Confluent Documentation | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/operate-and-deploy/flink-sql-troubleshooting.html): Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A f...
- [Confluent Documentation | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/operate-and-deploy/flink-sql-faq.html): Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A f...
- [Flink SQL and Table API Reference in Confluent Cloud for Apache Flink | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/reference/overview.html): Flink SQL and Table API Reference in Confluent Cloud for Apache Flink¶ This section describes the SQL language support in Confluent Cloud for Apache Flink®, including Data Definition Language (DDL) st...
- [SQL Statements in Confluent Cloud for Apache Flink | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/reference/statements/overview.html): DDL Statements in Confluent Cloud for Apache Flink¶ In Confluent Cloud for Apache Flink®, a statement is a high-level resource that’s created when you enter a SQL query. Data Definition Language (DDL)...
- [SQL Functions in Confluent Cloud for Apache Flink | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/reference/functions/overview.html): Flink SQL Functions in Confluent Cloud for Apache Flink¶ Confluent Cloud for Apache Flink® enables you to do data transformations and other operations with the following built-in functions. Aggregate
- [Confluent Documentation | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/reference/data-types.html): Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A f...
- [Flink SQL Syntax in Confluent Cloud for Apache Flink | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/reference/sql-syntax.html): Flink SQL Syntax in Confluent Cloud for Apache Flink¶ SQL is a domain-specific language for managing and manipulating data. It’s used primarily to work with structured data, where the types and relati...
- [Predefined RBAC roles in Confluent Cloud | Confluent Documentation](https://docs.confluent.io/cloud/current/security/access-control/rbac/predefined-rbac-roles.html#flinkdeveloper-role): Predefined RBAC Roles in Confluent Cloud¶ You can use Confluent Cloud RBAC roles to control access to an organization, its environments, the clusters within each environment, and the Kafka resources o...
- [Predefined RBAC roles in Confluent Cloud | Confluent Documentation](https://docs.confluent.io/cloud/current/security/access-control/rbac/predefined-rbac-roles.html#flinkadmin-role): Predefined RBAC Roles in Confluent Cloud¶ You can use Confluent Cloud RBAC roles to control access to an organization, its environments, the clusters within each environment, and the Kafka resources o...
- [Grant Role-Based Access for Flink SQL Statements in Confluent Cloud for Apache Flink | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/operate-and-deploy/flink-rbac.html#flink-rbac-grant-sa-and-user-permission-for-sql-statements): Grant Role-Based Access in Confluent Cloud for Apache Flink¶ When deploying Flink SQL statements in production, you must configure appropriate access controls for different types of users and workload...
- [Private Networking with Confluent Cloud for Apache Flink | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/concepts/flink-private-networking.html#flink-sql-private-networking): Private Networking with Confluent Cloud for Apache Flink¶ Confluent Cloud for Apache Flink® supports private networking on AWS, Azure, and Google Cloud. This feature enables Flink to securely read and...
- [Flink SQL Syntax in Confluent Cloud for Apache Flink | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/reference/sql-syntax.html#flink-sql-syntax): Flink SQL Syntax in Confluent Cloud for Apache Flink¶ SQL is a domain-specific language for managing and manipulating data. It’s used primarily to work with structured data, where the types and relati...
- [Java Table API Quick Start on Confluent Cloud for Apache Flink | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/get-started/quick-start-java-table-api.html#flink-java-table-api-quick-start): Java Table API Quick Start on Confluent Cloud for Apache Flink¶ Confluent Cloud for Apache Flink® supports programming applications with the Table API. Confluent provides a plugin for running applicat...
- [Python Table API Quick Start on Confluent Cloud for Apache Flink | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/get-started/quick-start-python-table-api.html#flink-python-table-api-quick-start): Python Table API Quick Start on Confluent Cloud for Apache Flink¶ Confluent Cloud for Apache Flink® supports programming applications with the Table API. Confluent provides a plugin for running applic...
- [Get Help with Confluent Cloud for Apache Flink | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/get-help.html#ccloud-flink-help): Get Help with Confluent Cloud for Apache Flink¶ You can request support in the Confluent Support Portal. You can access the portal directly, or you can navigate to it from the Confluent Cloud Console
- [Get Started with Confluent Cloud for Apache Flink | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/get-started/overview.html#flink-sql-get-started): Get Started with Confluent Cloud for Apache Flink¶ Welcome to Confluent Cloud for Apache Flink®. This section guides you through the steps to get your queries running using the Confluent Cloud Console...
- [Stream Processing Concepts in Confluent Cloud for Apache Flink | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/concepts/overview.html#flink-sql-stream-processing-concepts): Stream Processing Concepts in Confluent Cloud for Apache Flink¶ Apache Flink® SQL, a high-level API powered by Confluent Cloud for Apache Flink, offers a simple and easy way to leverage the power of s...
- [How-to Guides for Developing Flink Applications on Confluent Cloud for Apache Flink | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/how-to-guides/overview.html#flink-sql-how-to-guides): How-to Guides for Confluent Cloud for Apache Flink¶ Discover how Confluent Cloud for Apache Flink® can help you accomplish common processing tasks such as joins and aggregations. This section provides...
- [Operate and Deploy Flink SQL Statements with Confluent Cloud for Apache Flink | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/operate-and-deploy/overview.html#flink-sql-operate-and-deploy): Operate and Deploy Flink Statements with Confluent Cloud for Apache Flink¶ Confluent provides tools for operating Confluent Cloud for Apache Flink® in the Cloud Console, the Confluent CLI, the Conflue...
- [Flink SQL and Table API Reference in Confluent Cloud for Apache Flink | Confluent Documentation](https://docs.confluent.io/cloud/current/flink/reference/overview.html#flink-sql-reference): Flink SQL and Table API Reference in Confluent Cloud for Apache Flink¶ This section describes the SQL language support in Confluent Cloud for Apache Flink®, including Data Definition Language (DDL) st...
- [Introducing Confluent Cloud for Apache Flink](https://www.confluent.io/blog/introducing-flink-on-confluent-cloud/): Join us at Current New Orleans! Save $500 with early bird pricing until August 15 | Register NowLogin Contact UsIn the first three parts of our Inside Flink blog series, we discussed the benefits of s...
- [Getting Started with Flink SQL: In-Depth Guide](https://www.confluent.io/blog/getting-started-with-apache-flink-sql/): Join us at Current New Orleans! Save $500 with early bird pricing until August 15 | Register NowLogin Contact UsIn the first two parts of our Inside Flink blog series, we explored the benefits of stre...
- [Part 1: How to Use Flink SQL, Streamlit, and Kafka](https://www.confluent.io/blog/how-to-use-flinksql-streamlit-kafka-part-1/): Join us at Current New Orleans! Save $500 with early bird pricing until August 15 | Register NowLogin Contact UsMarket data analytics has always been a classic use case for Apache Kafka®. However, new...
- [Part 2: How to Use Flink SQL, Streamlit, and Kafka](https://www.confluent.io/blog/how-use-flinksql-streamlit-kafka-part-2/): Join us at Current New Orleans! Save $500 with early bird pricing until August 15 | Register NowLogin Contact UsIn part one of this series, we walked through how to use Streamlit, Apache Kafka®, and A...
- [Apache Flink® SQL](https://developer.confluent.io/courses/flink-sql/overview/): ‹ Back to coursescourse: Apache Flink® SQLApache Flink® SQLApache Flink® SQL This is a course about Flink SQL, which is part of the Apache Flink project. Exploring what Flink SQL can do is a great way...
- [Apache Flink - A Complete Introduction](https://developer.confluent.io/courses/apache-flink/intro/): ‹ Back to coursesView Transcriptcourse: Apache Flink® 101Introduction3 minApache Flink® 101 About This Course This course is an introduction to Apache Flink, focusing on its core concepts and architec...
- [Apache Flink with Java - An Introduction](https://developer.confluent.io/courses/flink-java/overview/): ‹ Back to coursesView Transcriptcourse: Building Apache Flink® Applications in JavaApache Flink with Java - An Introduction2 minBuilding Apache Flink Applications in Java About This Course Peel away t...
- [Exercise: Connecting the Flink Table API to Confluent Cloud](https://developer.confluent.io/courses/flink-table-api-java/exercise-connecting-to-confluent-cloud/): ‹ Back to coursescourse: Apache Flink® Table API: Processing Data Streams In JavaExercise: Connecting the Flink Table API to Confluent Cloud30 minConnecting the Apache Flink Table API to Confluent Clo...
- [Apache Flink: Stream Processing and Analytics | Confluent](https://www.confluent.io/product/flink/): Join us at Current New Orleans! Save $500 with early bird pricing until August 15 | Register NowLogin Contact UsSimple, Serverless Stream ProcessingEasily build high-quality, reusable data streams wit...

## Full Documentation Content

### Query Tableflow Tables with Confluent Cloud for Apache Flink | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/topics/tableflow/how-to-guides/query-engines/query-with-flink.html

Query Tableflow Tables with Flink in Confluent Cloud for Apache Flink®¶ Confluent Cloud for Apache Flink® supports snapshot queries that read data from a Tableflow-enabled topic at a specific point in time. Querying a Tableflow-enabled topic is similar to querying a Flink topic. If Tableflow is enabled on a topic with Confluent Managed Storage, the query reads from both Kafka and Parquet. If Tableflow is enabled on a topic with custom storage, the query reads from your S3 bucket. This guide shows how to run a snapshot query on a Tableflow-enabled topic. Note Snapshot query is an Early Access Program feature in Confluent Cloud for Apache Flink. An Early Access feature is a component of Confluent Cloud introduced to gain feedback. This feature should be used only for evaluation and non-production testing purposes or to provide feedback to Confluent, particularly as it becomes more widely available in follow-on preview editions. Early Access Program features are intended for evaluation use in development and testing environments only, and not for production use. Early Access Program features are provided: (a) without support; (b) “AS IS”; and (c) without indemnification, warranty, or condition of any kind. No service level commitment will apply to Early Access Program features. Early Access Program features are considered to be a Proof of Concept as defined in the Confluent Cloud Terms of Service. Confluent may discontinue providing preview releases of the Early Access Program features at any time in Confluent’s sole discretion. Prerequisites¶ Access to Confluent Cloud. The OrganizationAdmin, EnvironmentAdmin, or FlinkAdmin role for creating compute pools, or the FlinkDeveloper role if you already have a compute pool. If you don’t have the appropriate role, contact your OrganizationAdmin or EnvironmentAdmin. For more information, see Grant Role-Based Access in Confluent Cloud for Apache Flink. A provisioned Flink compute pool. Step 1: Enable Tableflow on your topic¶ If you want to try querying a table with mock data, complete the steps in Run a Snapshot Query, then proceed to the next step. If you want to query a table with mock data or data from your Kafka topic, and you want to use Confluent Managed Storage, complete the following steps, then proceed to Step 2: Run a snapshot query with Tableflow. In Confluent Cloud Console, navigate to your cluster. In the navigation menu, click Topics. In the topics list, find your topic and click it to open the details page. Click Enable Tableflow. In the Enable Tableflow dialog, select Iceberg and click Use Confluent storage. The topic status updates to Tableflow Syncing. If you want to query a table with data from your Kafka topic, and you want to use custom storage, complete steps 1-4 in Tableflow Quick Start Using Your Storage and AWS Glue and proceed to Step 2: Run a snapshot query with Tableflow. Step 2: Run a snapshot query with Tableflow¶ Once Tableflow is enabled on your topic, you can run a snapshot query on the table by using the same statements that you use for Flink tables. In a Flink workspace or the Flink SQL shell, prepend your query with the following SET statement: SET 'sql.snapshot.mode' = 'now'; Also, in the Flink workspace, you can change the Mode dropdown setting to Snapshot before running your query. For more information, see Run a Snapshot Query. Related content¶ Query with AWS Query with Snowflake Query with Trino Stream Processing with Confluent Cloud for Apache Flink Note This website includes content developed at the Apache Software Foundation under the terms of the Apache License v2.

#### Code Examples

```sql
SET 'sql.snapshot.mode' = 'now';
```

---

### Process Data with Confluent Cloud for Apache Flink | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/index.html

Process Data with Confluent Cloud for Apache Flink¶ Overview Get Started Overview Quick Start with Cloud Console Quick Start with SQL Shell in Confluent CLI Quick Start with Java Table API Quick Start with Python Table API Concepts Overview Autopilot Batch and Stream Processing Billing Comparison with Apache Flink Compute Pools Delivery Guarantees and Latency Determinism Private Networking Schema and Statement Evolution Snapshot Queries Statements Statement CFU Metrics Tables and Topics Time and Watermarks User-defined Functions How-To Guides Overview Aggregate a Stream in a Tumbling Window Combine Streams and Track Most Recent Records Compare Current and Previous Values in a Stream Convert the Serialization Format of a Topic Create a UDF Deduplicate Rows in a Table Enable UDF Logging Handle Multiple Event Types Mask Fields in a Table Process Schemaless Events Resolve Common SQL Query Problems Run a Snapshot Query Scan and Summarize Tables Transform a Topic View Time Series Data Operate and Deploy Overview Manage Compute Pools Monitor and Manage Statements Grant Role-Based Access Deploy a Statement with CI/CD Generate a Flink API Key REST API Move SQL Statements to Production Enable Private Networking Flink Reference Overview SQL Syntax DDL Statements DML Statements Functions Data Types Data Type Mappings Time Zone Keywords Information Schema Example Streams Supported Cloud Regions SQL Examples Table API CLI Reference Get Help

---

### Billing in Confluent Cloud for Apache Flink | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/concepts/flink-billing.html

Billing on Confluent Cloud for Apache Flink¶ Confluent Cloud for Apache Flink® is a serverless stream-processing platform with usage-based pricing, where you are charged only for the duration that your queries are running. You configure Flink by creating a Flink compute pool. You are charged for the CFUs consumed when you run statements within a compute pool. While the compute pool itself scales elastically to provide the necessary resources, your cost is determined by the actual CFUs used per minute, not the provisioned size of the pool. You can configure the maximum size of a compute pool to limit your spending. CFUs¶ A CFU is a logical unit of processing power that is used to measure the resources consumed by Confluent Cloud for Apache Flink. Each Flink statement consumes a minimum of 1 CFU-minute but may consume more depending on the needs of the workload. CFU billing¶ You are billed for the total number of CFUs consumed inside a compute pool per minute. Usage is stated in hours in order to apply hourly pricing to minute-by-minute use. For example, 30 CFU-minutes is 0.5 CFU-hours. CFU pricing$0.21/CFU-hour, calculated by the minute ($0.0035/CFU-minute) Prices vary by cloud region. Networking fees¶ Using Flink to read and write data from Apache Kafka® doesn’t add any new Flink-specific networking fees, but you’re still responsible for the Confluent Cloud networking rates for data read from and written to your Kafka clusters. These are existing Kafka costs, not new charges created by Flink. Cost Management¶ You can’t define the number of CFUs required for individual statements. CFUs are counted by Confluent Cloud for Apache Flink. You can configure the maximum size of a compute pool to limit your spending by setting a parameter named MAX_CFU, which sets an upper limit on the hourly spend on the compute pool. If the size of the workload in a pool exceeds MAX_CFU, new statements are rejected. Existing workloads continue running but may experience increased latency. Note You can increase the MAX_CFU value after you create a compute pool, but currently, decreasing the initial MAX_CFU value is not supported. For more information on CFU prices, see Confluent Cloud Pricing. Pricing examples¶ Data streaming is a real-time business, and data streams oscillate on a minute-by-minute basis, creating peaks and troughs of utilization. You don’t want to allocate and overpay for processing capacity that you aren’t using. With Confluent Cloud for Apache Flink, you pay only for the processing power that you actually use. The following examples provide additional detail on how pricing works when processing streams using Confluent Cloud for Apache Flink. Data exploration and discovery¶ Most SQL queries are short-running, interactive queries that help software and data engineers understand the streams they have access to. Querying the streams directly is an important and necessary step in the iterative development of apps and pipelines. In the following example, one user executes five different queries. Unlike other Flink offerings, Confluent Cloud for Apache Flink’s serverless architecture charges you only for the five minutes when these queries are executing, with all users able to share the resources of a single compute pool. It doesn’t matter if these queries are executed by the same person, by five different people at the same time or, as shown below, at different points in the hour. Example pricing calculation Number of queries executed = 5 Total CFU-minutes consumed = 5 Total charge: 5 CFU-minutes x $0.0035/CFU-minute = $0.0175 Note: The charge appears on the invoice as “0.083 CFU-hours x $0.21/CFU-hour”. Many data streaming apps and statements¶ Data streaming architectures are composed of many applications, each with their own workload requirements. An architecture can be a mix of interactive, terminating statements and continuous, streaming statements. Confluent Cloud for Apache Flink automatically scales the processing power of the Flink compute pool up and down in real-time to ensure your apps have the processing power they need, while charging only for the minutes needed. In the following example, five streaming statements are running in a single compute pool. The data streams are oscillating, and you can see spikes of utilization for short periods within the hour. Each statement attracts a minimum price of 1 CFU-minute ($0.0035 in this example) and is automatically scaled up and down as needed on a per-minute basis. Statement CFU-minutes Statement Type Q1 5 Interactive Q2 60 Streaming Q3 110 Streaming Q4 10 Interactive Q5 124 Streaming Total 309 Example pricing calculation Number of statements executing = 5 Total CFU-minutes consumed = 309 Total charge: 309 CFU-minutes x $0.0035/CFU-minute = $1.0815 Note: The charge appears on the invoice as “5.15 CFU-hours x $0.21/CFU-hour”. Related content¶ Compute Pools Confluent Cloud Pricing

---

### Comparing Apache Flink with Confluent Cloud for Apache Flink | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/concepts/comparison-with-apache-flink.html

Comparing Apache Flink with Confluent Cloud for Apache Flink¶ Confluent Cloud for Apache Flink® supports many of the capabilities of Apache Flink® and provides additional features. Also, Confluent Cloud for Apache Flink has some different behaviors and limitations relative to Apache Flink. This topic describes the key differences between Confluent Cloud for Apache Flink and Apache Flink. Additional features¶ The following list shows features provided by Confluent Cloud for Apache Flink that go beyond what Apache Flink offers. Auto-inference of environments, clusters, topics, and schemas¶ In Apache Flink, you must define and configure your tables and their schemas, including authentication and authorization to Apache Kafka®. Confluent Cloud for Apache Flink maps environments, clusters, topics, and schemas automatically from Confluent Cloud to the corresponding Apache Flink concepts of catalogs, databases, tables, and table schemas. Autoscaling¶ Autopilot scales up and scales down the compute resources that SQL statements use in Confluent Cloud. The autoscaling process is based on parallelism, which is the number of parallel operations that occur when the SQL statement is running. A SQL statement performs at its best when it has the optimal resources for its required parallelism. Default system column implementation¶ Confluent Cloud for Apache Flink has a default implementation for a system column named $rowtime. This column is mapped to the Kafka record timestamp, which can be either LogAppendTime or CreateTime. Default watermark strategy¶ Flink requires a watermark strategy for a variety of features, such as windowing and temporal joins. Confluent Cloud for Apache Flink has a default watermark strategy applied on all tables/topics, which is based on the $rowtime system column. Apache Flink requires you to define a watermark strategy manually. For more information, see Event Time and Watermarks. Because the default strategy is defined for general usage, there are cases that require a custom strategy, for example, when delays in record arrival of longer than 7 days occur in your streams. You can override the default strategy with a custom strategy by using the ALTER TABLE statement. Schema Registry support for JSON_SR and Protobuf¶ Confluent Cloud for Apache Flink has support for Schema Registry formats AVRO, JSON_SR, and Protobuf, while Apache Flink currently supports only Schema Registry AVRO. INFORMATION_SCHEMA support¶ Confluent Cloud for Apache Flink has an implementation for IMPLEMENTATION_SCHEMA, which is a system view that provides insights on catalogs, databases, tables, and schemas. This doesn’t exist in Apache Flink. Behavioral differences¶ The following list shows differences in behavior between Confluent Cloud for Apache Flink and Apache Flink. Configuration options¶ Apache Flink supports various optimization configuration options on different levels, like Execution Options, Optimizer Options, Table Options, and SQL Client Options. Confluent Cloud for Apache Flink supports only the necessary subset of these options. Some of these options have different names in Confluent Cloud for Apache Flink, as shown in the following table. Confluent Cloud for Apache Flink Apache Flink client.results-timeout table.exec.async-lookup.timeout client.statement-name – sql.current-catalog table.builtin-catalog-name sql.current-database table.builtin-database-name sql.dry-run – sql.inline-result – sql.local-time-zone table.local-time-zone sql.state-ttl table.exec.state.ttl sql.tables.scan.bounded.timestamp-millis scan.bounded.timestamp-millis sql.tables.scan.bounded.mode scan.bounded.mode sql.tables.scan.idle-timeout table.exec.source.idle-timeout sql.tables.scan.startup.timestamp-millis scan.startup.timestamp-millis sql.tables.scan.startup.mode scan.startup.mode sql.tables.scan.watermark-alignment.max-allowed-drift scan.watermark.alignment.max-drift sql.tables.scan.source-operator-parallelism – CREATE statements provision underlying resources¶ When you run a CREATE TABLE statement in Confluent Cloud for Apache Flink, it creates the underlying Kafka topic and a Schema Registry schema in Confluent Cloud. In Apache Flink, a CREATE TABLE statement only registers the object in the Apache Flink catalog and doesn’t create an underlying resource. This also means that temporary tables are not supported in Confluent Cloud for Apache Flink, while they are in Apache Flink. One Kafka connector and only Confluent Cloud support¶ Apache Flink contains a Kafka connector and an Upsert-Kafka connector, which, combined with the format, defines whether the source/sink is treated as an append-stream or update stream. Confluent Cloud for Apache Flink has only one Kafka connector and determines if the source/sink is an append-stream or update stream by examining the changelog.mode connector option. Confluent Cloud for Apache Flink only supports reading from and writing to Kafka topics that are located on Confluent Cloud. Apache Flink supports other connectors, like Kinesis, Pulsar, JDBC, etc., and also other Kafka environments, like on-premises and different cloud service providers. Limitations¶ The following list shows limitations of Confluent Cloud for Apache Flink compared with Apache Flink. Windowing functions syntax¶ Confluent Cloud for Apache Flink supports the TUMBLE, HOP, SESSION, and CUMULATE windowing functions only by using so-called Table-Valued Functions syntax. Apache Flink supports these windowing functions also by using the outdated Group Window Aggregations functions. Unsupported statements and features¶ Confluent Cloud for Apache Flink does not support the following statements and features. ANALYZE statements CALL statements CATALOG commands other than SHOW (No CREATE/DROP/ALTER) DATABASE command other than SHOW (No CREATE/DROP/ALTER) DELETE statements DROP CATALOG and DROP DATABASE JAR statements LOAD / UNLOAD statements TRUNCATE statements UPDATE statements Processing time operations, like PROCTIME(), TUMBLE_PROCTIME, HOP_PROCTIME, SESSION_PROCTIME, and CUMULATE_PROCTIME Limited support for ALTER¶ Confluent Cloud for Apache Flink has limited support for ALTER TABLE compared with Apache Flink. In Confluent Cloud for Apache Flink, you can use ALTER TABLE only to change the watermark strategy, add a metadata column, or change a parameter value. Related content¶ Flink SQL Autopilot Compute Pools DDL Statements in Confluent Cloud for Apache Flink Note This website includes content developed at the Apache Software Foundation under the terms of the Apache License v2.

#### Code Examples

```sql
LogAppendTime
```

```sql
TUMBLE_PROCTIME
```

```sql
HOP_PROCTIME
```

```sql
SESSION_PROCTIME
```

```sql
CUMULATE_PROCTIME
```

---

### Private Networking with Confluent Cloud for Apache Flink | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/concepts/flink-private-networking.html

Private Networking with Confluent Cloud for Apache Flink¶ Confluent Cloud for Apache Flink® supports private networking on AWS, Azure, and Google Cloud. This feature enables Flink to securely read and write data stored in Confluent Cloud clusters that are located in private networking, with no data flowing to the public internet. With private networking, you can use Flink and Apache Kafka® together for stream processing in Confluent Cloud, even in the most stringent regulatory environments. Confluent Cloud for Apache Flink supports private networking for AWS and Azure in all regions where Flink is supported. Google Cloud supports private networking in most regions where Flink is supported. For the regions that support Flink private networking, see Supported Cloud Regions. Connectivity options¶ There are a number of ways to access Flink with private networking. In all cases, they allow access to all types of private clusters (Enterprise, Dedicated, Freight), with all types of connectivity (VNET/VPC, Peering, Transit Gateway, PNI). PrivateLink Attachment: Works with any type of cluster and is available on AWS and Azure. Confluent Cloud network (CCN): Available on AWS only. If you already have an existing Confluent Cloud network, this is the easiest way to get started, but it works only on AWS when a Confluent Cloud network is already configured. If you need to create a new Confluent Cloud network, follow the steps in Create Confluent Cloud Network on AWS. PrivateLink Attachment¶ A PrivateLink Attachment is a resource that enables you to connect to Confluent serverless products, like Enterprise clusters and Flink. For Flink, the new PrivateLink Attachment is used only to establish a connection between your clients (like Cloud Console UI, Confluent CLI, Terraform, apps using the Confluent REST API) and Flink. Flink-to-Kafka is routed internally within Confluent Cloud. As a result, this PLATT is used only for submitting statements and fetching results from the client. For Dedicated clusters, regardless of the Kafka cluster connection type (Private Link, Peering, or Transit Gateway), Flink requires that you define a PLATT in the same region of the cluster, even if a private link exists for the Dedicated cluster. For Enterprise clusters, you can reuse the same PLATT used by your Enterprise clusters. By creating a PrivateLink Attachment to a Confluent Cloud environment in a region, you are enabling Flink statements created in that environment to securely access data in any of the Flink clusters in the same region, regardless of their environment. Access to the Flink clusters is governed by RBAC. Also, a PrivateLink Attachment enables your data-movement components in Confluent Cloud, including Flink statements and cluster links, to move data between all of the private networks in the organization, including the Confluent Cloud networks associated with any Dedicated Kafka clusters. For more information, see Enable private networking with PrivateLink Attachment. Confluent Cloud network (CCN)¶ If you have an existing Confluent Cloud network, this is the easiest way to get set up, but it works only on AWS when a Confluent Cloud network is configured already and at least one Kafka Dedicated cluster exists in the environment and region where you need to use Flink. For existing Kafka Dedicated users, this option requires no effort to configure, if everything is already configured for Kafka. If a reverse proxy is not set up, this requires setup for Flink or the use of a VM within the VPC to access Flink. To create a Confluent Cloud network, follow the steps in Create Confluent Cloud Network on AWS. For more information, see Enable private networking with Confluent Cloud Network. Protect resources with IP Filtering¶ With IP Filtering, you can enhance security for your Flink resources (statements and workspaces) based on trusted source IP addresses. IP Filtering is an authorization feature that allows you to create IP filters for your Confluent Cloud organization that permit inbound requests only from specified IP groups. All incoming API requests that originate from IP addresses not included in your IP filters are denied. For Flink resources, you can implement the following access controls: No public networks: Select the predefined No Public Networks group (ipg-none) to block all public network access, allowing access only from private network connections. This IP group cannot be combined with other IP groups in the same filter. Public: The default option if no IP filters are set. Flink statements and workspaces are accessible from all source networks when connecting over the public internet. While SQL queries are visible, private cluster data remains protected, and you can’t issue statements accessing private clusters. Public with restricted IP list: Create custom IP groups containing specific CIDR blocks to allow access only from trusted networks while maintaining the same protection for private cluster data. IP Filtering applies only to requests made over public networks and doesn’t limit requests made over private network connections. When creating IP filters for Flink resources, select the Flink operation group to control access to all operations related to Apache Flink data. For more information on setting IP filters, see IP Filtering and Manage IP Filters. The IP Filtering feature replaces the previous distinction between public and private Flink statements and workspaces. Administrators can modify access controls at any time by updating IP filters. For data protection in Kafka clusters, access is governed by network settings of the cluster: You can always read public data regardless of the connectivity, whether public or private. To read or write data in a private cluster, the cluster must use private connectivity. To prevent data exfiltration, you can’t write to public clusters when using private connectivity. Available endpoints for an environment and region¶ The following section shows the endpoints that are available for connecting to Flink. While the public endpoint is always present, others may require some effort to be created. Public endpoint PrivateLink Attachment Private connectivity through Confluent Cloud Network The following table shows how to get the endpoint value by using different Confluent interfaces. Interface Location Endpoint Cloud Console Flink Endpoints page Full FQDN shown for each network connection Confluent CLI confluent flink endpoint list Full FQDN shown for each network connection Network UI/API/CLI Network management details page in Environment overview GET /network/ confluent network describe Read the endpoint_suffix attribute, for example, <service-identifier>-abc1de.us-east-1.aws.glb.confluent.cloud Replace <service-identifier> with the relevant value, for example, flink for Flink or flinkpls for Language Service. Assign in interface (UI/CLI/Terraform) The following table shows the endpoint patterns for different DNS and cluster type combinations. Networking DNS Cluster Type Endpoints PrivateLink Private Enterprise (PrivateLink Attachment) flink.$region.$cloud.private.confluent.cloud flinkpls.$region.$cloud.private.confluent.cloud Dedicated flink.dom$id.$region.$cloud.private.confluent.cloud flinkpls.dom$id.$region.$cloud.private.confluent.cloud Public Dedicated flink-$nid.$region.$cloud.glb.confluent.cloud flinkpls-$nid.$region.$cloud.glb.confluent.cloud VPC Peering / Transit Gateway w/ /16 CIDR Public Dedicated flink-$nid.$region.$cloud.confluent.cloud flinkpls-$nid.$region.$cloud.confluent.cloud VPC Peering / Transit Gateway w/ /27 CIDRs Public Dedicated flink-$nid.$region.$cloud.glb.confluent.cloud flinkpls-$nid.$region.$cloud.glb.confluent.cloud Public endpoint¶ Source: Always present. Considerations: Can’t access Kafka private data. Kafka data access and scope: Can access public cluster data (read/write) in cloud region for this organization. Access to Flink statement and workspace: Configurable with IP Filtering. Endpoints: flink.<region>.<cloud>.confluent.cloud, for example: flink.us-east-2.aws.confluent.cloud. PrivateLink Attachment¶ Source: Must create a Private Link Attachment for the environment/region. Considerations: A single VPC can’t have private link connections to multiple Confluent Cloud environments. Available on AWS and Azure. Can access private cluster data (read/write) in Enterprise, Dedicated or Freight clusters for the cloud region for the organization of the endpoint. Can access public cluster data (read only). Access all Flink resources in the same environment and region of the endpoint Endpoints: flink.<region>.<cloud>.private.confluent.cloud, for example: flink.us-east-2.aws.private.confluent.cloud Private connectivity through Confluent Cloud Network¶ Source: Created with Kafka Dedicated clusters. Considerations: Easiest way to use Flink when the network is created already for Dedicated clusters. Available on AWS only. Can access private cluster data (read/write) in Enterprise, Dedicated or Freight clusters for the organization of the region. Can access public cluster data (read only). Access all Flink resources in the same environment and region of the endpoint To find the endpoints from the Cloud Console or Confluent CLI, see Available endpoints for an environment and region. Access private networking with the Confluent CLI¶ Run the confluent flink region --cloud <cloud-provider> --region <region> command to select a cloud provider and region. Run the confluent flink endpoint list command to list all endpoints, both public and private. Run the confluent flink endpoint use to select an endpoint. In addition to the main Flink endpoint listed here, you must have access to flinkpls.<network>.<region>.<cloud>.private.confluent.cloud (for private DNS resolution) or flinkpls-<network>.<region>.<cloud>.private.confluent.cloud (for public DNS resolution) to access the language service for autocompletion in the Flink SQL shell. In the case of public DNS resolution, routing is done transparently, but if you use private DNS resolution, you must make sure to route this endpoint from your client. For more information, see private DNS resolution. Access private networking with the Cloud Console¶ By default, public networking is used, which won’t work if IP Filtering is set, and/or the cluster is private. You can set defaults for each cloud region in an environment. For this, use the Flink Endpoints page. The default is per-user. When a default is set, it is used for all pages that access Flink, for example, the statement list, workspace list, and workspaces. If no default is set, the public endpoint is used. Related content¶ Video: Flink Queries on Dedicated PrivateLink Kafka Clusters in Confluent Cloud Use Confluent Cloud with Private Networking Flink Compute Pools Note This website includes content developed at the Apache Software Foundation under the terms of the Apache License v2.

#### Code Examples

```sql
confluent flink endpoint list
```

```sql
confluent network describe
```

```sql
endpoint_suffix
```

```sql
<service-identifier>-abc1de.us-east-1.aws.glb.confluent.cloud
```

```sql
<service-identifier>
```

```sql
flink.$region.$cloud.private.confluent.cloud
```

```sql
flinkpls.$region.$cloud.private.confluent.cloud
```

```sql
flink.dom$id.$region.$cloud.private.confluent.cloud
```

```sql
flinkpls.dom$id.$region.$cloud.private.confluent.cloud
```

```sql
flink-$nid.$region.$cloud.glb.confluent.cloud
```

```sql
flinkpls-$nid.$region.$cloud.glb.confluent.cloud
```

```sql
flink-$nid.$region.$cloud.confluent.cloud
```

```sql
flinkpls-$nid.$region.$cloud.confluent.cloud
```

```sql
flink-$nid.$region.$cloud.glb.confluent.cloud
```

```sql
flinkpls-$nid.$region.$cloud.glb.confluent.cloud
```

```sql
flink.<region>.<cloud>.confluent.cloud
```

```sql
flink.us-east-2.aws.confluent.cloud
```

```sql
flink.<region>.<cloud>.private.confluent.cloud
```

```sql
flink.us-east-2.aws.private.confluent.cloud
```

```sql
confluent flink region --cloud <cloud-provider> --region <region>
```

```sql
confluent flink endpoint list
```

```sql
confluent flink endpoint use
```

```sql
flinkpls.<network>.<region>.<cloud>.private.confluent.cloud
```

```sql
flinkpls-<network>.<region>.<cloud>.private.confluent.cloud
```

---

### Grant Role-Based Access for Flink SQL Statements in Confluent Cloud for Apache Flink | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/operate-and-deploy/flink-rbac.html

Grant Role-Based Access in Confluent Cloud for Apache Flink¶ When deploying Flink SQL statements in production, you must configure appropriate access controls for different types of users and workloads. Confluent Cloud for Apache Flink® supports Role-based Access Control (RBAC) with these roles: FlinkAdmin: Full access to Flink resources and compute pool management FlinkDeveloper: Limited access for running statements but not managing infrastructure Assigner: Enables delegation of statement execution to service accounts Layered permission model: Flink permissions follow a layered approach. Start with base permissions required for all Flink operations, then add additional layers based on what users need to accomplish. Operational considerations: Use service accounts for production workloads and apply least-privilege principles by granting only the permissions needed for each use case. For complete role definitions, see Predefined RBAC Roles. Permission layers Common user scenarios Production best practices Access for UDF Logging Audit log events Reference Permission layers¶ Flink permissions follow a layered approach, in which each layer builds upon the previous one. This design enables you to grant only the permissions needed for each use case, following the principle of least privilege. These are the permission layers: Base/required layer: Fundamental permissions needed for all Flink operations Data access layer: Read and write access to specific tables and topics Table management layer: Create, alter, and delete tables Administrative layer: Manage compute pools and infrastructure Logging permissions layer: Access to UDF logs and audit events Start with the base layer and add additional layers as needed for your specific use cases. Base/required layer¶ All Flink user accounts need these permissions. Flink access¶ Choose the appropriate Flink role based on the user’s responsibilities: FlinkDeveloper: Can create and run statements, manage workspaces and artifacts, but can’t manage compute pools FlinkAdmin: All FlinkDeveloper capabilities plus compute pool management (create, delete, alter compute pool settings) Run the following commands to grant the necessary permissions. # For most users (statement execution and development) confluent iam rbac role-binding create \ --environment ${ENV_ID} \ --principal User:${USER_ID} \ --role FlinkDeveloper # For infrastructure administrators confluent iam rbac role-binding create \ --environment ${ENV_ID} \ --principal User:${USER_ID} \ --role FlinkAdmin Kafka Transactional-Id permissions¶ Flink uses Kafka transactions to ensure exactly-once processing semantics. All Flink statements require: DeveloperRead on Transactional-Id _confluent-flink_* (to read transaction state) DeveloperWrite on Transactional-Id _confluent-flink_* (to create and manage transactions) Run the following commands to grant the necessary permissions. # Read transaction state confluent iam rbac role-binding create \ --role DeveloperRead \ --principal User:${USER_ID} \ --environment ${ENV_ID} \ --cloud-cluster ${KAFKA_ID} \ --kafka-cluster ${KAFKA_ID} \ --resource Transactional-Id:_confluent-flink_ \ --prefix # Create and manage transactions confluent iam rbac role-binding create \ --role DeveloperWrite \ --principal User:${USER_ID} \ --environment ${ENV_ID} \ --cloud-cluster ${KAFKA_ID} \ --kafka-cluster ${KAFKA_ID} \ --resource Transactional-Id:_confluent-flink_ \ --prefix Data access layer¶ The data access layer provides permissions for reading from and writing to existing tables in your Flink statements. This layer builds on the base layer and adds specific access to Kafka topics and Schema Registry subjects that your statements need to interact with. Read from existing tables¶ When your Flink SQL statements read from tables, for example, by using a SELECT * FROM my_table statement, you need these roles: DeveloperRead on the Kafka topic DeveloperRead on the Schema Registry subject Run the following commands to grant the necessary permissions. # Kafka topic read permission confluent iam rbac role-binding create \ --role DeveloperRead \ --principal User:${USER_ID} \ --environment ${ENV_ID} \ --cloud-cluster ${KAFKA_ID} \ --kafka-cluster ${KAFKA_ID} \ --resource Topic:${TOPIC_NAME} # Schema Registry subject read permission confluent iam rbac role-binding create \ --role DeveloperRead \ --principal User:${USER_ID} \ --environment ${ENV_ID} \ --cloud-cluster ${SR_ID} \ --schema-registry-cluster ${SR_ID} \ --resource Subject:${SUBJECT_NAME} Write to existing tables¶ When your Flink SQL statements write to tables, for example, by using an INSERT INTO my_sink_table statement, you need the following roles: DeveloperWrite on the Kafka topic DeveloperRead on the Schema Registry subject, to validate data format Run the following commands to grant the necessary permissions. # Kafka topic write permission confluent iam rbac role-binding create \ --role DeveloperWrite \ --principal User:${USER_ID} \ --environment ${ENV_ID} \ --cloud-cluster ${KAFKA_ID} \ --kafka-cluster ${KAFKA_ID} \ --resource Topic:${TOPIC_NAME} # Schema Registry subject read permission, to validate data format confluent iam rbac role-binding create \ --role DeveloperRead \ --principal User:${USER_ID} \ --environment ${ENV_ID} \ --cloud-cluster ${SR_ID} \ --schema-registry-cluster ${SR_ID} \ --resource Subject:${SUBJECT_NAME} Table management layer¶ The table management layer provides permissions for creating and modifying tables in your Flink statements. This layer builds on the data access layer and adds specific access to Kafka topics and Schema Registry subjects that your statements need to interact with. Create new tables¶ When your Flink SQL statements create new tables, for example, by using a CREATE TABLE or CREATE TABLE AS SELECT statement, you need the following roles: DeveloperManage on Kafka topics, to create topics DeveloperWrite on Schema Registry subjects, to create schemas Run the following commands to grant the necessary permissions. # Kafka topic create/manage permission confluent iam rbac role-binding create \ --role DeveloperManage \ --principal User:${USER_ID} \ --environment ${ENV_ID} \ --cloud-cluster ${KAFKA_ID} \ --kafka-cluster ${KAFKA_ID} \ --resource Topic:${TABLE_PREFIX} \ --prefix # Schema Registry subject create/write permission confluent iam rbac role-binding create \ --role DeveloperWrite \ --principal User:${USER_ID} \ --environment ${ENV_ID} \ --cloud-cluster ${SR_ID} \ --schema-registry-cluster ${SR_ID} \ --resource Subject:${TABLE_PREFIX} \ --prefix Modify existing tables¶ When your Flink SQL statements modify table structures, for example, by using an ALTER TABLE statement for watermarks, computed columns, or column type changes, you need the following roles: DeveloperManage on the Kafka topic, for table structure changes DeveloperWrite on the Schema Registry subject, for schema evolution Run the following commands to grant the necessary permissions. # Kafka topic manage permission, for table structure changes confluent iam rbac role-binding create \ --role DeveloperManage \ --principal User:${USER_ID} \ --environment ${ENV_ID} \ --cloud-cluster ${KAFKA_ID} \ --kafka-cluster ${KAFKA_ID} \ --resource Topic:${TABLE_NAME} # Schema Registry subject write permission, for schema evolution confluent iam rbac role-binding create \ --role DeveloperWrite \ --principal User:${USER_ID} \ --environment ${ENV_ID} \ --cloud-cluster ${SR_ID} \ --schema-registry-cluster ${SR_ID} \ --resource Subject:${TABLE_NAME} Administrative layer¶ The administrative layer provides permissions for managing Flink compute pools and infrastructure. This layer builds on the table management layer and adds specific access to Flink resources that your statements need to interact with. The following roles can manage Flink compute pools and infrastructure. FlinkAdminThis role is for Flink-specific administrative access. It provides these capabilities: Manage compute pools (create, delete, and alter compute pool settings) All FlinkDeveloper capabilities (statements, workspaces, and artifacts) Most common choice for Flink-focused administrators EnvironmentAdminThis role provides environment-wide administrative access. It provides these capabilities: All Flink administrative capabilities plus broader environment management Typically assigned for other reasons (managing multiple services in an environment) OrganizationAdminThis role provides organization-wide administrative access. It provides these capabilities: All Flink administrative capabilities plus organization-wide management Typically assigned for other reasons, like managing the entire organization Use FlinkAdmin for users who primarily manage Flink infrastructure. Users with EnvironmentAdmin or OrganizationAdmin roles already have the necessary Flink administrative capabilities. For complete role definitions and capabilities, see Predefined RBAC Roles in Confluent Cloud. Logging permissions layer¶ The logging permissions layer provides permissions for accessing UDF logs and audit events related to your Flink statements. This layer builds on the administrative layer and adds specific access to Kafka topics that your statements need to interact with. UDF logging access¶ To access UDF logs, you need these roles: FlinkAdmin or FlinkDeveloper role: provides describe access to UDF logs DeveloperRead on the UDF log topics: to read the actual log messages CloudClusterAdmin role: can manage logging settings by enabling or disabling logging Run the following command to grant the necessary permissions. confluent iam rbac role-binding create \ --role DeveloperRead \ --principal User:${USER_ID} \ --environment ${ENV_ID} \ --cloud-cluster ${LOGGING_KAFKA_ID} \ --kafka-cluster ${LOGGING_KAFKA_ID} \ --resource Topic:${UDF_LOG_TOPIC} How Flink permissions work¶ Understanding the Flink permission model helps you make informed decisions about access control and troubleshoot permission issues effectively. Principal-Based Access Control¶ Flink uses a principal-based permission model, in which statements inherit all permissions from the principal that runs them. The principal can be a user or a service account. The following key concepts help you understand how Flink permissions work. Statements are not principals - A Flink SQL statement doesn’t have its own permissions. It uses the permissions of the principal that runs it. Flexible principal assignment - You can run statements under your user account, which is recommended for ad-hoc queries, or under a service account, which is recommended for production workloads. Permission inheritance - A statement can access any data that the principal has permissions to access in that region, even across environments. For example, if a service account has DeveloperRead on topics in multiple environments, any statement running under this service account can read from topics in all of these environments, when in the same region. Separation of control plane and data plane access¶ Flink separates access into two distinct planes: the control plane and the data plane. Understanding this separation is key to configuring permissions correctly. Control plane access (infrastructure)¶ This is managed by Flink-specific roles and governs what actions you can perform within the Flink service. Control plane access has these characteristics: Controls who can create statements, manage compute pools, and other Flink resources Managed by using FlinkAdmin and FlinkDeveloper roles Environment-scoped permissions, which apply to all environments in an organization, and organization-scoped permissions, which apply to all environments in an organization Data plane access (data)¶ This is managed by Kafka and Schema Registry roles and governs which data that your Flink statements can interact with. Data plane access has these characteristics: Controls which data your statements can read from and write to Managed by using DeveloperRead, DeveloperWrite, and DeveloperManage roles Resource-scoped permissions (topics, subjects) Data plane access is important because a user needs permissions on both planes to execute a Flink SQL statement successfully. For example, a user might have the FlinkDeveloper role (control plane access to create a statement), but if they lack DeveloperRead on a source topic (data plane access), the statement fails at runtime. In contrast, a principal with extensive data access but no Flink role can’t create statements in the first place. Compute pools as shared infrastructure¶ It’s important to understand that compute pools are resources, not principals. A compute pool provides the computational infrastructure for running statements. Compute pools don’t have their own permissions or identity. Multiple users can share the same compute pool if they have appropriate Flink roles. The principal running the statement determines data access, not the compute pool. For example, users Alice and Bob both have the FlinkDeveloper role and can use the same compute pool. Alice’s statements access data based on Alice’s permissions, while Bob’s statements use Bob’s permissions, even when running on the same compute pool. Cross-environment data access¶ Flink statements can access data across environment boundaries based on the principal’s permissions. For example, a statement in Environment A can read from topics in Environment B if the principal has: FlinkDeveloper role in Environment A, to create the statement DeveloperRead role on the topics in Environment B, to access the data Cross-environment data access is important in these use cases: Cross-environment analytics and reporting Data pipeline orchestration across multiple environments Centralized processing with distributed data sources Important Grant cross-environment permissions carefully, because a statement has broad access based on its principal’s permissions. Common user scenarios¶ This section describes common user scenarios and the required permission configurations for each. These scenarios follow the layered permission model, starting with base permissions and adding additional layers as needed. Choose the scenario that best matches your use case, then follow the corresponding permission setup instructions. Developers¶ Assign the following permissions to developer accounts: Base/Required Layer (Flink Developer role + Transactional-Id permissions) Data Access Layer (read/write access to existing tables) Table Management Layer (for creating and modifying tables) Run the commands shown in the previous sections to grant the necessary permissions. Production workloads (service accounts)¶ For automated deployments and long-running statements, use service accounts to ensure stable identity that isn’t affected by changes to user accounts. Setup options¶ Broad-access approach Create a service account and grant the EnvironmentAdmin role. Grant a user the Assigner role on the service account. Deploy statements using the service account. Least-privilege approach Create a service account and grant base/required layer permissions. Grant specific Data Access Layer and Table Management Layer permissions as needed. Grant a user account the Assigner role on the service account. Run the following commands to grant the necessary permissions. # Create service account confluent iam service-account create ${SA_NAME} \ --description "${SA_DESCRIPTION}" # Broad access: Grant EnvironmentAdmin role confluent iam rbac role-binding create \ --environment ${ENV_ID} \ --principal User:${SERVICE_ACCOUNT_ID} \ --role EnvironmentAdmin # Grant user Assigner role (for both approaches) confluent iam rbac role-binding create \ --principal User:${USER_ID} \ --resource service-account:${SERVICE_ACCOUNT_ID} \ --role Assigner For the least-privilege approach, run the commands in the previous layer sections, using the service account as the principal instead of a user account. Administrators (infrastructure management)¶ For managing compute pools and Flink infrastructure, grant an administrative layer role. These are the administrative roles that can manage Flink infrastructure: FlinkAdmin: Most common choice for Flink-focused administrators EnvironmentAdmin: If they already manage other services in the environment OrganizationAdmin: If they already manage the entire organization Production best practices¶ Grant permissions incrementally, starting with base permissions and adding additional layers as needed for your production use cases. Start with base/required layer - Grant fundamental Flink and Kafka permissions. Add data access - Grant read/write access to existing tables as needed. Add capabilities - Table management, administrative access as required. Validate each layer - Test functionality after adding each permission layer. Service account delegation pattern¶ For automated deployments, run the following command to grant the Assigner role on production service accounts. # CI/CD service account with Assigner role on production service accounts confluent iam rbac role-binding create \ --principal User:${CICD_SA_ID} \ --resource service-account:${PROD_SA_ID} \ --role Assigner Access for UDF Logging¶ OrganizationAdmin and EnvironmentAdmin roles have full permission to enable and disable custom code logging. The FlinkAdmin and FlinkDeveloper roles have permission to describe custom code logs. The CloudClusterAdmin role can disable logging and delete logs by deleting the Kafka cluster associated with the custom logging destination. Any security principal that has READ permission to the destination Kafka cluster has READ access to the actual log topics. Audit log events¶ Auditable event methods for the FLINK_WORKSPACE and STATEMENT resource types are triggered by operations on a Flink workspace and generate event messages that are sent to the audit log cluster, where they are stored as event records in a Kafka topic. For more information, see Auditable Event Methods. Reference¶ Permission summary by layer¶ Layer Kafka Schema Registry Flink Base/Required Transactional-Id – FlinkDeveloper OR FlinkAdmin Data Access DeveloperRead/Write DeveloperRead/Write – Table Management DeveloperManage DeveloperWrite – Administrative – – FlinkAdmin, EnvironmentAdmin, or OrganizationAdmin Logging DeveloperRead (UDF logs) – – Access to Flink resources¶ The following table shows which Flink resources the RBAC roles can access. “CRUD” stands for “Create, Read, Update, Delete”. Scope Statements Workspaces Compute pools Artifacts User-defined functions UDF logging AI inference models Kafka clusters Kafka Topics EnvironmentAdmin CRUD CRUD CRUD CRUD CRUD CRUD CRUD CRUD CRUD FlinkAdmin CRUD CRUD CRUD CRUD CRUD -R– CRUD – – FlinkDeveloper CRUD CRUD -R– CRUD CRUD [1] -R– CRUD [1] – – OrganizationAdmin CRUD CRUD CRUD CRUD CRUD CRUD CRUD CRUD CRUD [1](1, 2) Requires access to cluster. Related content¶ Auditable Event Methods DDL Statements Manage RBAC Role Bindings Role-based Access Control (RBAC) Service Accounts UDF logs Note This website includes content developed at the Apache Software Foundation under the terms of the Apache License v2.

#### Code Examples

```sql
# For most users (statement execution and development)
confluent iam rbac role-binding create \
  --environment ${ENV_ID} \
  --principal User:${USER_ID} \
  --role FlinkDeveloper

# For infrastructure administrators
confluent iam rbac role-binding create \
  --environment ${ENV_ID} \
  --principal User:${USER_ID} \
  --role FlinkAdmin
```

```sql
_confluent-flink_*
```

```sql
_confluent-flink_*
```

```sql
# Read transaction state
confluent iam rbac role-binding create \
  --role DeveloperRead \
  --principal User:${USER_ID} \
  --environment ${ENV_ID} \
  --cloud-cluster ${KAFKA_ID} \
  --kafka-cluster ${KAFKA_ID} \
  --resource Transactional-Id:_confluent-flink_ \
  --prefix

# Create and manage transactions
confluent iam rbac role-binding create \
  --role DeveloperWrite \
  --principal User:${USER_ID} \
  --environment ${ENV_ID} \
  --cloud-cluster ${KAFKA_ID} \
  --kafka-cluster ${KAFKA_ID} \
  --resource Transactional-Id:_confluent-flink_ \
  --prefix
```

```sql
SELECT * FROM my_table
```

```sql
# Kafka topic read permission
confluent iam rbac role-binding create \
  --role DeveloperRead \
  --principal User:${USER_ID} \
  --environment ${ENV_ID} \
  --cloud-cluster ${KAFKA_ID} \
  --kafka-cluster ${KAFKA_ID} \
  --resource Topic:${TOPIC_NAME}

# Schema Registry subject read permission
confluent iam rbac role-binding create \
  --role DeveloperRead \
  --principal User:${USER_ID} \
  --environment ${ENV_ID} \
  --cloud-cluster ${SR_ID} \
  --schema-registry-cluster ${SR_ID} \
  --resource Subject:${SUBJECT_NAME}
```

```sql
INSERT INTO my_sink_table
```

```sql
# Kafka topic write permission
confluent iam rbac role-binding create \
  --role DeveloperWrite \
  --principal User:${USER_ID} \
  --environment ${ENV_ID} \
  --cloud-cluster ${KAFKA_ID} \
  --kafka-cluster ${KAFKA_ID} \
  --resource Topic:${TOPIC_NAME}

# Schema Registry subject read permission, to validate data format
confluent iam rbac role-binding create \
  --role DeveloperRead \
  --principal User:${USER_ID} \
  --environment ${ENV_ID} \
  --cloud-cluster ${SR_ID} \
  --schema-registry-cluster ${SR_ID} \
  --resource Subject:${SUBJECT_NAME}
```

```sql
CREATE TABLE
```

```sql
CREATE TABLE AS SELECT
```

```sql
# Kafka topic create/manage permission
confluent iam rbac role-binding create \
  --role DeveloperManage \
  --principal User:${USER_ID} \
  --environment ${ENV_ID} \
  --cloud-cluster ${KAFKA_ID} \
  --kafka-cluster ${KAFKA_ID} \
  --resource Topic:${TABLE_PREFIX} \
  --prefix

# Schema Registry subject create/write permission
confluent iam rbac role-binding create \
  --role DeveloperWrite \
  --principal User:${USER_ID} \
  --environment ${ENV_ID} \
  --cloud-cluster ${SR_ID} \
  --schema-registry-cluster ${SR_ID} \
  --resource Subject:${TABLE_PREFIX} \
  --prefix
```

```sql
ALTER TABLE
```

```sql
# Kafka topic manage permission, for table structure changes
confluent iam rbac role-binding create \
  --role DeveloperManage \
  --principal User:${USER_ID} \
  --environment ${ENV_ID} \
  --cloud-cluster ${KAFKA_ID} \
  --kafka-cluster ${KAFKA_ID} \
  --resource Topic:${TABLE_NAME}

# Schema Registry subject write permission, for schema evolution
confluent iam rbac role-binding create \
  --role DeveloperWrite \
  --principal User:${USER_ID} \
  --environment ${ENV_ID} \
  --cloud-cluster ${SR_ID} \
  --schema-registry-cluster ${SR_ID} \
  --resource Subject:${TABLE_NAME}
```

```sql
confluent iam rbac role-binding create \
  --role DeveloperRead \
  --principal User:${USER_ID} \
  --environment ${ENV_ID} \
  --cloud-cluster ${LOGGING_KAFKA_ID} \
  --kafka-cluster ${LOGGING_KAFKA_ID} \
  --resource Topic:${UDF_LOG_TOPIC}
```

```sql
# Create service account
confluent iam service-account create ${SA_NAME} \
  --description "${SA_DESCRIPTION}"

# Broad access: Grant EnvironmentAdmin role
confluent iam rbac role-binding create \
  --environment ${ENV_ID} \
  --principal User:${SERVICE_ACCOUNT_ID} \
  --role EnvironmentAdmin

# Grant user Assigner role (for both approaches)
confluent iam rbac role-binding create \
  --principal User:${USER_ID} \
  --resource service-account:${SERVICE_ACCOUNT_ID} \
  --role Assigner
```

```sql
# CI/CD service account with Assigner role on production service accounts
confluent iam rbac role-binding create \
  --principal User:${CICD_SA_ID} \
  --resource service-account:${PROD_SA_ID} \
  --role Assigner
```

```sql
FLINK_WORKSPACE
```

---

### Deploy a Flink SQL Statement in Confluent Cloud for Apache Flink | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/operate-and-deploy/deploy-flink-sql-statement.html

Deploy a Flink SQL Statement Using CI/CD and Confluent Cloud for Apache Flink¶ GitHub Actions is a powerful feature on GitHub that enables automating your software development workflows. If your source code is stored in a GitHub repository, you can easily create a custom workflow in GitHub Actions to build, test, package, release, or deploy any code project. This topic shows how to create a CI/CD workflow that deploys an Apache Flink® SQL statement programmatically on Confluent Cloud for Apache Flink by using Hashicorp Terraform and GitHub Actions. With the steps in this topic, you can streamline your development process. In this walkthrough, you perform the following steps: Step 1: Set up a Terraform Cloud workspace Step 2: Set up a repository and secrets in GitHub Step 3. Create a CI/CD workflow in GitHub Actions Step 4. Deploy resources in Confluent Cloud Step 5. Deploy a Flink SQL statement Prerequisites¶ You need the following prerequisites to complete this tutorial: Access to Confluent Cloud A GitHub account to set up a repository and create the CI/CD workflow A Terraform Cloud account Step 1: Set up a Terraform Cloud workspace¶ You need a Terraform Cloud account to follow this tutorial. If you don’t have one yet, create an account for free at Terraform Cloud. With a Terraform Cloud account, you can manage your infrastructure-as-code and collaborate with your team. Create a workspace¶ If you have created a new Terraform Cloud account and the Getting Started page is displayed, click Create a new organization, and in the Organization name textbox, enter “flink_ccloud”. Click Create organization. Otherwise, from the Terraform Cloud homepage, click New to create a new workspace. In the Create a new workspace page, click the API-Driven Workflow tile, and in the Workspace name textbox, enter “cicd_flink_ccloud”. Click Create to create the workspace. Create a Terraform Cloud API token¶ By creating an API token, you can authenticate securely with Terraform Cloud and integrate it with GitHub Actions. Save the token in a secure location, and don’t share it with anyone. At the top of the navigation menu, click your user icon and select User settings. In the navigation menu, click Tokens, and in the Tokens page, click Create an API token. Give your token a meaningful description, like “github_actions”, and click Generate token. Your token appears in the Tokens list. Save the API token in a secure location. It won’t be displayed again. Step 2: Set up a repository and secrets in GitHub¶ To create an Action Secret in GitHub for securely storing the API token from Terraform Cloud, follow these steps. Log in to your GitHub account and create a new repository. In the Create a new repository page, use the Owner dropdown to choose an owner, and give the repository a unique name, like “<your-name-flink-ccloud>”. Click Create. In the repository details page, click Settings. In the navigation menu, click Secrets and variables, and in the context menu, select Actions to open the Actions secrets and variables page. Click New repository secret. In the New secret page, enter the following settings. In the Name textbox, enter “TF_API_TOKEN”. In the Secret textbox, enter the API token value that you saved from the previous Terraform Cloud step. Click Add secret to save the Action Secret. By creating an Action Secret for the API token, you can use it securely in your CI/CD pipelines, such as in GitHub Actions. Keep the secret safe, and don’t share it with anyone who shouldn’t have access to it. Step 3. Create a CI/CD workflow in GitHub Actions¶ The following steps show how to create an Action Workflow for automating the deployment of a Flink SQL statement on Confluent Cloud using Terraform. In the toolbar at the top of the screen, click Actions. The Get started with GitHub Actions page opens. Click set up a workflow yourself ->. If you already have a workflow defined, click new workflow, and then click set up a workflow yourself ->. Copy the following YAML into the editor. This YAML file defines a workflow that runs when changes are pushed to the main branch of your repository. It includes a job named “terraform_flink_ccloud_tutorial” that runs on the latest version of Ubuntu. The job includes these steps: Check out the code Set up Terraform Log in to Terraform Cloud using the API token stored in the Action Secret Initialize Terraform Apply the Terraform configuration to deploy changes to your Confluent Cloud account on: push: branches: - main jobs: terraform_flink_ccloud_tutorial: name: "terraform_flink_ccloud_tutorial" runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v4 - name: Setup Terraform uses: hashicorp/setup-terraform@v3 with: cli_config_credentials_token: ${{ secrets.TF_API_TOKEN }} - name: Terraform Init id: init run: terraform init - name: Terraform Validate id: validate run: terraform validate -no-color - name: Terraform Plan id: plan run: terraform plan env: TF_VAR_confluent_cloud_api_key: ${{ secrets.CONFLUENT_CLOUD_API_KEY }} TF_VAR_confluent_cloud_api_secret: ${{ secrets.CONFLUENT_CLOUD_API_SECRET }} - name: Terraform Apply id: apply run: terraform apply -auto-approve env: TF_VAR_confluent_cloud_api_key: ${{ secrets.CONFLUENT_CLOUD_API_KEY }} TF_VAR_confluent_cloud_api_secret: ${{ secrets.CONFLUENT_CLOUD_API_SECRET }} Click Commit changes, and in the dialog, enter a description in the Extended description textbox, for example, “CI/CD workflow to automate deployment on Confluent Cloud”. Click Commit changes. The file main.yml is created in the .github/workflows directory in your repository. With this Action Workflow, your deployment of Flink SQL statements on Confluent Cloud is now automatic. Step 4. Deploy resources in Confluent Cloud¶ In this section, you deploy a Flink SQL statement programmatically to Confluent Cloud that runs continuously until stopped manually. In VS Code or another IDE, clone your repository and create a new file in the root named “main.tf” with the following code. Replace the organization and workspace names with your Terraform Cloud organization name and workspace names from Step 1. terraform { cloud { organization = "<your-terraform-org-name>" workspaces { name = "cicd_flink_ccloud" } } required_providers { confluent = { source = "confluentinc/confluent" version = "2.2.0" } } } Commit and push the changes to the repository. The CI/CD workflow that you created previously runs automatically. Verify that it’s running by navigating to the Actions section in your repository and clicking on the latest workflow run. Create a Confluent Cloud API key¶ To access Confluent Cloud securely, you must have a Confluent Cloud API key. After you generate an API key, you store securely it in your GitHub repository’s Secrets and variables page, the same way that you stored the Terraform API token. Follow the instructions here to create a new API key for Confluent Cloud, and on the https://confluent.cloud/settings/api-keys page, select the Cloud resource management tile for the API key’s resource scope. You will use this API key to communicate securely with Confluent Cloud. Return to the Settings page for your GitHub repository, and in the navigation menu, click Secrets and variables. In the context menu, select Actions to open the Actions secrets and variables page. Click New repository secret. In the New secret page, enter the following settings. In the Name textbox, enter “CONFLUENT_CLOUD_API_KEY”. In the Secret textbox, enter the Cloud API key. Click Add secret to save the Cloud API key as an Action Secret. Click New repository secret and repeat the previous steps for the Cloud API secret. Name the secret “CONFLUENT_CLOUD_API_SECRET”. Your Repository secrets list should resemble the following: Deploy resources¶ In this section, you add resources to your Terraform configuration file and provision them when the GitHub Action runs. In your repository, create a new file named “variables.tf” with the following code. variable "confluent_cloud_api_key" { description = "Confluent Cloud API Key" type = string } variable "confluent_cloud_api_secret" { description = "Confluent Cloud API Secret" type = string sensitive = true } In the “main.tf” file, add the following code. This code references the Cloud API key and secret you added in the previous steps and creates a new environment and Kafka cluster for your organization. Optionally, you can choose to use an existing environment. locals { cloud = "AWS" region = "us-east-2" } provider "confluent" { cloud_api_key = var.confluent_cloud_api_key cloud_api_secret = var.confluent_cloud_api_secret } # Create a new environment. resource "confluent_environment" "my_env" { display_name = "my_env" stream_governance { package = "ESSENTIALS" } } # Create a new Kafka cluster. resource "confluent_kafka_cluster" "my_kafka_cluster" { display_name = "my_kafka_cluster" availability = "SINGLE_ZONE" cloud = local.cloud region = local.region basic {} environment { id = confluent_environment.my_env.id } depends_on = [ confluent_environment.my_env ] } # Access the Stream Governance Essentials package to the environment. data "confluent_schema_registry_cluster" "my_sr_cluster" { environment { id = confluent_environment.my_env.id } } Create a Service Account and provide a role binding by adding the following code to “main.tf”. The role binding gives the Service Account the necessary permissions to create topics, Flink statements, and other resources. In production, you may want to assign a less privileged role than OrganizationAdmin. # Create a new Service Account. This will used during Kafka API key creation and Flink SQL statement submission. resource "confluent_service_account" "my_service_account" { display_name = "my_service_account" } data "confluent_organization" "my_org" {} # Assign the OrganizationAdmin role binding to the above Service Account. # This will give the Service Account the necessary permissions to create topics, Flink statements, etc. # In production, you may want to assign a less privileged role. resource "confluent_role_binding" "my_org_admin_role_binding" { principal = "User:${confluent_service_account.my_service_account.id}" role_name = "OrganizationAdmin" crn_pattern = data.confluent_organization.my_org.resource_name depends_on = [ confluent_service_account.my_service_account ] } Push all changes to your repository and check the Actions page to ensure the workflow runs successfully. At this point, you should have a new environment, an Apache Kafka® cluster, and a Stream Governance package provisioned in your Confluent Cloud organization. Step 5. Deploy a Flink SQL statement¶ To use Flink, you must create a Flink compute pool. A compute pool represents a set of compute resources that are bound to a region and are used to run your Flink SQL statements. For more information, see Compute Pools. Create a new compute pool by adding the following code to “main.tf”. # Create a Flink compute pool to execute a Flink SQL statement. resource "confluent_flink_compute_pool" "my_compute_pool" { display_name = "my_compute_pool" cloud = local.cloud region = local.region max_cfu = 10 environment { id = confluent_environment.my_env.id } depends_on = [ confluent_environment.my_env ] } Create a Flink-specific API key, which is required for submitting statements to Confluent Cloud, by adding the following code to “main.tf”. # Create a Flink-specific API key that will be used to submit statements. data "confluent_flink_region" "my_flink_region" { cloud = local.cloud region = local.region } resource "confluent_api_key" "my_flink_api_key" { display_name = "my_flink_api_key" owner { id = confluent_service_account.my_service_account.id api_version = confluent_service_account.my_service_account.api_version kind = confluent_service_account.my_service_account.kind } managed_resource { id = data.confluent_flink_region.my_flink_region.id api_version = data.confluent_flink_region.my_flink_region.api_version kind = data.confluent_flink_region.my_flink_region.kind environment { id = confluent_environment.my_env.id } } depends_on = [ confluent_environment.my_env, confluent_service_account.my_service_account ] } Deploy a Flink SQL statement on Confluent Cloud by adding the following code to “main.tf”. The statement consumes data from examples.marketplace.orders, aggregates in 1 minute windows and ingests the filtered data into sink_topic. Because you’re using a Service Account, the statement runs in Confluent Cloud continuously until manually stopped. # Deploy a Flink SQL statement to Confluent Cloud. resource "confluent_flink_statement" "my_flink_statement" { organization { id = data.confluent_organization.my_org.id } environment { id = confluent_environment.my_env.id } compute_pool { id = confluent_flink_compute_pool.my_compute_pool.id } principal { id = confluent_service_account.my_service_account.id } # This SQL reads data from source_topic, filters it, and ingests the filtered data into sink_topic. statement = <<EOT CREATE TABLE my_sink_topic AS SELECT window_start, window_end, SUM(price) AS total_revenue, COUNT(*) AS cnt FROM TABLE(TUMBLE(TABLE `examples`.`marketplace`.`orders`, DESCRIPTOR($rowtime), INTERVAL '1' MINUTE)) GROUP BY window_start, window_end; EOT properties = { "sql.current-catalog" = confluent_environment.my_env.display_name "sql.current-database" = confluent_kafka_cluster.my_kafka_cluster.display_name } rest_endpoint = data.confluent_flink_region.my_flink_region.rest_endpoint credentials { key = confluent_api_key.my_flink_api_key.id secret = confluent_api_key.my_flink_api_key.secret } depends_on = [ confluent_api_key.my_flink_api_key, confluent_flink_compute_pool.my_compute_pool, confluent_kafka_cluster.my_kafka_cluster ] } Push all changes to your repository and check the Actions page to ensure the workflow runs successfully. In Confluent Cloud Console, verify that the statement has been deployed and that sink_topic is receiving the data. You have a fully functioning CI/CD pipeline with Confluent Cloud and Terraform. This pipeline enables automating the deployment and management of your infrastructure, making it more efficient and scalable. Related content¶ Get Started with Confluent Cloud for Apache Flink Compute Pools

#### Code Examples

```sql
on:
 push:
    branches:
    - main

jobs:
 terraform_flink_ccloud_tutorial:
    name: "terraform_flink_ccloud_tutorial"
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
         cli_config_credentials_token: ${{ secrets.TF_API_TOKEN }}

      - name: Terraform Init
        id: init
        run: terraform init

      - name: Terraform Validate
        id: validate
        run: terraform validate -no-color

      - name: Terraform Plan
        id: plan
        run: terraform plan
        env:
          TF_VAR_confluent_cloud_api_key: ${{ secrets.CONFLUENT_CLOUD_API_KEY }}
          TF_VAR_confluent_cloud_api_secret: ${{ secrets.CONFLUENT_CLOUD_API_SECRET }}

      - name: Terraform Apply
        id: apply
        run: terraform apply -auto-approve
        env:
          TF_VAR_confluent_cloud_api_key: ${{ secrets.CONFLUENT_CLOUD_API_KEY }}
          TF_VAR_confluent_cloud_api_secret: ${{ secrets.CONFLUENT_CLOUD_API_SECRET }}
```

```sql
.github/workflows
```

```sql
terraform {
  cloud {
    organization = "<your-terraform-org-name>"

    workspaces {
      name = "cicd_flink_ccloud"
    }
  }

  required_providers {
    confluent = {
      source  = "confluentinc/confluent"
      version = "2.2.0"
    }
  }
}
```

```sql
variable "confluent_cloud_api_key" {
  description = "Confluent Cloud API Key"
  type        = string
}

variable "confluent_cloud_api_secret" {
  description = "Confluent Cloud API Secret"
  type        = string
  sensitive   = true
}
```

```sql
locals {
  cloud  = "AWS"
  region = "us-east-2"
}

provider "confluent" {
  cloud_api_key    = var.confluent_cloud_api_key
  cloud_api_secret = var.confluent_cloud_api_secret
}

# Create a new environment.
resource "confluent_environment" "my_env" {
  display_name = "my_env"

  stream_governance {
    package = "ESSENTIALS"
  }
}

# Create a new Kafka cluster.
resource "confluent_kafka_cluster" "my_kafka_cluster" {
  display_name = "my_kafka_cluster"
  availability = "SINGLE_ZONE"
  cloud        = local.cloud
  region       = local.region
  basic {}

  environment {
    id = confluent_environment.my_env.id
  }

  depends_on = [
    confluent_environment.my_env
  ]
}

# Access the Stream Governance Essentials package to the environment.
data "confluent_schema_registry_cluster" "my_sr_cluster" {
  environment {
    id = confluent_environment.my_env.id
  }
}
```

```sql
# Create a new Service Account. This will used during Kafka API key creation and Flink SQL statement submission.
resource "confluent_service_account" "my_service_account" {
  display_name = "my_service_account"
}

data "confluent_organization" "my_org" {}

# Assign the OrganizationAdmin role binding to the above Service Account.
# This will give the Service Account the necessary permissions to create topics, Flink statements, etc.
# In production, you may want to assign a less privileged role.
resource "confluent_role_binding" "my_org_admin_role_binding" {
  principal   = "User:${confluent_service_account.my_service_account.id}"
  role_name   = "OrganizationAdmin"
  crn_pattern = data.confluent_organization.my_org.resource_name

  depends_on = [
    confluent_service_account.my_service_account
  ]
}
```

```sql
# Create a Flink compute pool to execute a Flink SQL statement.
resource "confluent_flink_compute_pool" "my_compute_pool" {
  display_name = "my_compute_pool"
  cloud        = local.cloud
  region       = local.region
  max_cfu      = 10

  environment {
    id = confluent_environment.my_env.id
  }

  depends_on = [
    confluent_environment.my_env
  ]
}
```

```sql
# Create a Flink-specific API key that will be used to submit statements.
data "confluent_flink_region" "my_flink_region" {
  cloud  = local.cloud
  region = local.region
}

resource "confluent_api_key" "my_flink_api_key" {
  display_name = "my_flink_api_key"

  owner {
    id          = confluent_service_account.my_service_account.id
    api_version = confluent_service_account.my_service_account.api_version
    kind        = confluent_service_account.my_service_account.kind
  }

  managed_resource {
    id          = data.confluent_flink_region.my_flink_region.id
    api_version = data.confluent_flink_region.my_flink_region.api_version
    kind        = data.confluent_flink_region.my_flink_region.kind

    environment {
      id = confluent_environment.my_env.id
    }
  }

  depends_on = [
    confluent_environment.my_env,
    confluent_service_account.my_service_account
  ]
}
```

```sql
examples.marketplace.orders
```

```sql
# Deploy a Flink SQL statement to Confluent Cloud.
resource "confluent_flink_statement" "my_flink_statement" {
  organization {
    id = data.confluent_organization.my_org.id
  }

  environment {
    id = confluent_environment.my_env.id
  }

  compute_pool {
    id = confluent_flink_compute_pool.my_compute_pool.id
  }

  principal {
    id = confluent_service_account.my_service_account.id
  }

  # This SQL reads data from source_topic, filters it, and ingests the filtered data into sink_topic.
  statement = <<EOT
    CREATE TABLE my_sink_topic AS
    SELECT
      window_start,
      window_end,
      SUM(price) AS total_revenue,
      COUNT(*) AS cnt
    FROM
    TABLE(TUMBLE(TABLE `examples`.`marketplace`.`orders`, DESCRIPTOR($rowtime), INTERVAL '1' MINUTE))
    GROUP BY window_start, window_end;
    EOT

  properties = {
    "sql.current-catalog"  = confluent_environment.my_env.display_name
    "sql.current-database" = confluent_kafka_cluster.my_kafka_cluster.display_name
  }

  rest_endpoint = data.confluent_flink_region.my_flink_region.rest_endpoint

  credentials {
    key    = confluent_api_key.my_flink_api_key.id
    secret = confluent_api_key.my_flink_api_key.secret
  }

  depends_on = [
    confluent_api_key.my_flink_api_key,
    confluent_flink_compute_pool.my_compute_pool,
    confluent_kafka_cluster.my_kafka_cluster
  ]
}
```

---

### Generate an API key for Programmatic Access to Confluent Cloud for Apache Flink | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/operate-and-deploy/generate-api-key-for-flink.html

Generate an API Key for Access in Confluent Cloud for Apache Flink¶ To manage Flink workloads programmatically in Confluent Cloud for Apache Flink®, you need an API key that’s specific to Flink. You can use the Confluent CLI, the Confluent Cloud APIs, the Confluent Terraform Provider, or the Cloud Console to create API keys. Before you create an API key for Flink access, decide whether you want to create long-running statements. If you need long-running statements, you should use a service account and create an API key for it. If you only need to run interactive queries or run statements for a short time while developing queries, you can create an API key for your user account. A Flink API key is scoped to an environment and region pair, for example, env-abc123.aws.us-east-2. The key enables creating, reading, updating, and deleting Flink SQL statements. To create an API key for Flink access by using the Confluent Cloud APIs or the Confluent Terraform Provider, you must first create a Cloud API key. This step is done automatically if you use the Confluent CLI to create an API key for Flink access. Create a service account (optional)¶ If you need to create long-running Flink SQL statements, create a service account principal before you create a Flink API key. Create a service account by using the Cloud Console or the CLI. Assign the OrganizationAdmin role to the service account by following the steps in Add a role binding to a principal. Store the service account ID in a convenient location, for example, in an environment variable: export PRINCIPAL_ID="<service-account-id>" Generate an API Key¶ You can use the Confluent Cloud APIs, the Confluent Terraform Provider, the Confluent CLI, or the Cloud Console to create an API key for Flink access. For more information, see Manage API Keys. Cloud ConsoleConfluent CLIConfluent Cloud APIsTerraformYou can use the Cloud Console to generate an API key for Flink access. Log in to the Confluent Cloud Console and navigate to the environment that hosts your data and compute pools. Click Flink and in the Flink overview page, click API keys. Click Add API Key to open the Create API key page. Select either the My account tile to create an API key for your user account or the Service account tile to create an API key for a service account. For production Flink deployments, select the Service account option, and click either Existing account or New account to assign the service account principal. Click Next to open the Resource scope page. Select the cloud provider and region for the API key. Ensure that you choose the same provider and region where your data and compute pools are located. Click Next to open the API key detail page. Enter a name and a description for the new API key. This step is optional. Click Create API key. The API key download page opens. Click Download API key and save the key to a secure location on your local machine. Click Complete. You can use the Confluent CLI to generate an API key for Flink access. For more information, see confluent api-key create . Log in to Confluent Cloud: confluent login To see the available regions for Flink, run the following command: confluent flink region list Your output should resemble: Current | Name | Cloud | Region ----------+--------------------------+-------+--------------- | Frankfurt (eu-central-1) | aws | eu-central-1 | Ireland (eu-west-1) | aws | eu-west-1 * | N. Virginia (us-east-1) | aws | us-east-1 | Ohio (us-east-2) | aws | us-east-2 Run the following command to create an API key. Enure that the environment variables are set to your values. # Example values for environment variables. export CLOUD_PROVIDER=aws export CLOUD_REGION=us-east-1 export ENV_ID=env-a12b34 # Generate the API key and secret. confluent api-key create \ --resource flink \ --cloud ${CLOUD_PROVIDER} \ --region ${CLOUD_REGION} \ --environment ${ENV_ID} Your output should resemble: It may take a couple of minutes for the API key to be ready. Save the API key and secret. The secret is not retrievable later. +------------+------------------------------------------------------------------+ | API Key | ABC1DDN2BNASQVRU | | API Secret | B0b+xCoSPY2pSNETeuyrziWmsPmou0WP9rH0Nxed4y4/msnESzjj7kBrRWGOMu1a | +------------+------------------------------------------------------------------+ If the environment, cloud, and region flags are set globally, you can create an API key by running confluent api-key create --resource flink. For more information, see Manage API Keys in Confluent Cloud. To create an API key for an existing service account, provide the --service-account <sa-a1b2c3> option. This enables submitting long-running Flink SQL statements. To create an API key for Flink access by using the Confluent Cloud APIs, you must first create a Cloud API key. To generate the Flink key, you send your Cloud API key and secret in the request header, encoded as a base64 string. Create a Cloud API key for the principal, which is either a service account or your user account. For more information, see Add an API key. Assign the Cloud API key and secret to environment variables that you use in your REST API requests. export CLOUD_API_KEY="<cloud-api-key>" export CLOUD_API_SECRET="<cloud-api-secret>" export PRINCIPAL_ID="<service-account-id>" # or "<user-account-id>" export ENV_REGION_ID="<environment-id>.<cloud-region>" # example: "env-z3y2x1.aws.us-east-1" The ENV_REGION_ID variable is a concatenation of your environment ID and the cloud provider region of your Kafka cluster, separated by a . character. To see the available regions, run the confluent flink region list command. Run the following command to send a POST request to the api-keys endpoint. The REST API uses basic authentication, which means that you provide a base64-encoded string made from your Cloud API key and secret in the request header. curl --request POST \ --url 'https://api.confluent.cloud/iam/v2/api-keys' \ --header "Authorization: Basic $(echo -n "${CLOUD_API_KEY}:${CLOUD_API_SECRET}" | base64 -w 0)" \ --header 'content-type: application/json' \ --data "{"spec":{"display_name":"flinkapikey","owner":{"id":"${PRINCIPAL_ID}"},"resource":{"api_version":"fcpm/v2","id":"${ENV_REGION_ID}"}}}" Your output should resemble: { "api_version": "iam/v2", "id": "KJDYFDMBOBDNQEIU", "kind": "ApiKey", "metadata": { "created_at": "2023-12-15T23:10:20.406556Z", "resource_name": "crn://api.confluent.cloud/organization=b0b21724-4586-4a07-b787-d0bb5aacbf87/user=u-lq1dr3/api-key=KJDYFDMBOBDNQEIU", "self": "https://api.confluent.cloud/iam/v2/api-keys/KJDYFDMBOBDNQEIU", "updated_at": "2023-12-15T23:10:20.406556Z" }, "spec": { "description": "", "display_name": "flinkapikey", "owner": { "api_version": "iam/v2", "id": "u-lq1dr3", "kind": "User", "related": "https://api.confluent.cloud/iam/v2/users/u-lq2dr7", "resource_name": "crn://api.confluent.cloud/organization=b0b21724-4586-4a07-b787-d0bb5aacbf87/user=u-lq2dr7" }, "resource": { "api_version": "fcpm/v2", "id": "env-z3q9rd.aws.us-east-1", "kind": "Region", "related": "https://api.confluent.cloud/fcpm/v2/regions?cloud=aws", "resource_name": "crn://api.confluent.cloud/organization=b0b21724-4586-4a07-b787-d0bb5aacbf87/environment=env-z3q9rd/flink-region=aws.us-east-1" }, "secret": "B0BYFzyd0bb5Q58ZZJJYV52mbwDDHnZx21f0gOTz2k6Qv2V9I4KraVztwFOlQx6z" } } You can use the Confluent Terraform Provider to generate an API key for Flink access. Follow the steps in Sample Project for Confluent Terraform Provider and use the configuration shown in Example Flink API Key. When your API key and secret are generated, save them in environment variables for later use. export FLINK_API_KEY="<flink-api-key>" export FLINK_API_SECRET="<flink-api-secret>" You can manage the API key by using the Confluent CLI commands. For more information, see confluent api-key . Also, you can use the REST API and Cloud Console. Next steps¶ Flink SQL REST API Related content¶ Manage API Keys Confluent CLI commands with Confluent Cloud for Apache Flink Note This website includes content developed at the Apache Software Foundation under the terms of the Apache License v2.

#### Code Examples

```sql
env-abc123.aws.us-east-2
```

```sql
export PRINCIPAL_ID="<service-account-id>"
```

```sql
confluent login
```

```sql
confluent flink region list
```

```sql
Current |           Name           | Cloud |    Region
----------+--------------------------+-------+---------------
          | Frankfurt (eu-central-1) | aws   | eu-central-1
          | Ireland (eu-west-1)      | aws   | eu-west-1
  *       | N. Virginia (us-east-1)  | aws   | us-east-1
          | Ohio (us-east-2)         | aws   | us-east-2
```

```sql
# Example values for environment variables.
export CLOUD_PROVIDER=aws
export CLOUD_REGION=us-east-1
export ENV_ID=env-a12b34

# Generate the API key and secret.
confluent api-key create \
  --resource flink \
  --cloud ${CLOUD_PROVIDER} \
  --region ${CLOUD_REGION} \
  --environment ${ENV_ID}
```

```sql
It may take a couple of minutes for the API key to be ready.
Save the API key and secret. The secret is not retrievable later.
+------------+------------------------------------------------------------------+
| API Key    | ABC1DDN2BNASQVRU                                                 |
| API Secret | B0b+xCoSPY2pSNETeuyrziWmsPmou0WP9rH0Nxed4y4/msnESzjj7kBrRWGOMu1a |
+------------+------------------------------------------------------------------+
```

```sql
confluent api-key create --resource flink
```

```sql
--service-account <sa-a1b2c3>
```

```sql
export CLOUD_API_KEY="<cloud-api-key>"
export CLOUD_API_SECRET="<cloud-api-secret>"
export PRINCIPAL_ID="<service-account-id>" # or "<user-account-id>"
export ENV_REGION_ID="<environment-id>.<cloud-region>" # example: "env-z3y2x1.aws.us-east-1"
```

```sql
confluent flink region list
```

```sql
curl --request POST \
  --url 'https://api.confluent.cloud/iam/v2/api-keys' \
  --header "Authorization: Basic $(echo -n "${CLOUD_API_KEY}:${CLOUD_API_SECRET}" | base64 -w 0)" \
  --header 'content-type: application/json' \
  --data "{"spec":{"display_name":"flinkapikey","owner":{"id":"${PRINCIPAL_ID}"},"resource":{"api_version":"fcpm/v2","id":"${ENV_REGION_ID}"}}}"
```

```sql
{
  "api_version": "iam/v2",
  "id": "KJDYFDMBOBDNQEIU",
  "kind": "ApiKey",
  "metadata": {
    "created_at": "2023-12-15T23:10:20.406556Z",
    "resource_name": "crn://api.confluent.cloud/organization=b0b21724-4586-4a07-b787-d0bb5aacbf87/user=u-lq1dr3/api-key=KJDYFDMBOBDNQEIU",
    "self": "https://api.confluent.cloud/iam/v2/api-keys/KJDYFDMBOBDNQEIU",
    "updated_at": "2023-12-15T23:10:20.406556Z"
  },
  "spec": {
    "description": "",
    "display_name": "flinkapikey",
    "owner": {
      "api_version": "iam/v2",
      "id": "u-lq1dr3",
      "kind": "User",
      "related": "https://api.confluent.cloud/iam/v2/users/u-lq2dr7",
      "resource_name": "crn://api.confluent.cloud/organization=b0b21724-4586-4a07-b787-d0bb5aacbf87/user=u-lq2dr7"
    },
    "resource": {
      "api_version": "fcpm/v2",
      "id": "env-z3q9rd.aws.us-east-1",
      "kind": "Region",
      "related": "https://api.confluent.cloud/fcpm/v2/regions?cloud=aws",
      "resource_name": "crn://api.confluent.cloud/organization=b0b21724-4586-4a07-b787-d0bb5aacbf87/environment=env-z3q9rd/flink-region=aws.us-east-1"
    },
    "secret": "B0BYFzyd0bb5Q58ZZJJYV52mbwDDHnZx21f0gOTz2k6Qv2V9I4KraVztwFOlQx6z"
  }
}
```

```sql
export FLINK_API_KEY="<flink-api-key>"
export FLINK_API_SECRET="<flink-api-secret>"
```

---

### Confluent Documentation | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/concepts/flink-compute-pools.html

Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A fully-managed data streaming platform with a cloud-native Apache Kafka® engine for elastic scaling, enterprise-grade security, stream processing, and governance. Confluent Platform An enterprise-grade distribution of Apache Kafka available on-premises as self-managed software, with enterprise-grade security, stream processing, and governance. Apache Kafka An open-source distributed data streaming engine used by thousands of companies to build streaming data pipelines and applications, powering mission-critical operational and analytics use cases. What's New Try out the latest data streaming platform features, including Confluent Cloud for Apache Flink® and Confluent for VS Code. Confluent Tableflow Represent Apache Kafka® topics as open table formats like Apache Iceberg™ and Delta Lake Confluent Cloud for Apache Flink A cloud-native, serverless service for Apache Flink that enables simple, scalable, and secure stream processing that integrates seamlessly with Kafka Confluent for VS Code Build, manage, and monitor data streaming applications with Confluent for VS Code Release Notes for Confluent Cloud Discover the latest features Confluent Cloud APIs Use the API to manage your deployment Cloud Network Management Learn about the latest networking features Featured Content Get started with the fully-managed Confluent Cloud Quick Start or on-premises using the Confluent Platform ZIP and TAR installation. Confluent Cloud Quick Start Get started with quickly with a fully-managed data streaming platform ZIP and TAR Installation Install Confluent Platform on-premises using ZIP and TAR archive Install Confluent CLI Manage Confluent Platform and Cloud using the command-line interface Confluent Platform Quick Start Use your laptop to quickly get started with Confluent Platform on-premises Get Started with Connect Get started and learn the Kafka Connect basics Cluster Types Learn about the Kafka cluster types available in Confluent Cloud Browse by Feature Find the guides, examples, and tutorials for Confluent features, including the Kafka consumer or the Kafka CLI tools. Kafka Streams Confluent Cloud | Confluent Platform Kafka Connect Confluent Cloud | Confluent Platform Schema Registry Confluent Cloud | Confluent Platform Apache Flink Confluent Cloud | Confluent Platform Networking Confluent Cloud Security Confluent Cloud | Confluent Platform Confluent CLI Confluent Cloud and Confluent Platform Kafka Clients Confluent Cloud | Confluent Platform Apache Kafka Learn more Kafka Consumer Confluent Cloud | Confluent Platform Kafka Producer Confluent Cloud | Confluent Platform Multi-Region Clusters Confluent Platform Stream Lineage Confluent Cloud Terraform Confluent Cloud Confluent for Kubernetes Confluent Platform Ansible Playbooks Confluent Platform Stream Governance Confluent Cloud | Confluent Platform Replicator Confluent Platform Cluster Linking Confluent Cloud | Confluent Platform Auto Data Balancer Confluent Platform WarpStream Learn more More Ways to Learn Explore the video courses available on Confluent Developer. Apache Kafka 101 Learn the fundamentals of Kafka with this video course Kafka Connect 101 Learn the fundamentals of Kafka Connect with this video course Apache Flink SQL Learn how you can use Flink SQL with with Confluent Cloud for Apache Flink

---

### Stream Processing Concepts in Confluent Cloud for Apache Flink | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/concepts/overview.html

Stream Processing Concepts in Confluent Cloud for Apache Flink¶ Apache Flink® SQL, a high-level API powered by Confluent Cloud for Apache Flink, offers a simple and easy way to leverage the power of stream processing. With support for a wide variety of built-in functions, queries, and statements, Flink SQL provides real-time insights into streaming data. Time is a critical element in stream processing, and Flink SQL makes it easy to process data as it arrives, avoiding delays. By using SQL syntax, you can declare expressions that filter, aggregate, route, and mutate streams of data, simplifying your data processing workflows. Stream processing¶ Streams are the de-facto way to create data. Whether the data comprises events from web servers, trades from a stock exchange, or sensor readings from a machine on a factory floor, data is created as part of a stream. When you analyze data, you can either organize your processing around bounded or unbounded streams, and which of these paradigms you choose has significant consequences. Batch processing is the paradigm at work when you process a bounded data stream. In this mode of operation, you can choose to ingest the entire dataset before producing any results, which means that it’s possible, for example, to sort the data, compute global statistics, or produce a final report that summarizes all of the input. Snapshot queries are a type of batch processing query that enables you to process a subset of data from a Kafka topic. Stream processing, on the other hand, involves unbounded data streams. Conceptually, at least, the input may never end, and so you must process the data continuously as it arrives. Bounded and unbounded tables¶ In the context of a Flink table, bounded mode refers to processing data that is finite, which means that the dataset has a clear beginning and end and does not grow continuously or update over time. This is in contrast to unbounded mode, where data arrives as a continuous stream, potentially with no end. The scan.bounded.mode property controls how Flink consumes data from a Kafka topic. A table can be bounded by committed offsets in Kafka brokers of a specific consumer group, by latest offsets, or by a user-supplied timestamp. Key characteristics of bounded mode¶ Finite data: The table represents a static dataset, similar to a traditional table in a relational database or a file in a data lake. Once all records are read, there is no more data to process. Batch processing: Operations on bounded tables are executed in batch mode. This means Flink processes all the available data, computes the results, and then the job finishes. This is suitable for use cases like ETL, reporting, and historical analysis. Optimized execution: Since the system knows the data is finite, it can apply optimizations that are not possible with unbounded (streaming) data. For example, it can sort by any column, perform global aggregations, and use blocking operators. No need for state retention: Unlike streaming mode, where Flink must keep state around to handle late or out-of-order events, batch mode can drop state as soon as it is no longer needed, reducing resource usage. The following table compares the characteristics of bounded and unbounded tables. Aspect Bounded Mode (Batch) Unbounded Mode (Streaming) Data Size Finite (static) Infinite (dynamic, continuous) Processing Style Batch processing Real-time/continuous processing Query Semantics All data available at once Data arrives over time State Management Minimal, can drop state when done Must retain state for late/out-of-order data Use Cases ETL, reporting, historical analytics Real-time analytics, monitoring, alerting Parallel dataflows¶ Programs in Flink are inherently parallel and distributed. During execution, a stream has one or more stream partitions, and each operator has one or more operator subtasks. The operator subtasks are independent of one another, and execute in different threads and possibly on different machines or containers. The number of operator subtasks is the parallelism of that particular operator. Different operators of the same program may have different levels of parallelism. A parallel dataflow in Flink with condensed view (above) and parallelized view (below).¶ Streams can transport data between two operators in a one-to-one (or forwarding) pattern, or in a redistributing pattern: One-to-one streams (for example between the Source and the map() operators in the figure above) preserve the partitioning and ordering of the elements. That means that subtask[1] of the map() operator will see the same elements in the same order as they were produced by subtask[1] of the Source operator. Redistributing streams (as between map() and keyBy/window above, as well as between keyBy/window and Sink) change the partitioning of streams. Each operator subtask sends data to different target subtasks, depending on the selected transformation. Examples are keyBy() (which re-partitions by hashing the key), broadcast(), or rebalance() (which re-partitions randomly). In a redistributing exchange the ordering among the elements is only preserved within each pair of sending and receiving subtasks (for example, subtask[1] of map() and subtask[2] of keyBy/window). So, for example, the redistribution between the keyBy/window and the Sink operators shown above introduces non-determinism regarding the order in which the aggregated results for different keys arrive at the Sink. Timely stream processing¶ For most streaming applications it is very valuable to be able re-process historic data with the same code that is used to process live data - and to produce deterministic, consistent results, regardless. It can also be crucial to pay attention to the order in which events occurred, rather than the order in which they are delivered for processing, and to be able to reason about when a set of events is (or should be) complete. For example, consider the set of events involved in an e-commerce transaction, or financial trade. These requirements for timely stream processing can be met by using event time timestamps that are recorded in the data stream, rather than using the clocks of the machines processing the data. Stateful stream processing¶ Flink operations can be stateful. This means that how one event is handled can depend on the accumulated effect of all the events that came before it. State may be used for something simple, such as counting events per minute to display on a dashboard, or for something more complex, such as computing features for a fraud detection model. A Flink application is run in parallel on a distributed cluster. The various parallel instances of a given operator will execute independently, in separate threads, and in general will be running on different machines. The set of parallel instances of a stateful operator is effectively a sharded key-value store. Each parallel instance is responsible for handling events for a specific group of keys, and the state for those keys is kept locally. The following diagram shows a job running with a parallelism of two across the first three operators in the job graph, terminating in a sink that has a parallelism of one. The third operator is stateful, and a fully-connected network shuffle is occurring between the second and third operators. This is being done to partition the stream by some key, so that all of the events that need to be processed together will be. A Flink job running with a parallelism of two.¶ State is always accessed locally, which helps Flink applications achieve high throughput and low-latency. State management¶ Fault tolerance via state snapshots¶ Flink is able to provide fault-tolerant, exactly-once semantics through a combination of state snapshots and stream replay. These snapshots capture the entire state of the distributed pipeline, recording offsets into the input queues as well as the state throughout the job graph that has resulted from having ingested the data up to that point. When a failure occurs, the sources are rewound, the state is restored, and processing is resumed. As depicted above, these state snapshots are captured asynchronously, without impeding the ongoing processing. Table programs that run in streaming mode leverage all capabilities of Flink as a stateful stream processor. In particular, a table program can be configured with a state backend and various checkpointing options for handling different requirements regarding state size and fault tolerance. It is possible to take a savepoint of a running Table API and SQL pipeline and to restore the application’s state at a later point in time. State usage¶ Due to the declarative nature of Table API & SQL programs, it is not always obvious where and how much state is used within a pipeline. The planner decides whether state is necessary to compute a correct result. A pipeline is optimized to claim as little state as possible given the current set of optimizer rules. Conceptually, source tables are never kept entirely in state. An implementer deals with logical tables, named Tables and Topics. Their state requirements depend on the operations that are in use. Queries such as SELECT ... FROM ... WHERE which consist only of field projections or filters are usually stateless pipelines. But operations like joins, aggregations, or deduplications require keeping intermediate results in a fault-tolerant storage for which Flink state abstractions are used. Refer to the individual operator documentation for more details about how much state is required and how to limit a potentially ever-growing state size. For example, a regular SQL join of two tables requires the operator to keep both input tables in state entirely. For correct SQL semantics, the runtime needs to assume that a match could occur at any point in time from both sides of the join. Flink provides optimized window and interval joins that aim to keep the state size small by exploiting the concept of watermark strategies. Another example is the following query that computes the number of clicks per session. SELECT sessionId, COUNT(*) FROM clicks GROUP BY sessionId; The sessionId attribute is used as a grouping key and the continuous query maintains a count for each sessionId it observes. The sessionId attribute is evolving over time and sessionId values are only active until the session ends, i.e., for a limited period of time. However, the continuous query cannot know about this property of sessionId and expects that every sessionId value can occur at any point of time. It maintains a count for each observed sessionId value. Consequently, the total state size of the query is continuously growing as more and more sessionId values are observed. Dataflow Model¶ Flink implements many techniques from the Dataflow Model. The following articles provide a good introduction to event time and watermark strategies. Blog post: Streaming 101 by Tyler Akidau Dataflow Model Related content¶ Autopilot Comparison with Apache Flink Compute Pools Dynamic Tables Statements Time and Watermarks Time attributes Joins in Continuous Queries Determinism in Continuous Queries Note This website includes content developed at the Apache Software Foundation under the terms of the Apache License v2.

#### Code Examples

```sql
SELECT ... FROM ... WHERE
```

```sql
SELECT sessionId, COUNT(*) FROM clicks GROUP BY sessionId;
```

---

### Confluent Documentation | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/concepts/flink-sql-api-keys.html

Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A fully-managed data streaming platform with a cloud-native Apache Kafka® engine for elastic scaling, enterprise-grade security, stream processing, and governance. Confluent Platform An enterprise-grade distribution of Apache Kafka available on-premises as self-managed software, with enterprise-grade security, stream processing, and governance. Apache Kafka An open-source distributed data streaming engine used by thousands of companies to build streaming data pipelines and applications, powering mission-critical operational and analytics use cases. What's New Try out the latest data streaming platform features, including Confluent Cloud for Apache Flink® and Confluent for VS Code. Confluent Tableflow Represent Apache Kafka® topics as open table formats like Apache Iceberg™ and Delta Lake Confluent Cloud for Apache Flink A cloud-native, serverless service for Apache Flink that enables simple, scalable, and secure stream processing that integrates seamlessly with Kafka Confluent for VS Code Build, manage, and monitor data streaming applications with Confluent for VS Code Release Notes for Confluent Cloud Discover the latest features Confluent Cloud APIs Use the API to manage your deployment Cloud Network Management Learn about the latest networking features Featured Content Get started with the fully-managed Confluent Cloud Quick Start or on-premises using the Confluent Platform ZIP and TAR installation. Confluent Cloud Quick Start Get started with quickly with a fully-managed data streaming platform ZIP and TAR Installation Install Confluent Platform on-premises using ZIP and TAR archive Install Confluent CLI Manage Confluent Platform and Cloud using the command-line interface Confluent Platform Quick Start Use your laptop to quickly get started with Confluent Platform on-premises Get Started with Connect Get started and learn the Kafka Connect basics Cluster Types Learn about the Kafka cluster types available in Confluent Cloud Browse by Feature Find the guides, examples, and tutorials for Confluent features, including the Kafka consumer or the Kafka CLI tools. Kafka Streams Confluent Cloud | Confluent Platform Kafka Connect Confluent Cloud | Confluent Platform Schema Registry Confluent Cloud | Confluent Platform Apache Flink Confluent Cloud | Confluent Platform Networking Confluent Cloud Security Confluent Cloud | Confluent Platform Confluent CLI Confluent Cloud and Confluent Platform Kafka Clients Confluent Cloud | Confluent Platform Apache Kafka Learn more Kafka Consumer Confluent Cloud | Confluent Platform Kafka Producer Confluent Cloud | Confluent Platform Multi-Region Clusters Confluent Platform Stream Lineage Confluent Cloud Terraform Confluent Cloud Confluent for Kubernetes Confluent Platform Ansible Playbooks Confluent Platform Stream Governance Confluent Cloud | Confluent Platform Replicator Confluent Platform Cluster Linking Confluent Cloud | Confluent Platform Auto Data Balancer Confluent Platform WarpStream Learn more More Ways to Learn Explore the video courses available on Confluent Developer. Apache Kafka 101 Learn the fundamentals of Kafka with this video course Kafka Connect 101 Learn the fundamentals of Kafka Connect with this video course Apache Flink SQL Learn how you can use Flink SQL with with Confluent Cloud for Apache Flink

---

### Confluent Documentation | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/concepts/flink-sql-statements.html

Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A fully-managed data streaming platform with a cloud-native Apache Kafka® engine for elastic scaling, enterprise-grade security, stream processing, and governance. Confluent Platform An enterprise-grade distribution of Apache Kafka available on-premises as self-managed software, with enterprise-grade security, stream processing, and governance. Apache Kafka An open-source distributed data streaming engine used by thousands of companies to build streaming data pipelines and applications, powering mission-critical operational and analytics use cases. What's New Try out the latest data streaming platform features, including Confluent Cloud for Apache Flink® and Confluent for VS Code. Confluent Tableflow Represent Apache Kafka® topics as open table formats like Apache Iceberg™ and Delta Lake Confluent Cloud for Apache Flink A cloud-native, serverless service for Apache Flink that enables simple, scalable, and secure stream processing that integrates seamlessly with Kafka Confluent for VS Code Build, manage, and monitor data streaming applications with Confluent for VS Code Release Notes for Confluent Cloud Discover the latest features Confluent Cloud APIs Use the API to manage your deployment Cloud Network Management Learn about the latest networking features Featured Content Get started with the fully-managed Confluent Cloud Quick Start or on-premises using the Confluent Platform ZIP and TAR installation. Confluent Cloud Quick Start Get started with quickly with a fully-managed data streaming platform ZIP and TAR Installation Install Confluent Platform on-premises using ZIP and TAR archive Install Confluent CLI Manage Confluent Platform and Cloud using the command-line interface Confluent Platform Quick Start Use your laptop to quickly get started with Confluent Platform on-premises Get Started with Connect Get started and learn the Kafka Connect basics Cluster Types Learn about the Kafka cluster types available in Confluent Cloud Browse by Feature Find the guides, examples, and tutorials for Confluent features, including the Kafka consumer or the Kafka CLI tools. Kafka Streams Confluent Cloud | Confluent Platform Kafka Connect Confluent Cloud | Confluent Platform Schema Registry Confluent Cloud | Confluent Platform Apache Flink Confluent Cloud | Confluent Platform Networking Confluent Cloud Security Confluent Cloud | Confluent Platform Confluent CLI Confluent Cloud and Confluent Platform Kafka Clients Confluent Cloud | Confluent Platform Apache Kafka Learn more Kafka Consumer Confluent Cloud | Confluent Platform Kafka Producer Confluent Cloud | Confluent Platform Multi-Region Clusters Confluent Platform Stream Lineage Confluent Cloud Terraform Confluent Cloud Confluent for Kubernetes Confluent Platform Ansible Playbooks Confluent Platform Stream Governance Confluent Cloud | Confluent Platform Replicator Confluent Platform Cluster Linking Confluent Cloud | Confluent Platform Auto Data Balancer Confluent Platform WarpStream Learn more More Ways to Learn Explore the video courses available on Confluent Developer. Apache Kafka 101 Learn the fundamentals of Kafka with this video course Kafka Connect 101 Learn the fundamentals of Kafka Connect with this video course Apache Flink SQL Learn how you can use Flink SQL with with Confluent Cloud for Apache Flink

---

### Confluent Documentation | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/concepts/flink-sql-user-defined-functions.html

Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A fully-managed data streaming platform with a cloud-native Apache Kafka® engine for elastic scaling, enterprise-grade security, stream processing, and governance. Confluent Platform An enterprise-grade distribution of Apache Kafka available on-premises as self-managed software, with enterprise-grade security, stream processing, and governance. Apache Kafka An open-source distributed data streaming engine used by thousands of companies to build streaming data pipelines and applications, powering mission-critical operational and analytics use cases. What's New Try out the latest data streaming platform features, including Confluent Cloud for Apache Flink® and Confluent for VS Code. Confluent Tableflow Represent Apache Kafka® topics as open table formats like Apache Iceberg™ and Delta Lake Confluent Cloud for Apache Flink A cloud-native, serverless service for Apache Flink that enables simple, scalable, and secure stream processing that integrates seamlessly with Kafka Confluent for VS Code Build, manage, and monitor data streaming applications with Confluent for VS Code Release Notes for Confluent Cloud Discover the latest features Confluent Cloud APIs Use the API to manage your deployment Cloud Network Management Learn about the latest networking features Featured Content Get started with the fully-managed Confluent Cloud Quick Start or on-premises using the Confluent Platform ZIP and TAR installation. Confluent Cloud Quick Start Get started with quickly with a fully-managed data streaming platform ZIP and TAR Installation Install Confluent Platform on-premises using ZIP and TAR archive Install Confluent CLI Manage Confluent Platform and Cloud using the command-line interface Confluent Platform Quick Start Use your laptop to quickly get started with Confluent Platform on-premises Get Started with Connect Get started and learn the Kafka Connect basics Cluster Types Learn about the Kafka cluster types available in Confluent Cloud Browse by Feature Find the guides, examples, and tutorials for Confluent features, including the Kafka consumer or the Kafka CLI tools. Kafka Streams Confluent Cloud | Confluent Platform Kafka Connect Confluent Cloud | Confluent Platform Schema Registry Confluent Cloud | Confluent Platform Apache Flink Confluent Cloud | Confluent Platform Networking Confluent Cloud Security Confluent Cloud | Confluent Platform Confluent CLI Confluent Cloud and Confluent Platform Kafka Clients Confluent Cloud | Confluent Platform Apache Kafka Learn more Kafka Consumer Confluent Cloud | Confluent Platform Kafka Producer Confluent Cloud | Confluent Platform Multi-Region Clusters Confluent Platform Stream Lineage Confluent Cloud Terraform Confluent Cloud Confluent for Kubernetes Confluent Platform Ansible Playbooks Confluent Platform Stream Governance Confluent Cloud | Confluent Platform Replicator Confluent Platform Cluster Linking Confluent Cloud | Confluent Platform Auto Data Balancer Confluent Platform WarpStream Learn more More Ways to Learn Explore the video courses available on Confluent Developer. Apache Kafka 101 Learn the fundamentals of Kafka with this video course Kafka Connect 101 Learn the fundamentals of Kafka Connect with this video course Apache Flink SQL Learn how you can use Flink SQL with with Confluent Cloud for Apache Flink

---

### Confluent Documentation | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/concepts/flink-kafka-table-mapping.html

Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A fully-managed data streaming platform with a cloud-native Apache Kafka® engine for elastic scaling, enterprise-grade security, stream processing, and governance. Confluent Platform An enterprise-grade distribution of Apache Kafka available on-premises as self-managed software, with enterprise-grade security, stream processing, and governance. Apache Kafka An open-source distributed data streaming engine used by thousands of companies to build streaming data pipelines and applications, powering mission-critical operational and analytics use cases. What's New Try out the latest data streaming platform features, including Confluent Cloud for Apache Flink® and Confluent for VS Code. Confluent Tableflow Represent Apache Kafka® topics as open table formats like Apache Iceberg™ and Delta Lake Confluent Cloud for Apache Flink A cloud-native, serverless service for Apache Flink that enables simple, scalable, and secure stream processing that integrates seamlessly with Kafka Confluent for VS Code Build, manage, and monitor data streaming applications with Confluent for VS Code Release Notes for Confluent Cloud Discover the latest features Confluent Cloud APIs Use the API to manage your deployment Cloud Network Management Learn about the latest networking features Featured Content Get started with the fully-managed Confluent Cloud Quick Start or on-premises using the Confluent Platform ZIP and TAR installation. Confluent Cloud Quick Start Get started with quickly with a fully-managed data streaming platform ZIP and TAR Installation Install Confluent Platform on-premises using ZIP and TAR archive Install Confluent CLI Manage Confluent Platform and Cloud using the command-line interface Confluent Platform Quick Start Use your laptop to quickly get started with Confluent Platform on-premises Get Started with Connect Get started and learn the Kafka Connect basics Cluster Types Learn about the Kafka cluster types available in Confluent Cloud Browse by Feature Find the guides, examples, and tutorials for Confluent features, including the Kafka consumer or the Kafka CLI tools. Kafka Streams Confluent Cloud | Confluent Platform Kafka Connect Confluent Cloud | Confluent Platform Schema Registry Confluent Cloud | Confluent Platform Apache Flink Confluent Cloud | Confluent Platform Networking Confluent Cloud Security Confluent Cloud | Confluent Platform Confluent CLI Confluent Cloud and Confluent Platform Kafka Clients Confluent Cloud | Confluent Platform Apache Kafka Learn more Kafka Consumer Confluent Cloud | Confluent Platform Kafka Producer Confluent Cloud | Confluent Platform Multi-Region Clusters Confluent Platform Stream Lineage Confluent Cloud Terraform Confluent Cloud Confluent for Kubernetes Confluent Platform Ansible Playbooks Confluent Platform Stream Governance Confluent Cloud | Confluent Platform Replicator Confluent Platform Cluster Linking Confluent Cloud | Confluent Platform Auto Data Balancer Confluent Platform WarpStream Learn more More Ways to Learn Explore the video courses available on Confluent Developer. Apache Kafka 101 Learn the fundamentals of Kafka with this video course Kafka Connect 101 Learn the fundamentals of Kafka Connect with this video course Apache Flink SQL Learn how you can use Flink SQL with with Confluent Cloud for Apache Flink

---

### Confluent Documentation | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/concepts/flink-sql-limits.html

Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A fully-managed data streaming platform with a cloud-native Apache Kafka® engine for elastic scaling, enterprise-grade security, stream processing, and governance. Confluent Platform An enterprise-grade distribution of Apache Kafka available on-premises as self-managed software, with enterprise-grade security, stream processing, and governance. Apache Kafka An open-source distributed data streaming engine used by thousands of companies to build streaming data pipelines and applications, powering mission-critical operational and analytics use cases. What's New Try out the latest data streaming platform features, including Confluent Cloud for Apache Flink® and Confluent for VS Code. Confluent Tableflow Represent Apache Kafka® topics as open table formats like Apache Iceberg™ and Delta Lake Confluent Cloud for Apache Flink A cloud-native, serverless service for Apache Flink that enables simple, scalable, and secure stream processing that integrates seamlessly with Kafka Confluent for VS Code Build, manage, and monitor data streaming applications with Confluent for VS Code Release Notes for Confluent Cloud Discover the latest features Confluent Cloud APIs Use the API to manage your deployment Cloud Network Management Learn about the latest networking features Featured Content Get started with the fully-managed Confluent Cloud Quick Start or on-premises using the Confluent Platform ZIP and TAR installation. Confluent Cloud Quick Start Get started with quickly with a fully-managed data streaming platform ZIP and TAR Installation Install Confluent Platform on-premises using ZIP and TAR archive Install Confluent CLI Manage Confluent Platform and Cloud using the command-line interface Confluent Platform Quick Start Use your laptop to quickly get started with Confluent Platform on-premises Get Started with Connect Get started and learn the Kafka Connect basics Cluster Types Learn about the Kafka cluster types available in Confluent Cloud Browse by Feature Find the guides, examples, and tutorials for Confluent features, including the Kafka consumer or the Kafka CLI tools. Kafka Streams Confluent Cloud | Confluent Platform Kafka Connect Confluent Cloud | Confluent Platform Schema Registry Confluent Cloud | Confluent Platform Apache Flink Confluent Cloud | Confluent Platform Networking Confluent Cloud Security Confluent Cloud | Confluent Platform Confluent CLI Confluent Cloud and Confluent Platform Kafka Clients Confluent Cloud | Confluent Platform Apache Kafka Learn more Kafka Consumer Confluent Cloud | Confluent Platform Kafka Producer Confluent Cloud | Confluent Platform Multi-Region Clusters Confluent Platform Stream Lineage Confluent Cloud Terraform Confluent Cloud Confluent for Kubernetes Confluent Platform Ansible Playbooks Confluent Platform Stream Governance Confluent Cloud | Confluent Platform Replicator Confluent Platform Cluster Linking Confluent Cloud | Confluent Platform Auto Data Balancer Confluent Platform WarpStream Learn more More Ways to Learn Explore the video courses available on Confluent Developer. Apache Kafka 101 Learn the fundamentals of Kafka with this video course Kafka Connect 101 Learn the fundamentals of Kafka Connect with this video course Apache Flink SQL Learn how you can use Flink SQL with with Confluent Cloud for Apache Flink

---

### Confluent Documentation | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/concepts/flink-sql-serialization.html

Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A fully-managed data streaming platform with a cloud-native Apache Kafka® engine for elastic scaling, enterprise-grade security, stream processing, and governance. Confluent Platform An enterprise-grade distribution of Apache Kafka available on-premises as self-managed software, with enterprise-grade security, stream processing, and governance. Apache Kafka An open-source distributed data streaming engine used by thousands of companies to build streaming data pipelines and applications, powering mission-critical operational and analytics use cases. What's New Try out the latest data streaming platform features, including Confluent Cloud for Apache Flink® and Confluent for VS Code. Confluent Tableflow Represent Apache Kafka® topics as open table formats like Apache Iceberg™ and Delta Lake Confluent Cloud for Apache Flink A cloud-native, serverless service for Apache Flink that enables simple, scalable, and secure stream processing that integrates seamlessly with Kafka Confluent for VS Code Build, manage, and monitor data streaming applications with Confluent for VS Code Release Notes for Confluent Cloud Discover the latest features Confluent Cloud APIs Use the API to manage your deployment Cloud Network Management Learn about the latest networking features Featured Content Get started with the fully-managed Confluent Cloud Quick Start or on-premises using the Confluent Platform ZIP and TAR installation. Confluent Cloud Quick Start Get started with quickly with a fully-managed data streaming platform ZIP and TAR Installation Install Confluent Platform on-premises using ZIP and TAR archive Install Confluent CLI Manage Confluent Platform and Cloud using the command-line interface Confluent Platform Quick Start Use your laptop to quickly get started with Confluent Platform on-premises Get Started with Connect Get started and learn the Kafka Connect basics Cluster Types Learn about the Kafka cluster types available in Confluent Cloud Browse by Feature Find the guides, examples, and tutorials for Confluent features, including the Kafka consumer or the Kafka CLI tools. Kafka Streams Confluent Cloud | Confluent Platform Kafka Connect Confluent Cloud | Confluent Platform Schema Registry Confluent Cloud | Confluent Platform Apache Flink Confluent Cloud | Confluent Platform Networking Confluent Cloud Security Confluent Cloud | Confluent Platform Confluent CLI Confluent Cloud and Confluent Platform Kafka Clients Confluent Cloud | Confluent Platform Apache Kafka Learn more Kafka Consumer Confluent Cloud | Confluent Platform Kafka Producer Confluent Cloud | Confluent Platform Multi-Region Clusters Confluent Platform Stream Lineage Confluent Cloud Terraform Confluent Cloud Confluent for Kubernetes Confluent Platform Ansible Playbooks Confluent Platform Stream Governance Confluent Cloud | Confluent Platform Replicator Confluent Platform Cluster Linking Confluent Cloud | Confluent Platform Auto Data Balancer Confluent Platform WarpStream Learn more More Ways to Learn Explore the video courses available on Confluent Developer. Apache Kafka 101 Learn the fundamentals of Kafka with this video course Kafka Connect 101 Learn the fundamentals of Kafka Connect with this video course Apache Flink SQL Learn how you can use Flink SQL with with Confluent Cloud for Apache Flink

---

### Confluent Documentation | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/concepts/flink-sql-security.html

Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A fully-managed data streaming platform with a cloud-native Apache Kafka® engine for elastic scaling, enterprise-grade security, stream processing, and governance. Confluent Platform An enterprise-grade distribution of Apache Kafka available on-premises as self-managed software, with enterprise-grade security, stream processing, and governance. Apache Kafka An open-source distributed data streaming engine used by thousands of companies to build streaming data pipelines and applications, powering mission-critical operational and analytics use cases. What's New Try out the latest data streaming platform features, including Confluent Cloud for Apache Flink® and Confluent for VS Code. Confluent Tableflow Represent Apache Kafka® topics as open table formats like Apache Iceberg™ and Delta Lake Confluent Cloud for Apache Flink A cloud-native, serverless service for Apache Flink that enables simple, scalable, and secure stream processing that integrates seamlessly with Kafka Confluent for VS Code Build, manage, and monitor data streaming applications with Confluent for VS Code Release Notes for Confluent Cloud Discover the latest features Confluent Cloud APIs Use the API to manage your deployment Cloud Network Management Learn about the latest networking features Featured Content Get started with the fully-managed Confluent Cloud Quick Start or on-premises using the Confluent Platform ZIP and TAR installation. Confluent Cloud Quick Start Get started with quickly with a fully-managed data streaming platform ZIP and TAR Installation Install Confluent Platform on-premises using ZIP and TAR archive Install Confluent CLI Manage Confluent Platform and Cloud using the command-line interface Confluent Platform Quick Start Use your laptop to quickly get started with Confluent Platform on-premises Get Started with Connect Get started and learn the Kafka Connect basics Cluster Types Learn about the Kafka cluster types available in Confluent Cloud Browse by Feature Find the guides, examples, and tutorials for Confluent features, including the Kafka consumer or the Kafka CLI tools. Kafka Streams Confluent Cloud | Confluent Platform Kafka Connect Confluent Cloud | Confluent Platform Schema Registry Confluent Cloud | Confluent Platform Apache Flink Confluent Cloud | Confluent Platform Networking Confluent Cloud Security Confluent Cloud | Confluent Platform Confluent CLI Confluent Cloud and Confluent Platform Kafka Clients Confluent Cloud | Confluent Platform Apache Kafka Learn more Kafka Consumer Confluent Cloud | Confluent Platform Kafka Producer Confluent Cloud | Confluent Platform Multi-Region Clusters Confluent Platform Stream Lineage Confluent Cloud Terraform Confluent Cloud Confluent for Kubernetes Confluent Platform Ansible Playbooks Confluent Platform Stream Governance Confluent Cloud | Confluent Platform Replicator Confluent Platform Cluster Linking Confluent Cloud | Confluent Platform Auto Data Balancer Confluent Platform WarpStream Learn more More Ways to Learn Explore the video courses available on Confluent Developer. Apache Kafka 101 Learn the fundamentals of Kafka with this video course Kafka Connect 101 Learn the fundamentals of Kafka Connect with this video course Apache Flink SQL Learn how you can use Flink SQL with with Confluent Cloud for Apache Flink

---

### Confluent Documentation | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/concepts/flink-sql-troubleshooting.html

Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A fully-managed data streaming platform with a cloud-native Apache Kafka® engine for elastic scaling, enterprise-grade security, stream processing, and governance. Confluent Platform An enterprise-grade distribution of Apache Kafka available on-premises as self-managed software, with enterprise-grade security, stream processing, and governance. Apache Kafka An open-source distributed data streaming engine used by thousands of companies to build streaming data pipelines and applications, powering mission-critical operational and analytics use cases. What's New Try out the latest data streaming platform features, including Confluent Cloud for Apache Flink® and Confluent for VS Code. Confluent Tableflow Represent Apache Kafka® topics as open table formats like Apache Iceberg™ and Delta Lake Confluent Cloud for Apache Flink A cloud-native, serverless service for Apache Flink that enables simple, scalable, and secure stream processing that integrates seamlessly with Kafka Confluent for VS Code Build, manage, and monitor data streaming applications with Confluent for VS Code Release Notes for Confluent Cloud Discover the latest features Confluent Cloud APIs Use the API to manage your deployment Cloud Network Management Learn about the latest networking features Featured Content Get started with the fully-managed Confluent Cloud Quick Start or on-premises using the Confluent Platform ZIP and TAR installation. Confluent Cloud Quick Start Get started with quickly with a fully-managed data streaming platform ZIP and TAR Installation Install Confluent Platform on-premises using ZIP and TAR archive Install Confluent CLI Manage Confluent Platform and Cloud using the command-line interface Confluent Platform Quick Start Use your laptop to quickly get started with Confluent Platform on-premises Get Started with Connect Get started and learn the Kafka Connect basics Cluster Types Learn about the Kafka cluster types available in Confluent Cloud Browse by Feature Find the guides, examples, and tutorials for Confluent features, including the Kafka consumer or the Kafka CLI tools. Kafka Streams Confluent Cloud | Confluent Platform Kafka Connect Confluent Cloud | Confluent Platform Schema Registry Confluent Cloud | Confluent Platform Apache Flink Confluent Cloud | Confluent Platform Networking Confluent Cloud Security Confluent Cloud | Confluent Platform Confluent CLI Confluent Cloud and Confluent Platform Kafka Clients Confluent Cloud | Confluent Platform Apache Kafka Learn more Kafka Consumer Confluent Cloud | Confluent Platform Kafka Producer Confluent Cloud | Confluent Platform Multi-Region Clusters Confluent Platform Stream Lineage Confluent Cloud Terraform Confluent Cloud Confluent for Kubernetes Confluent Platform Ansible Playbooks Confluent Platform Stream Governance Confluent Cloud | Confluent Platform Replicator Confluent Platform Cluster Linking Confluent Cloud | Confluent Platform Auto Data Balancer Confluent Platform WarpStream Learn more More Ways to Learn Explore the video courses available on Confluent Developer. Apache Kafka 101 Learn the fundamentals of Kafka with this video course Kafka Connect 101 Learn the fundamentals of Kafka Connect with this video course Apache Flink SQL Learn how you can use Flink SQL with with Confluent Cloud for Apache Flink

---

### Confluent Documentation | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/concepts/flink-sql-faq.html

Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A fully-managed data streaming platform with a cloud-native Apache Kafka® engine for elastic scaling, enterprise-grade security, stream processing, and governance. Confluent Platform An enterprise-grade distribution of Apache Kafka available on-premises as self-managed software, with enterprise-grade security, stream processing, and governance. Apache Kafka An open-source distributed data streaming engine used by thousands of companies to build streaming data pipelines and applications, powering mission-critical operational and analytics use cases. What's New Try out the latest data streaming platform features, including Confluent Cloud for Apache Flink® and Confluent for VS Code. Confluent Tableflow Represent Apache Kafka® topics as open table formats like Apache Iceberg™ and Delta Lake Confluent Cloud for Apache Flink A cloud-native, serverless service for Apache Flink that enables simple, scalable, and secure stream processing that integrates seamlessly with Kafka Confluent for VS Code Build, manage, and monitor data streaming applications with Confluent for VS Code Release Notes for Confluent Cloud Discover the latest features Confluent Cloud APIs Use the API to manage your deployment Cloud Network Management Learn about the latest networking features Featured Content Get started with the fully-managed Confluent Cloud Quick Start or on-premises using the Confluent Platform ZIP and TAR installation. Confluent Cloud Quick Start Get started with quickly with a fully-managed data streaming platform ZIP and TAR Installation Install Confluent Platform on-premises using ZIP and TAR archive Install Confluent CLI Manage Confluent Platform and Cloud using the command-line interface Confluent Platform Quick Start Use your laptop to quickly get started with Confluent Platform on-premises Get Started with Connect Get started and learn the Kafka Connect basics Cluster Types Learn about the Kafka cluster types available in Confluent Cloud Browse by Feature Find the guides, examples, and tutorials for Confluent features, including the Kafka consumer or the Kafka CLI tools. Kafka Streams Confluent Cloud | Confluent Platform Kafka Connect Confluent Cloud | Confluent Platform Schema Registry Confluent Cloud | Confluent Platform Apache Flink Confluent Cloud | Confluent Platform Networking Confluent Cloud Security Confluent Cloud | Confluent Platform Confluent CLI Confluent Cloud and Confluent Platform Kafka Clients Confluent Cloud | Confluent Platform Apache Kafka Learn more Kafka Consumer Confluent Cloud | Confluent Platform Kafka Producer Confluent Cloud | Confluent Platform Multi-Region Clusters Confluent Platform Stream Lineage Confluent Cloud Terraform Confluent Cloud Confluent for Kubernetes Confluent Platform Ansible Playbooks Confluent Platform Stream Governance Confluent Cloud | Confluent Platform Replicator Confluent Platform Cluster Linking Confluent Cloud | Confluent Platform Auto Data Balancer Confluent Platform WarpStream Learn more More Ways to Learn Explore the video courses available on Confluent Developer. Apache Kafka 101 Learn the fundamentals of Kafka with this video course Kafka Connect 101 Learn the fundamentals of Kafka Connect with this video course Apache Flink SQL Learn how you can use Flink SQL with with Confluent Cloud for Apache Flink

---

### Get Started with Confluent Cloud for Apache Flink | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/get-started/overview.html

Get Started with Confluent Cloud for Apache Flink¶ Welcome to Confluent Cloud for Apache Flink®. This section guides you through the steps to get your queries running using the Confluent Cloud Console (browser-based) and the Flink SQL shell (CLI-based). Get Started for Free Sign up for a Confluent Cloud trial and get $400 of free credit. If you’re currently using Confluent Cloud in a region that doesn’t yet support Flink, so you can’t use your data in existing Apache Kafka® topics, you can still try out Flink SQL by using sample data generators or the Example catalog, which are used in the quick starts and How-to Guides for Confluent Cloud for Apache Flink. Choose one of the following quick starts to get started with Flink SQL on Confluent Cloud: Flink SQL Quick Start with Confluent Cloud Console Flink SQL Shell Quick Start Also, you can access Flink by using the REST API and the Confluent Terraform Provider. REST API-based data streams Sample Project for Confluent Terraform Provider If you get stuck, have a question, or want to provide feedback or feature requests, don’t hesitate to reach out. Check out Get Help with Confluent Cloud for Apache Flink for our support channels. Next steps¶ Flink SQL Quick Start with Confluent Cloud Console Flink SQL Shell Quick Start Related content¶ Stream Processing Concepts Flink SQL Queries Note This website includes content developed at the Apache Software Foundation under the terms of the Apache License v2.

---

### Confluent Documentation | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/get-started/quick-start-sql.html

Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A fully-managed data streaming platform with a cloud-native Apache Kafka® engine for elastic scaling, enterprise-grade security, stream processing, and governance. Confluent Platform An enterprise-grade distribution of Apache Kafka available on-premises as self-managed software, with enterprise-grade security, stream processing, and governance. Apache Kafka An open-source distributed data streaming engine used by thousands of companies to build streaming data pipelines and applications, powering mission-critical operational and analytics use cases. What's New Try out the latest data streaming platform features, including Confluent Cloud for Apache Flink® and Confluent for VS Code. Confluent Tableflow Represent Apache Kafka® topics as open table formats like Apache Iceberg™ and Delta Lake Confluent Cloud for Apache Flink A cloud-native, serverless service for Apache Flink that enables simple, scalable, and secure stream processing that integrates seamlessly with Kafka Confluent for VS Code Build, manage, and monitor data streaming applications with Confluent for VS Code Release Notes for Confluent Cloud Discover the latest features Confluent Cloud APIs Use the API to manage your deployment Cloud Network Management Learn about the latest networking features Featured Content Get started with the fully-managed Confluent Cloud Quick Start or on-premises using the Confluent Platform ZIP and TAR installation. Confluent Cloud Quick Start Get started with quickly with a fully-managed data streaming platform ZIP and TAR Installation Install Confluent Platform on-premises using ZIP and TAR archive Install Confluent CLI Manage Confluent Platform and Cloud using the command-line interface Confluent Platform Quick Start Use your laptop to quickly get started with Confluent Platform on-premises Get Started with Connect Get started and learn the Kafka Connect basics Cluster Types Learn about the Kafka cluster types available in Confluent Cloud Browse by Feature Find the guides, examples, and tutorials for Confluent features, including the Kafka consumer or the Kafka CLI tools. Kafka Streams Confluent Cloud | Confluent Platform Kafka Connect Confluent Cloud | Confluent Platform Schema Registry Confluent Cloud | Confluent Platform Apache Flink Confluent Cloud | Confluent Platform Networking Confluent Cloud Security Confluent Cloud | Confluent Platform Confluent CLI Confluent Cloud and Confluent Platform Kafka Clients Confluent Cloud | Confluent Platform Apache Kafka Learn more Kafka Consumer Confluent Cloud | Confluent Platform Kafka Producer Confluent Cloud | Confluent Platform Multi-Region Clusters Confluent Platform Stream Lineage Confluent Cloud Terraform Confluent Cloud Confluent for Kubernetes Confluent Platform Ansible Playbooks Confluent Platform Stream Governance Confluent Cloud | Confluent Platform Replicator Confluent Platform Cluster Linking Confluent Cloud | Confluent Platform Auto Data Balancer Confluent Platform WarpStream Learn more More Ways to Learn Explore the video courses available on Confluent Developer. Apache Kafka 101 Learn the fundamentals of Kafka with this video course Kafka Connect 101 Learn the fundamentals of Kafka Connect with this video course Apache Flink SQL Learn how you can use Flink SQL with with Confluent Cloud for Apache Flink

---

### Java Table API Quick Start on Confluent Cloud for Apache Flink | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/get-started/quick-start-java-table-api.html

Java Table API Quick Start on Confluent Cloud for Apache Flink¶ Confluent Cloud for Apache Flink® supports programming applications with the Table API. Confluent provides a plugin for running applications that use the Table API on Confluent Cloud. For more information, see Table API. For code examples, see Java Examples for Table API on Confluent Cloud. For a Confluent Developer course, see Apache Flink Table API: Processing Data Streams in Java. Note The Flink Table API is available for preview. A Preview feature is a Confluent Cloud component that is being introduced to gain early feedback from developers. Preview features can be used for evaluation and non-production testing purposes or to provide feedback to Confluent. The warranty, SLA, and Support Services provisions of your agreement with Confluent do not apply to Preview features. Confluent may discontinue providing preview releases of the Preview features at any time in Confluent’s’ sole discretion. Comments, questions, and suggestions related to the Table API are encouraged and can be submitted through the established channels. Prerequisites¶ Access to Confluent Cloud A compute pool in Confluent Cloud A Apache Kafka® cluster, if you want to run examples that store data in Kafka Java version 11 or later Maven (see Installing Apache Maven) To run Table API and Flink SQL programs, you must generate an API key that’s specific to the Flink environment. Also, you need Confluent Cloud account details, like your organization and environment identifiers. Flink API Key: Follow the steps in Generate a Flink API key. For convenience, assign your Flink key and secret to the FLINK_API_KEY and FLINK_API_SECRET environment variables. Organization ID: The identifier your organization, for example, b0b421724-4586-4a07-b787-d0bb5aacbf87. For convenience, assign your organization identifier to the ORG_ID environment variable. Environment ID: The identifier of the environment where your Flink SQL statements run, for example, env-z3y2x1. For convenience, assign your environment identifier to the ENV_ID environment variable. Cloud provider name: The name of the cloud provider where your cluster runs, for example, aws. To see the available providers, run the confluent flink region list command. For convenience, assign your cloud provider to the CLOUD_PROVIDER environment variable. Cloud region: The name of the region where your cluster runs, for example, us-east-1. To see the available regions, run the confluent flink region list command. For convenience, assign your cloud region to the CLOUD_REGION environment variable. export CLOUD_PROVIDER="aws" export CLOUD_REGION="us-east-1" export FLINK_API_KEY="<your-flink-api-key>" export FLINK_API_SECRET="<your-flink-api-secret>" export ORG_ID="<your-organization-id>" export ENV_ID="<your-environment-id>" export COMPUTE_POOL_ID="<your-compute-pool-id>" Compile and run a Table API program¶ The following code example shows how to run a “Hello World” statement and how to query an example data stream. Copy the following project object model (POM) into a file named pom.xml. pom.xml <project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"> <modelVersion>4.0.0</modelVersion> <groupId>example</groupId> <artifactId>flink-table-api-java-hello-world</artifactId> <version>1.0</version> <packaging>jar</packaging> <name>Apache Flink® Table API Java Hello World Example on Confluent Cloud</name> <properties> <flink.version>1.20.0</flink.version> <confluent-plugin.version>1.20-42</confluent-plugin.version> <target.java.version>11</target.java.version> <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding> <maven.compiler.source>${target.java.version}</maven.compiler.source> <maven.compiler.target>${target.java.version}</maven.compiler.target> <log4j.version>2.17.1</log4j.version> </properties> <repositories> <repository> <id>confluent</id> <url>https://packages.confluent.io/maven/</url> </repository> <repository> <id>apache.snapshots</id> <name>Apache Development Snapshot Repository</name> <url>https://repository.apache.org/content/repositories/snapshots/</url> <releases> <enabled>false</enabled> </releases> <snapshots> <enabled>true</enabled> </snapshots> </repository> </repositories> <dependencies> <!-- Apache Flink dependencies --> <dependency> <groupId>org.apache.flink</groupId> <artifactId>flink-table-api-java</artifactId> <version>${flink.version}</version> </dependency> <!-- Confluent Flink Table API Java plugin --> <dependency> <groupId>io.confluent.flink</groupId> <artifactId>confluent-flink-table-api-java-plugin</artifactId> <version>${confluent-plugin.version}</version> </dependency> <!-- Add logging framework, to produce console output when running in the IDE. --> <!-- These dependencies are excluded from the application JAR by default. --> <dependency> <groupId>org.apache.logging.log4j</groupId> <artifactId>log4j-slf4j-impl</artifactId> <version>${log4j.version}</version> <scope>runtime</scope> </dependency> <dependency> <groupId>org.apache.logging.log4j</groupId> <artifactId>log4j-api</artifactId> <version>${log4j.version}</version> <scope>runtime</scope> </dependency> <dependency> <groupId>org.apache.logging.log4j</groupId> <artifactId>log4j-core</artifactId> <version>${log4j.version}</version> <scope>runtime</scope> </dependency> </dependencies> <build> <sourceDirectory>./example</sourceDirectory> <plugins> <!-- Java Compiler --> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <version>3.10.1</version> <configuration> <source>${target.java.version}</source> <target>${target.java.version}</target> </configuration> </plugin> <!-- We use the maven-shade plugin to create a fat jar that contains all necessary dependencies. --> <!-- Change the value of <mainClass>...</mainClass> if your program entry point changes. --> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-shade-plugin</artifactId> <version>3.4.1</version> <executions> <!-- Run shade goal on package phase --> <execution> <phase>package</phase> <goals> <goal>shade</goal> </goals> <configuration> <artifactSet> <excludes> <exclude>org.apache.flink:flink-shaded-force-shading</exclude> <exclude>com.google.code.findbugs:jsr305</exclude> </excludes> </artifactSet> <filters> <filter> <!-- Do not copy the signatures in the META-INF folder. Otherwise, this might cause SecurityExceptions when using the JAR. --> <artifact>*:*</artifact> <excludes> <exclude>META-INF/*.SF</exclude> <exclude>META-INF/*.DSA</exclude> <exclude>META-INF/*.RSA</exclude> </excludes> </filter> </filters> <transformers> <transformer implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/> <transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"> <mainClass>example.hello_table_api</mainClass> </transformer> </transformers> </configuration> </execution> </executions> </plugin> </plugins> <pluginManagement> <plugins> <!-- This improves the out-of-the-box experience in Eclipse by resolving some warnings. --> <plugin> <groupId>org.eclipse.m2e</groupId> <artifactId>lifecycle-mapping</artifactId> <version>1.0.0</version> <configuration> <lifecycleMappingMetadata> <pluginExecutions> <pluginExecution> <pluginExecutionFilter> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-shade-plugin</artifactId> <versionRange>[3.1.1,)</versionRange> <goals> <goal>shade</goal> </goals> </pluginExecutionFilter> <action> <ignore/> </action> </pluginExecution> <pluginExecution> <pluginExecutionFilter> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <versionRange>[3.1,)</versionRange> <goals> <goal>testCompile</goal> <goal>compile</goal> </goals> </pluginExecutionFilter> <action> <ignore/> </action> </pluginExecution> </pluginExecutions> </lifecycleMappingMetadata> </configuration> </plugin> </plugins> </pluginManagement> </build> </project> Create a directory named “example”. mkdir example Create a file named hello_table_api.java in the example directory. touch example/hello_table_api.java Copy the following code into hello_table_api.java. package example; import io.confluent.flink.plugin.ConfluentSettings; import io.confluent.flink.plugin.ConfluentTools; import org.apache.flink.table.api.EnvironmentSettings; import org.apache.flink.table.api.Table; import org.apache.flink.table.api.TableEnvironment; import org.apache.flink.types.Row; import java.util.List; /** * A table program example to get started with the Apache Flink® Table API. * * <p>It executes two foreground statements in Confluent Cloud. The results of both statements are * printed to the console. */ public class hello_table_api { // All logic is defined in a main() method. It can run both in an IDE or CI/CD system. public static void main(String[] args) { // Set up connection properties to Confluent Cloud. // Use the fromGlobalVariables() method if you assigned environment variables. // EnvironmentSettings settings = ConfluentSettings.fromGlobalVariables(); // Use the fromArgs(args) method if you want to run with command-line arguments. EnvironmentSettings settings = ConfluentSettings.fromArgs(args); // Initialize the session context to get started. TableEnvironment env = TableEnvironment.create(settings); System.out.println("Running with printing..."); // The Table API centers on 'Table' objects, which help in defining data pipelines // fluently. You can define pipelines fully programmatically. Table table = env.fromValues("Hello world!"); // Also, You can define pipelines with embedded Flink SQL. // Table table = env.sqlQuery("SELECT 'Hello world!'"); // Once the pipeline is defined, execute it on Confluent Cloud. // If no target table has been defined, results are streamed back and can be printed // locally. This can be useful for development and debugging. table.execute().print(); System.out.println("Running with collecting..."); // Results can be collected locally and accessed individually. // This can be useful for testing. Table moreHellos = env.fromValues("Hello Bob", "Hello Alice", "Hello Peter").as("greeting"); List<Row> rows = ConfluentTools.collectChangelog(moreHellos, 10); rows.forEach( r -> { String column = r.getFieldAs("greeting"); System.out.println("Greeting: " + column); }); } } Run the following command to build the jar file. mvn clean package Run the jar. If you assigned your cloud configuration to the environment variables specified in the Prerequisites section, and you used the fromGlobalVariables method in the hello_table_api code, you don’t need to provide the command-line options. java -jar target/flink-table-api-java-hello-world-1.0.jar \ --cloud aws \ --region us-east-1 \ --flink-api-key key \ --flink-api-secret secret \ --organization-id b0b21724-4586-4a07-b787-d0bb5aacbf87 \ --environment-id env-z3y2x1 \ --compute-pool-id lfcp-8m03rm Your output should resemble: Running with printing... +----+--------------------------------+ | op | f0 | +----+--------------------------------+ | +I | Hello world! | +----+--------------------------------+ 1 row in set Running with collecting... Greeting: Hello Bob Greeting: Hello Alice Greeting: Hello Peter Next steps¶ Python Table API Quick Start How-to Guides for Confluent Cloud for Apache Flink Related content¶ Course: Apache Flink® Table API: Processing Data Streams in Java GitHub repo: Java Examples for Table API on Confluent Cloud GitHub repo: Python Examples for Table API on Confluent Cloud Built-in Functions Note This website includes content developed at the Apache Software Foundation under the terms of the Apache License v2.

#### Code Examples

```sql
b0b421724-4586-4a07-b787-d0bb5aacbf87
```

```sql
confluent flink region list
```

```sql
confluent flink region list
```

```sql
export CLOUD_PROVIDER="aws"
export CLOUD_REGION="us-east-1"
export FLINK_API_KEY="<your-flink-api-key>"
export FLINK_API_SECRET="<your-flink-api-secret>"
export ORG_ID="<your-organization-id>"
export ENV_ID="<your-environment-id>"
export COMPUTE_POOL_ID="<your-compute-pool-id>"
```

```sql
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
        xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>example</groupId>
    <artifactId>flink-table-api-java-hello-world</artifactId>
    <version>1.0</version>
    <packaging>jar</packaging>

    <name>Apache Flink® Table API Java Hello World Example on Confluent Cloud</name>

    <properties>
        <flink.version>1.20.0</flink.version>
        <confluent-plugin.version>1.20-42</confluent-plugin.version>
        <target.java.version>11</target.java.version>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
        <maven.compiler.source>${target.java.version}</maven.compiler.source>
        <maven.compiler.target>${target.java.version}</maven.compiler.target>
        <log4j.version>2.17.1</log4j.version>
    </properties>

    <repositories>
        <repository>
            <id>confluent</id>
            <url>https://packages.confluent.io/maven/</url>
        </repository>
        <repository>
            <id>apache.snapshots</id>
            <name>Apache Development Snapshot Repository</name>
            <url>https://repository.apache.org/content/repositories/snapshots/</url>
            <releases>
                <enabled>false</enabled>
            </releases>
            <snapshots>
                <enabled>true</enabled>
            </snapshots>
        </repository>
    </repositories>

    <dependencies>
        <!-- Apache Flink dependencies -->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-table-api-java</artifactId>
            <version>${flink.version}</version>
        </dependency>

        <!-- Confluent Flink Table API Java plugin -->
        <dependency>
            <groupId>io.confluent.flink</groupId>
            <artifactId>confluent-flink-table-api-java-plugin</artifactId>
            <version>${confluent-plugin.version}</version>
        </dependency>

        <!-- Add logging framework, to produce console output when running in the IDE. -->
        <!-- These dependencies are excluded from the application JAR by default. -->
        <dependency>
            <groupId>org.apache.logging.log4j</groupId>
            <artifactId>log4j-slf4j-impl</artifactId>
            <version>${log4j.version}</version>
            <scope>runtime</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.logging.log4j</groupId>
            <artifactId>log4j-api</artifactId>
            <version>${log4j.version}</version>
            <scope>runtime</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.logging.log4j</groupId>
            <artifactId>log4j-core</artifactId>
            <version>${log4j.version}</version>
            <scope>runtime</scope>
        </dependency>
    </dependencies>

    <build>
    <sourceDirectory>./example</sourceDirectory>
        <plugins>

            <!-- Java Compiler -->
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>3.10.1</version>
                <configuration>
                    <source>${target.java.version}</source>
                    <target>${target.java.version}</target>
                </configuration>
            </plugin>

            <!-- We use the maven-shade plugin to create a fat jar that contains all necessary dependencies. -->
            <!-- Change the value of <mainClass>...</mainClass> if your program entry point changes. -->
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-shade-plugin</artifactId>
                <version>3.4.1</version>
                <executions>
                    <!-- Run shade goal on package phase -->
                    <execution>
                        <phase>package</phase>
                        <goals>
                            <goal>shade</goal>
                        </goals>
                        <configuration>
                            <artifactSet>
                                <excludes>
                                    <exclude>org.apache.flink:flink-shaded-force-shading</exclude>
                                    <exclude>com.google.code.findbugs:jsr305</exclude>
                                </excludes>
                            </artifactSet>
                            <filters>
                                <filter>
                                    <!-- Do not copy the signatures in the META-INF folder.
                                    Otherwise, this might cause SecurityExceptions when using the JAR. -->
                                    <artifact>*:*</artifact>
                                    <excludes>
                                        <exclude>META-INF/*.SF</exclude>
                                        <exclude>META-INF/*.DSA</exclude>
                                        <exclude>META-INF/*.RSA</exclude>
                                    </excludes>
                                </filter>
                            </filters>
                            <transformers>
                                <transformer
                                        implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/>
                                <transformer
                                        implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                                    <mainClass>example.hello_table_api</mainClass>
                                </transformer>
                            </transformers>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
        </plugins>

        <pluginManagement>
            <plugins>

                <!-- This improves the out-of-the-box experience in Eclipse by resolving some warnings. -->
                <plugin>
                    <groupId>org.eclipse.m2e</groupId>
                    <artifactId>lifecycle-mapping</artifactId>
                    <version>1.0.0</version>
                    <configuration>
                        <lifecycleMappingMetadata>
                            <pluginExecutions>
                                <pluginExecution>
                                    <pluginExecutionFilter>
                                        <groupId>org.apache.maven.plugins</groupId>
                                        <artifactId>maven-shade-plugin</artifactId>
                                        <versionRange>[3.1.1,)</versionRange>
                                        <goals>
                                            <goal>shade</goal>
                                        </goals>
                                    </pluginExecutionFilter>
                                    <action>
                                        <ignore/>
                                    </action>
                                </pluginExecution>
                                <pluginExecution>
                                    <pluginExecutionFilter>
                                        <groupId>org.apache.maven.plugins</groupId>
                                        <artifactId>maven-compiler-plugin</artifactId>
                                        <versionRange>[3.1,)</versionRange>
                                        <goals>
                                            <goal>testCompile</goal>
                                            <goal>compile</goal>
                                        </goals>
                                    </pluginExecutionFilter>
                                    <action>
                                        <ignore/>
                                    </action>
                                </pluginExecution>
                            </pluginExecutions>
                        </lifecycleMappingMetadata>
                    </configuration>
                </plugin>
            </plugins>
        </pluginManagement>
    </build>
</project>
```

```sql
mkdir example
```

```sql
hello_table_api.java
```

```sql
touch example/hello_table_api.java
```

```sql
hello_table_api.java
```

```sql
package example;
import io.confluent.flink.plugin.ConfluentSettings;
import io.confluent.flink.plugin.ConfluentTools;
import org.apache.flink.table.api.EnvironmentSettings;
import org.apache.flink.table.api.Table;
import org.apache.flink.table.api.TableEnvironment;
import org.apache.flink.types.Row;
import java.util.List;

/**
 * A table program example to get started with the Apache Flink® Table API.
 *
 * <p>It executes two foreground statements in Confluent Cloud. The results of both statements are
 * printed to the console.
 */
public class hello_table_api {

    // All logic is defined in a main() method. It can run both in an IDE or CI/CD system.
    public static void main(String[] args) {

        // Set up connection properties to Confluent Cloud.
        // Use the fromGlobalVariables() method if you assigned environment variables.
        // EnvironmentSettings settings = ConfluentSettings.fromGlobalVariables();

        // Use the fromArgs(args) method if you want to run with command-line arguments.
        EnvironmentSettings settings = ConfluentSettings.fromArgs(args);

        // Initialize the session context to get started.
        TableEnvironment env = TableEnvironment.create(settings);

        System.out.println("Running with printing...");

        // The Table API centers on 'Table' objects, which help in defining data pipelines
        // fluently. You can define pipelines fully programmatically.
        Table table = env.fromValues("Hello world!");

        // Also, You can define pipelines with embedded Flink SQL.
        // Table table = env.sqlQuery("SELECT 'Hello world!'");

        // Once the pipeline is defined, execute it on Confluent Cloud.
        // If no target table has been defined, results are streamed back and can be printed
        // locally. This can be useful for development and debugging.
        table.execute().print();

        System.out.println("Running with collecting...");

        // Results can be collected locally and accessed individually.
        // This can be useful for testing.
        Table moreHellos = env.fromValues("Hello Bob", "Hello Alice", "Hello Peter").as("greeting");
        List<Row> rows = ConfluentTools.collectChangelog(moreHellos, 10);
        rows.forEach(
                r -> {
                    String column = r.getFieldAs("greeting");
                    System.out.println("Greeting: " + column);
                });
    }
}
```

```sql
mvn clean package
```

```sql
fromGlobalVariables
```

```sql
hello_table_api
```

```sql
java -jar target/flink-table-api-java-hello-world-1.0.jar \
  --cloud aws \
  --region us-east-1 \
  --flink-api-key key \
  --flink-api-secret secret \
  --organization-id b0b21724-4586-4a07-b787-d0bb5aacbf87 \
  --environment-id env-z3y2x1 \
  --compute-pool-id lfcp-8m03rm
```

```sql
Running with printing...
+----+--------------------------------+
| op |                             f0 |
+----+--------------------------------+
| +I |                   Hello world! |
+----+--------------------------------+
1 row in set
Running with collecting...
Greeting: Hello Bob
Greeting: Hello Alice
Greeting: Hello Peter
```

---

### Python Table API Quick Start on Confluent Cloud for Apache Flink | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/get-started/quick-start-python-table-api.html

Python Table API Quick Start on Confluent Cloud for Apache Flink¶ Confluent Cloud for Apache Flink® supports programming applications with the Table API. Confluent provides a plugin for running applications that use the Table API on Confluent Cloud. For more information, see Table API. For code examples, see Python Examples for Table API on Confluent Cloud. Note The Flink Table API is available for preview. A Preview feature is a Confluent Cloud component that is being introduced to gain early feedback from developers. Preview features can be used for evaluation and non-production testing purposes or to provide feedback to Confluent. The warranty, SLA, and Support Services provisions of your agreement with Confluent do not apply to Preview features. Confluent may discontinue providing preview releases of the Preview features at any time in Confluent’s’ sole discretion. Comments, questions, and suggestions related to the Table API are encouraged and can be submitted through the established channels. Prerequisites¶ Access to Confluent Cloud A compute pool in Confluent Cloud A Apache Kafka® cluster, if you want to run examples that store data in Kafka Java version 11 or later To run Table API and Flink SQL programs, you must generate an API key that’s specific to the Flink environment. Also, you need Confluent Cloud account details, like your organization and environment identifiers. Flink API Key: Follow the steps in Generate a Flink API key. For convenience, assign your Flink key and secret to the FLINK_API_KEY and FLINK_API_SECRET environment variables. Organization ID: The identifier your organization, for example, b0b421724-4586-4a07-b787-d0bb5aacbf87. For convenience, assign your organization identifier to the ORG_ID environment variable. Environment ID: The identifier of the environment where your Flink SQL statements run, for example, env-z3y2x1. For convenience, assign your environment identifier to the ENV_ID environment variable. Cloud provider name: The name of the cloud provider where your cluster runs, for example, aws. To see the available providers, run the confluent flink region list command. For convenience, assign your cloud provider to the CLOUD_PROVIDER environment variable. Cloud region: The name of the region where your cluster runs, for example, us-east-1. To see the available regions, run the confluent flink region list command. For convenience, assign your cloud region to the CLOUD_REGION environment variable. export CLOUD_PROVIDER="aws" export CLOUD_REGION="us-east-1" export FLINK_API_KEY="<your-flink-api-key>" export FLINK_API_SECRET="<your-flink-api-secret>" export ORG_ID="<your-organization-id>" export ENV_ID="<your-environment-id>" export COMPUTE_POOL_ID="<your-compute-pool-id>" Compile and run a Table API program¶ Use poetry to create a virtual environment that contains all required dependencies and project files. Follow the instructions here to install pipx. Run the following command to install poetry. pipx install poetry Copy the following code into a file named hello_table_api.py. from pyflink.table.confluent import ConfluentSettings, ConfluentTools from pyflink.table import TableEnvironment, Row from pyflink.table.expressions import col, row def run(): # Set up the connection to Confluent Cloud settings = ConfluentSettings.from_file("/cloud.properties") env = TableEnvironment.create(settings) # Run your first Flink statement in Table API env.from_elements([row("Hello world!")]).execute().print() # Or use SQL env.sql_query("SELECT 'Hello world!'").execute().print() # Structure your code with Table objects - the main ingredient of Table API. table = env.from_path("examples.marketplace.clicks") \ .filter(col("user_agent").like("Mozilla%")) \ .select(col("click_id"), col("user_id")) table.print_schema() print(table.explain()) # Use the provided tools to test on a subset of the streaming data expected = ConfluentTools.collect_materialized_limit(table, 50) actual = [Row(42, 500)] if expected != actual: print("Results don't match!") if __name__ == "__main__": run() In the directory where you created hello_table_api.py, run the following command to build a virtual environment containing all required dependencies and project files. poetry install Run the following command to execute the Table API program. poetry run hello_table_api.py Next steps¶ Java Table API Quick Start How-to Guides for Confluent Cloud for Apache Flink Related content¶ GitHub repo: Java Examples for Table API on Confluent Cloud GitHub repo: Python Examples for Table API on Confluent Cloud Built-in Functions Note This website includes content developed at the Apache Software Foundation under the terms of the Apache License v2.

#### Code Examples

```sql
b0b421724-4586-4a07-b787-d0bb5aacbf87
```

```sql
confluent flink region list
```

```sql
confluent flink region list
```

```sql
export CLOUD_PROVIDER="aws"
export CLOUD_REGION="us-east-1"
export FLINK_API_KEY="<your-flink-api-key>"
export FLINK_API_SECRET="<your-flink-api-secret>"
export ORG_ID="<your-organization-id>"
export ENV_ID="<your-environment-id>"
export COMPUTE_POOL_ID="<your-compute-pool-id>"
```

```sql
pipx install poetry
```

```sql
hello_table_api.py
```

```sql
from pyflink.table.confluent import ConfluentSettings, ConfluentTools
from pyflink.table import TableEnvironment, Row
from pyflink.table.expressions import col, row

def run():
    # Set up the connection to Confluent Cloud
    settings = ConfluentSettings.from_file("/cloud.properties")
    env = TableEnvironment.create(settings)

    # Run your first Flink statement in Table API
    env.from_elements([row("Hello world!")]).execute().print()

    # Or use SQL
    env.sql_query("SELECT 'Hello world!'").execute().print()

    # Structure your code with Table objects - the main ingredient of Table API.
    table = env.from_path("examples.marketplace.clicks") \
        .filter(col("user_agent").like("Mozilla%")) \
        .select(col("click_id"), col("user_id"))

    table.print_schema()
    print(table.explain())

    # Use the provided tools to test on a subset of the streaming data
    expected = ConfluentTools.collect_materialized_limit(table, 50)
    actual = [Row(42, 500)]
    if expected != actual:
        print("Results don't match!")

if __name__ == "__main__":
    run()
```

```sql
hello_table_api.py
```

```sql
poetry install
```

```sql
poetry run hello_table_api.py
```

---

### Confluent Documentation | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/get-started/quick-start-confluent-cli.html

Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A fully-managed data streaming platform with a cloud-native Apache Kafka® engine for elastic scaling, enterprise-grade security, stream processing, and governance. Confluent Platform An enterprise-grade distribution of Apache Kafka available on-premises as self-managed software, with enterprise-grade security, stream processing, and governance. Apache Kafka An open-source distributed data streaming engine used by thousands of companies to build streaming data pipelines and applications, powering mission-critical operational and analytics use cases. What's New Try out the latest data streaming platform features, including Confluent Cloud for Apache Flink® and Confluent for VS Code. Confluent Tableflow Represent Apache Kafka® topics as open table formats like Apache Iceberg™ and Delta Lake Confluent Cloud for Apache Flink A cloud-native, serverless service for Apache Flink that enables simple, scalable, and secure stream processing that integrates seamlessly with Kafka Confluent for VS Code Build, manage, and monitor data streaming applications with Confluent for VS Code Release Notes for Confluent Cloud Discover the latest features Confluent Cloud APIs Use the API to manage your deployment Cloud Network Management Learn about the latest networking features Featured Content Get started with the fully-managed Confluent Cloud Quick Start or on-premises using the Confluent Platform ZIP and TAR installation. Confluent Cloud Quick Start Get started with quickly with a fully-managed data streaming platform ZIP and TAR Installation Install Confluent Platform on-premises using ZIP and TAR archive Install Confluent CLI Manage Confluent Platform and Cloud using the command-line interface Confluent Platform Quick Start Use your laptop to quickly get started with Confluent Platform on-premises Get Started with Connect Get started and learn the Kafka Connect basics Cluster Types Learn about the Kafka cluster types available in Confluent Cloud Browse by Feature Find the guides, examples, and tutorials for Confluent features, including the Kafka consumer or the Kafka CLI tools. Kafka Streams Confluent Cloud | Confluent Platform Kafka Connect Confluent Cloud | Confluent Platform Schema Registry Confluent Cloud | Confluent Platform Apache Flink Confluent Cloud | Confluent Platform Networking Confluent Cloud Security Confluent Cloud | Confluent Platform Confluent CLI Confluent Cloud and Confluent Platform Kafka Clients Confluent Cloud | Confluent Platform Apache Kafka Learn more Kafka Consumer Confluent Cloud | Confluent Platform Kafka Producer Confluent Cloud | Confluent Platform Multi-Region Clusters Confluent Platform Stream Lineage Confluent Cloud Terraform Confluent Cloud Confluent for Kubernetes Confluent Platform Ansible Playbooks Confluent Platform Stream Governance Confluent Cloud | Confluent Platform Replicator Confluent Platform Cluster Linking Confluent Cloud | Confluent Platform Auto Data Balancer Confluent Platform WarpStream Learn more More Ways to Learn Explore the video courses available on Confluent Developer. Apache Kafka 101 Learn the fundamentals of Kafka with this video course Kafka Connect 101 Learn the fundamentals of Kafka Connect with this video course Apache Flink SQL Learn how you can use Flink SQL with with Confluent Cloud for Apache Flink

---

### Confluent Documentation | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/get-started/quick-start-rest-api.html

Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A fully-managed data streaming platform with a cloud-native Apache Kafka® engine for elastic scaling, enterprise-grade security, stream processing, and governance. Confluent Platform An enterprise-grade distribution of Apache Kafka available on-premises as self-managed software, with enterprise-grade security, stream processing, and governance. Apache Kafka An open-source distributed data streaming engine used by thousands of companies to build streaming data pipelines and applications, powering mission-critical operational and analytics use cases. What's New Try out the latest data streaming platform features, including Confluent Cloud for Apache Flink® and Confluent for VS Code. Confluent Tableflow Represent Apache Kafka® topics as open table formats like Apache Iceberg™ and Delta Lake Confluent Cloud for Apache Flink A cloud-native, serverless service for Apache Flink that enables simple, scalable, and secure stream processing that integrates seamlessly with Kafka Confluent for VS Code Build, manage, and monitor data streaming applications with Confluent for VS Code Release Notes for Confluent Cloud Discover the latest features Confluent Cloud APIs Use the API to manage your deployment Cloud Network Management Learn about the latest networking features Featured Content Get started with the fully-managed Confluent Cloud Quick Start or on-premises using the Confluent Platform ZIP and TAR installation. Confluent Cloud Quick Start Get started with quickly with a fully-managed data streaming platform ZIP and TAR Installation Install Confluent Platform on-premises using ZIP and TAR archive Install Confluent CLI Manage Confluent Platform and Cloud using the command-line interface Confluent Platform Quick Start Use your laptop to quickly get started with Confluent Platform on-premises Get Started with Connect Get started and learn the Kafka Connect basics Cluster Types Learn about the Kafka cluster types available in Confluent Cloud Browse by Feature Find the guides, examples, and tutorials for Confluent features, including the Kafka consumer or the Kafka CLI tools. Kafka Streams Confluent Cloud | Confluent Platform Kafka Connect Confluent Cloud | Confluent Platform Schema Registry Confluent Cloud | Confluent Platform Apache Flink Confluent Cloud | Confluent Platform Networking Confluent Cloud Security Confluent Cloud | Confluent Platform Confluent CLI Confluent Cloud and Confluent Platform Kafka Clients Confluent Cloud | Confluent Platform Apache Kafka Learn more Kafka Consumer Confluent Cloud | Confluent Platform Kafka Producer Confluent Cloud | Confluent Platform Multi-Region Clusters Confluent Platform Stream Lineage Confluent Cloud Terraform Confluent Cloud Confluent for Kubernetes Confluent Platform Ansible Playbooks Confluent Platform Stream Governance Confluent Cloud | Confluent Platform Replicator Confluent Platform Cluster Linking Confluent Cloud | Confluent Platform Auto Data Balancer Confluent Platform WarpStream Learn more More Ways to Learn Explore the video courses available on Confluent Developer. Apache Kafka 101 Learn the fundamentals of Kafka with this video course Kafka Connect 101 Learn the fundamentals of Kafka Connect with this video course Apache Flink SQL Learn how you can use Flink SQL with with Confluent Cloud for Apache Flink

---

### Confluent Documentation | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/get-started/flink-sql-vs-code.html

Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A fully-managed data streaming platform with a cloud-native Apache Kafka® engine for elastic scaling, enterprise-grade security, stream processing, and governance. Confluent Platform An enterprise-grade distribution of Apache Kafka available on-premises as self-managed software, with enterprise-grade security, stream processing, and governance. Apache Kafka An open-source distributed data streaming engine used by thousands of companies to build streaming data pipelines and applications, powering mission-critical operational and analytics use cases. What's New Try out the latest data streaming platform features, including Confluent Cloud for Apache Flink® and Confluent for VS Code. Confluent Tableflow Represent Apache Kafka® topics as open table formats like Apache Iceberg™ and Delta Lake Confluent Cloud for Apache Flink A cloud-native, serverless service for Apache Flink that enables simple, scalable, and secure stream processing that integrates seamlessly with Kafka Confluent for VS Code Build, manage, and monitor data streaming applications with Confluent for VS Code Release Notes for Confluent Cloud Discover the latest features Confluent Cloud APIs Use the API to manage your deployment Cloud Network Management Learn about the latest networking features Featured Content Get started with the fully-managed Confluent Cloud Quick Start or on-premises using the Confluent Platform ZIP and TAR installation. Confluent Cloud Quick Start Get started with quickly with a fully-managed data streaming platform ZIP and TAR Installation Install Confluent Platform on-premises using ZIP and TAR archive Install Confluent CLI Manage Confluent Platform and Cloud using the command-line interface Confluent Platform Quick Start Use your laptop to quickly get started with Confluent Platform on-premises Get Started with Connect Get started and learn the Kafka Connect basics Cluster Types Learn about the Kafka cluster types available in Confluent Cloud Browse by Feature Find the guides, examples, and tutorials for Confluent features, including the Kafka consumer or the Kafka CLI tools. Kafka Streams Confluent Cloud | Confluent Platform Kafka Connect Confluent Cloud | Confluent Platform Schema Registry Confluent Cloud | Confluent Platform Apache Flink Confluent Cloud | Confluent Platform Networking Confluent Cloud Security Confluent Cloud | Confluent Platform Confluent CLI Confluent Cloud and Confluent Platform Kafka Clients Confluent Cloud | Confluent Platform Apache Kafka Learn more Kafka Consumer Confluent Cloud | Confluent Platform Kafka Producer Confluent Cloud | Confluent Platform Multi-Region Clusters Confluent Platform Stream Lineage Confluent Cloud Terraform Confluent Cloud Confluent for Kubernetes Confluent Platform Ansible Playbooks Confluent Platform Stream Governance Confluent Cloud | Confluent Platform Replicator Confluent Platform Cluster Linking Confluent Cloud | Confluent Platform Auto Data Balancer Confluent Platform WarpStream Learn more More Ways to Learn Explore the video courses available on Confluent Developer. Apache Kafka 101 Learn the fundamentals of Kafka with this video course Kafka Connect 101 Learn the fundamentals of Kafka Connect with this video course Apache Flink SQL Learn how you can use Flink SQL with with Confluent Cloud for Apache Flink

---

### Confluent Documentation | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/get-started/flink-sql-cloud-console.html

Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A fully-managed data streaming platform with a cloud-native Apache Kafka® engine for elastic scaling, enterprise-grade security, stream processing, and governance. Confluent Platform An enterprise-grade distribution of Apache Kafka available on-premises as self-managed software, with enterprise-grade security, stream processing, and governance. Apache Kafka An open-source distributed data streaming engine used by thousands of companies to build streaming data pipelines and applications, powering mission-critical operational and analytics use cases. What's New Try out the latest data streaming platform features, including Confluent Cloud for Apache Flink® and Confluent for VS Code. Confluent Tableflow Represent Apache Kafka® topics as open table formats like Apache Iceberg™ and Delta Lake Confluent Cloud for Apache Flink A cloud-native, serverless service for Apache Flink that enables simple, scalable, and secure stream processing that integrates seamlessly with Kafka Confluent for VS Code Build, manage, and monitor data streaming applications with Confluent for VS Code Release Notes for Confluent Cloud Discover the latest features Confluent Cloud APIs Use the API to manage your deployment Cloud Network Management Learn about the latest networking features Featured Content Get started with the fully-managed Confluent Cloud Quick Start or on-premises using the Confluent Platform ZIP and TAR installation. Confluent Cloud Quick Start Get started with quickly with a fully-managed data streaming platform ZIP and TAR Installation Install Confluent Platform on-premises using ZIP and TAR archive Install Confluent CLI Manage Confluent Platform and Cloud using the command-line interface Confluent Platform Quick Start Use your laptop to quickly get started with Confluent Platform on-premises Get Started with Connect Get started and learn the Kafka Connect basics Cluster Types Learn about the Kafka cluster types available in Confluent Cloud Browse by Feature Find the guides, examples, and tutorials for Confluent features, including the Kafka consumer or the Kafka CLI tools. Kafka Streams Confluent Cloud | Confluent Platform Kafka Connect Confluent Cloud | Confluent Platform Schema Registry Confluent Cloud | Confluent Platform Apache Flink Confluent Cloud | Confluent Platform Networking Confluent Cloud Security Confluent Cloud | Confluent Platform Confluent CLI Confluent Cloud and Confluent Platform Kafka Clients Confluent Cloud | Confluent Platform Apache Kafka Learn more Kafka Consumer Confluent Cloud | Confluent Platform Kafka Producer Confluent Cloud | Confluent Platform Multi-Region Clusters Confluent Platform Stream Lineage Confluent Cloud Terraform Confluent Cloud Confluent for Kubernetes Confluent Platform Ansible Playbooks Confluent Platform Stream Governance Confluent Cloud | Confluent Platform Replicator Confluent Platform Cluster Linking Confluent Cloud | Confluent Platform Auto Data Balancer Confluent Platform WarpStream Learn more More Ways to Learn Explore the video courses available on Confluent Developer. Apache Kafka 101 Learn the fundamentals of Kafka with this video course Kafka Connect 101 Learn the fundamentals of Kafka Connect with this video course Apache Flink SQL Learn how you can use Flink SQL with with Confluent Cloud for Apache Flink

---

### Confluent Documentation | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/get-started/flink-sql-confluent-cli.html

Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A fully-managed data streaming platform with a cloud-native Apache Kafka® engine for elastic scaling, enterprise-grade security, stream processing, and governance. Confluent Platform An enterprise-grade distribution of Apache Kafka available on-premises as self-managed software, with enterprise-grade security, stream processing, and governance. Apache Kafka An open-source distributed data streaming engine used by thousands of companies to build streaming data pipelines and applications, powering mission-critical operational and analytics use cases. What's New Try out the latest data streaming platform features, including Confluent Cloud for Apache Flink® and Confluent for VS Code. Confluent Tableflow Represent Apache Kafka® topics as open table formats like Apache Iceberg™ and Delta Lake Confluent Cloud for Apache Flink A cloud-native, serverless service for Apache Flink that enables simple, scalable, and secure stream processing that integrates seamlessly with Kafka Confluent for VS Code Build, manage, and monitor data streaming applications with Confluent for VS Code Release Notes for Confluent Cloud Discover the latest features Confluent Cloud APIs Use the API to manage your deployment Cloud Network Management Learn about the latest networking features Featured Content Get started with the fully-managed Confluent Cloud Quick Start or on-premises using the Confluent Platform ZIP and TAR installation. Confluent Cloud Quick Start Get started with quickly with a fully-managed data streaming platform ZIP and TAR Installation Install Confluent Platform on-premises using ZIP and TAR archive Install Confluent CLI Manage Confluent Platform and Cloud using the command-line interface Confluent Platform Quick Start Use your laptop to quickly get started with Confluent Platform on-premises Get Started with Connect Get started and learn the Kafka Connect basics Cluster Types Learn about the Kafka cluster types available in Confluent Cloud Browse by Feature Find the guides, examples, and tutorials for Confluent features, including the Kafka consumer or the Kafka CLI tools. Kafka Streams Confluent Cloud | Confluent Platform Kafka Connect Confluent Cloud | Confluent Platform Schema Registry Confluent Cloud | Confluent Platform Apache Flink Confluent Cloud | Confluent Platform Networking Confluent Cloud Security Confluent Cloud | Confluent Platform Confluent CLI Confluent Cloud and Confluent Platform Kafka Clients Confluent Cloud | Confluent Platform Apache Kafka Learn more Kafka Consumer Confluent Cloud | Confluent Platform Kafka Producer Confluent Cloud | Confluent Platform Multi-Region Clusters Confluent Platform Stream Lineage Confluent Cloud Terraform Confluent Cloud Confluent for Kubernetes Confluent Platform Ansible Playbooks Confluent Platform Stream Governance Confluent Cloud | Confluent Platform Replicator Confluent Platform Cluster Linking Confluent Cloud | Confluent Platform Auto Data Balancer Confluent Platform WarpStream Learn more More Ways to Learn Explore the video courses available on Confluent Developer. Apache Kafka 101 Learn the fundamentals of Kafka with this video course Kafka Connect 101 Learn the fundamentals of Kafka Connect with this video course Apache Flink SQL Learn how you can use Flink SQL with with Confluent Cloud for Apache Flink

---

### Confluent Documentation | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/get-started/flink-sql-rest-api.html

Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A fully-managed data streaming platform with a cloud-native Apache Kafka® engine for elastic scaling, enterprise-grade security, stream processing, and governance. Confluent Platform An enterprise-grade distribution of Apache Kafka available on-premises as self-managed software, with enterprise-grade security, stream processing, and governance. Apache Kafka An open-source distributed data streaming engine used by thousands of companies to build streaming data pipelines and applications, powering mission-critical operational and analytics use cases. What's New Try out the latest data streaming platform features, including Confluent Cloud for Apache Flink® and Confluent for VS Code. Confluent Tableflow Represent Apache Kafka® topics as open table formats like Apache Iceberg™ and Delta Lake Confluent Cloud for Apache Flink A cloud-native, serverless service for Apache Flink that enables simple, scalable, and secure stream processing that integrates seamlessly with Kafka Confluent for VS Code Build, manage, and monitor data streaming applications with Confluent for VS Code Release Notes for Confluent Cloud Discover the latest features Confluent Cloud APIs Use the API to manage your deployment Cloud Network Management Learn about the latest networking features Featured Content Get started with the fully-managed Confluent Cloud Quick Start or on-premises using the Confluent Platform ZIP and TAR installation. Confluent Cloud Quick Start Get started with quickly with a fully-managed data streaming platform ZIP and TAR Installation Install Confluent Platform on-premises using ZIP and TAR archive Install Confluent CLI Manage Confluent Platform and Cloud using the command-line interface Confluent Platform Quick Start Use your laptop to quickly get started with Confluent Platform on-premises Get Started with Connect Get started and learn the Kafka Connect basics Cluster Types Learn about the Kafka cluster types available in Confluent Cloud Browse by Feature Find the guides, examples, and tutorials for Confluent features, including the Kafka consumer or the Kafka CLI tools. Kafka Streams Confluent Cloud | Confluent Platform Kafka Connect Confluent Cloud | Confluent Platform Schema Registry Confluent Cloud | Confluent Platform Apache Flink Confluent Cloud | Confluent Platform Networking Confluent Cloud Security Confluent Cloud | Confluent Platform Confluent CLI Confluent Cloud and Confluent Platform Kafka Clients Confluent Cloud | Confluent Platform Apache Kafka Learn more Kafka Consumer Confluent Cloud | Confluent Platform Kafka Producer Confluent Cloud | Confluent Platform Multi-Region Clusters Confluent Platform Stream Lineage Confluent Cloud Terraform Confluent Cloud Confluent for Kubernetes Confluent Platform Ansible Playbooks Confluent Platform Stream Governance Confluent Cloud | Confluent Platform Replicator Confluent Platform Cluster Linking Confluent Cloud | Confluent Platform Auto Data Balancer Confluent Platform WarpStream Learn more More Ways to Learn Explore the video courses available on Confluent Developer. Apache Kafka 101 Learn the fundamentals of Kafka with this video course Kafka Connect 101 Learn the fundamentals of Kafka Connect with this video course Apache Flink SQL Learn how you can use Flink SQL with with Confluent Cloud for Apache Flink

---

### Confluent Documentation | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/get-started/flink-sql-notebooks.html

Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A fully-managed data streaming platform with a cloud-native Apache Kafka® engine for elastic scaling, enterprise-grade security, stream processing, and governance. Confluent Platform An enterprise-grade distribution of Apache Kafka available on-premises as self-managed software, with enterprise-grade security, stream processing, and governance. Apache Kafka An open-source distributed data streaming engine used by thousands of companies to build streaming data pipelines and applications, powering mission-critical operational and analytics use cases. What's New Try out the latest data streaming platform features, including Confluent Cloud for Apache Flink® and Confluent for VS Code. Confluent Tableflow Represent Apache Kafka® topics as open table formats like Apache Iceberg™ and Delta Lake Confluent Cloud for Apache Flink A cloud-native, serverless service for Apache Flink that enables simple, scalable, and secure stream processing that integrates seamlessly with Kafka Confluent for VS Code Build, manage, and monitor data streaming applications with Confluent for VS Code Release Notes for Confluent Cloud Discover the latest features Confluent Cloud APIs Use the API to manage your deployment Cloud Network Management Learn about the latest networking features Featured Content Get started with the fully-managed Confluent Cloud Quick Start or on-premises using the Confluent Platform ZIP and TAR installation. Confluent Cloud Quick Start Get started with quickly with a fully-managed data streaming platform ZIP and TAR Installation Install Confluent Platform on-premises using ZIP and TAR archive Install Confluent CLI Manage Confluent Platform and Cloud using the command-line interface Confluent Platform Quick Start Use your laptop to quickly get started with Confluent Platform on-premises Get Started with Connect Get started and learn the Kafka Connect basics Cluster Types Learn about the Kafka cluster types available in Confluent Cloud Browse by Feature Find the guides, examples, and tutorials for Confluent features, including the Kafka consumer or the Kafka CLI tools. Kafka Streams Confluent Cloud | Confluent Platform Kafka Connect Confluent Cloud | Confluent Platform Schema Registry Confluent Cloud | Confluent Platform Apache Flink Confluent Cloud | Confluent Platform Networking Confluent Cloud Security Confluent Cloud | Confluent Platform Confluent CLI Confluent Cloud and Confluent Platform Kafka Clients Confluent Cloud | Confluent Platform Apache Kafka Learn more Kafka Consumer Confluent Cloud | Confluent Platform Kafka Producer Confluent Cloud | Confluent Platform Multi-Region Clusters Confluent Platform Stream Lineage Confluent Cloud Terraform Confluent Cloud Confluent for Kubernetes Confluent Platform Ansible Playbooks Confluent Platform Stream Governance Confluent Cloud | Confluent Platform Replicator Confluent Platform Cluster Linking Confluent Cloud | Confluent Platform Auto Data Balancer Confluent Platform WarpStream Learn more More Ways to Learn Explore the video courses available on Confluent Developer. Apache Kafka 101 Learn the fundamentals of Kafka with this video course Kafka Connect 101 Learn the fundamentals of Kafka Connect with this video course Apache Flink SQL Learn how you can use Flink SQL with with Confluent Cloud for Apache Flink

---

### Confluent Documentation | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/get-started/flink-sql-terraform.html

Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A fully-managed data streaming platform with a cloud-native Apache Kafka® engine for elastic scaling, enterprise-grade security, stream processing, and governance. Confluent Platform An enterprise-grade distribution of Apache Kafka available on-premises as self-managed software, with enterprise-grade security, stream processing, and governance. Apache Kafka An open-source distributed data streaming engine used by thousands of companies to build streaming data pipelines and applications, powering mission-critical operational and analytics use cases. What's New Try out the latest data streaming platform features, including Confluent Cloud for Apache Flink® and Confluent for VS Code. Confluent Tableflow Represent Apache Kafka® topics as open table formats like Apache Iceberg™ and Delta Lake Confluent Cloud for Apache Flink A cloud-native, serverless service for Apache Flink that enables simple, scalable, and secure stream processing that integrates seamlessly with Kafka Confluent for VS Code Build, manage, and monitor data streaming applications with Confluent for VS Code Release Notes for Confluent Cloud Discover the latest features Confluent Cloud APIs Use the API to manage your deployment Cloud Network Management Learn about the latest networking features Featured Content Get started with the fully-managed Confluent Cloud Quick Start or on-premises using the Confluent Platform ZIP and TAR installation. Confluent Cloud Quick Start Get started with quickly with a fully-managed data streaming platform ZIP and TAR Installation Install Confluent Platform on-premises using ZIP and TAR archive Install Confluent CLI Manage Confluent Platform and Cloud using the command-line interface Confluent Platform Quick Start Use your laptop to quickly get started with Confluent Platform on-premises Get Started with Connect Get started and learn the Kafka Connect basics Cluster Types Learn about the Kafka cluster types available in Confluent Cloud Browse by Feature Find the guides, examples, and tutorials for Confluent features, including the Kafka consumer or the Kafka CLI tools. Kafka Streams Confluent Cloud | Confluent Platform Kafka Connect Confluent Cloud | Confluent Platform Schema Registry Confluent Cloud | Confluent Platform Apache Flink Confluent Cloud | Confluent Platform Networking Confluent Cloud Security Confluent Cloud | Confluent Platform Confluent CLI Confluent Cloud and Confluent Platform Kafka Clients Confluent Cloud | Confluent Platform Apache Kafka Learn more Kafka Consumer Confluent Cloud | Confluent Platform Kafka Producer Confluent Cloud | Confluent Platform Multi-Region Clusters Confluent Platform Stream Lineage Confluent Cloud Terraform Confluent Cloud Confluent for Kubernetes Confluent Platform Ansible Playbooks Confluent Platform Stream Governance Confluent Cloud | Confluent Platform Replicator Confluent Platform Cluster Linking Confluent Cloud | Confluent Platform Auto Data Balancer Confluent Platform WarpStream Learn more More Ways to Learn Explore the video courses available on Confluent Developer. Apache Kafka 101 Learn the fundamentals of Kafka with this video course Kafka Connect 101 Learn the fundamentals of Kafka Connect with this video course Apache Flink SQL Learn how you can use Flink SQL with with Confluent Cloud for Apache Flink

---

### Confluent Documentation | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/get-started/flink-sql-compute-pools.html

Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A fully-managed data streaming platform with a cloud-native Apache Kafka® engine for elastic scaling, enterprise-grade security, stream processing, and governance. Confluent Platform An enterprise-grade distribution of Apache Kafka available on-premises as self-managed software, with enterprise-grade security, stream processing, and governance. Apache Kafka An open-source distributed data streaming engine used by thousands of companies to build streaming data pipelines and applications, powering mission-critical operational and analytics use cases. What's New Try out the latest data streaming platform features, including Confluent Cloud for Apache Flink® and Confluent for VS Code. Confluent Tableflow Represent Apache Kafka® topics as open table formats like Apache Iceberg™ and Delta Lake Confluent Cloud for Apache Flink A cloud-native, serverless service for Apache Flink that enables simple, scalable, and secure stream processing that integrates seamlessly with Kafka Confluent for VS Code Build, manage, and monitor data streaming applications with Confluent for VS Code Release Notes for Confluent Cloud Discover the latest features Confluent Cloud APIs Use the API to manage your deployment Cloud Network Management Learn about the latest networking features Featured Content Get started with the fully-managed Confluent Cloud Quick Start or on-premises using the Confluent Platform ZIP and TAR installation. Confluent Cloud Quick Start Get started with quickly with a fully-managed data streaming platform ZIP and TAR Installation Install Confluent Platform on-premises using ZIP and TAR archive Install Confluent CLI Manage Confluent Platform and Cloud using the command-line interface Confluent Platform Quick Start Use your laptop to quickly get started with Confluent Platform on-premises Get Started with Connect Get started and learn the Kafka Connect basics Cluster Types Learn about the Kafka cluster types available in Confluent Cloud Browse by Feature Find the guides, examples, and tutorials for Confluent features, including the Kafka consumer or the Kafka CLI tools. Kafka Streams Confluent Cloud | Confluent Platform Kafka Connect Confluent Cloud | Confluent Platform Schema Registry Confluent Cloud | Confluent Platform Apache Flink Confluent Cloud | Confluent Platform Networking Confluent Cloud Security Confluent Cloud | Confluent Platform Confluent CLI Confluent Cloud and Confluent Platform Kafka Clients Confluent Cloud | Confluent Platform Apache Kafka Learn more Kafka Consumer Confluent Cloud | Confluent Platform Kafka Producer Confluent Cloud | Confluent Platform Multi-Region Clusters Confluent Platform Stream Lineage Confluent Cloud Terraform Confluent Cloud Confluent for Kubernetes Confluent Platform Ansible Playbooks Confluent Platform Stream Governance Confluent Cloud | Confluent Platform Replicator Confluent Platform Cluster Linking Confluent Cloud | Confluent Platform Auto Data Balancer Confluent Platform WarpStream Learn more More Ways to Learn Explore the video courses available on Confluent Developer. Apache Kafka 101 Learn the fundamentals of Kafka with this video course Kafka Connect 101 Learn the fundamentals of Kafka Connect with this video course Apache Flink SQL Learn how you can use Flink SQL with with Confluent Cloud for Apache Flink

---

### Confluent Documentation | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/get-started/flink-sql-statements.html

Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A fully-managed data streaming platform with a cloud-native Apache Kafka® engine for elastic scaling, enterprise-grade security, stream processing, and governance. Confluent Platform An enterprise-grade distribution of Apache Kafka available on-premises as self-managed software, with enterprise-grade security, stream processing, and governance. Apache Kafka An open-source distributed data streaming engine used by thousands of companies to build streaming data pipelines and applications, powering mission-critical operational and analytics use cases. What's New Try out the latest data streaming platform features, including Confluent Cloud for Apache Flink® and Confluent for VS Code. Confluent Tableflow Represent Apache Kafka® topics as open table formats like Apache Iceberg™ and Delta Lake Confluent Cloud for Apache Flink A cloud-native, serverless service for Apache Flink that enables simple, scalable, and secure stream processing that integrates seamlessly with Kafka Confluent for VS Code Build, manage, and monitor data streaming applications with Confluent for VS Code Release Notes for Confluent Cloud Discover the latest features Confluent Cloud APIs Use the API to manage your deployment Cloud Network Management Learn about the latest networking features Featured Content Get started with the fully-managed Confluent Cloud Quick Start or on-premises using the Confluent Platform ZIP and TAR installation. Confluent Cloud Quick Start Get started with quickly with a fully-managed data streaming platform ZIP and TAR Installation Install Confluent Platform on-premises using ZIP and TAR archive Install Confluent CLI Manage Confluent Platform and Cloud using the command-line interface Confluent Platform Quick Start Use your laptop to quickly get started with Confluent Platform on-premises Get Started with Connect Get started and learn the Kafka Connect basics Cluster Types Learn about the Kafka cluster types available in Confluent Cloud Browse by Feature Find the guides, examples, and tutorials for Confluent features, including the Kafka consumer or the Kafka CLI tools. Kafka Streams Confluent Cloud | Confluent Platform Kafka Connect Confluent Cloud | Confluent Platform Schema Registry Confluent Cloud | Confluent Platform Apache Flink Confluent Cloud | Confluent Platform Networking Confluent Cloud Security Confluent Cloud | Confluent Platform Confluent CLI Confluent Cloud and Confluent Platform Kafka Clients Confluent Cloud | Confluent Platform Apache Kafka Learn more Kafka Consumer Confluent Cloud | Confluent Platform Kafka Producer Confluent Cloud | Confluent Platform Multi-Region Clusters Confluent Platform Stream Lineage Confluent Cloud Terraform Confluent Cloud Confluent for Kubernetes Confluent Platform Ansible Playbooks Confluent Platform Stream Governance Confluent Cloud | Confluent Platform Replicator Confluent Platform Cluster Linking Confluent Cloud | Confluent Platform Auto Data Balancer Confluent Platform WarpStream Learn more More Ways to Learn Explore the video courses available on Confluent Developer. Apache Kafka 101 Learn the fundamentals of Kafka with this video course Kafka Connect 101 Learn the fundamentals of Kafka Connect with this video course Apache Flink SQL Learn how you can use Flink SQL with with Confluent Cloud for Apache Flink

---

### Confluent Documentation | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/get-started/flink-sql-user-defined-functions.html

Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A fully-managed data streaming platform with a cloud-native Apache Kafka® engine for elastic scaling, enterprise-grade security, stream processing, and governance. Confluent Platform An enterprise-grade distribution of Apache Kafka available on-premises as self-managed software, with enterprise-grade security, stream processing, and governance. Apache Kafka An open-source distributed data streaming engine used by thousands of companies to build streaming data pipelines and applications, powering mission-critical operational and analytics use cases. What's New Try out the latest data streaming platform features, including Confluent Cloud for Apache Flink® and Confluent for VS Code. Confluent Tableflow Represent Apache Kafka® topics as open table formats like Apache Iceberg™ and Delta Lake Confluent Cloud for Apache Flink A cloud-native, serverless service for Apache Flink that enables simple, scalable, and secure stream processing that integrates seamlessly with Kafka Confluent for VS Code Build, manage, and monitor data streaming applications with Confluent for VS Code Release Notes for Confluent Cloud Discover the latest features Confluent Cloud APIs Use the API to manage your deployment Cloud Network Management Learn about the latest networking features Featured Content Get started with the fully-managed Confluent Cloud Quick Start or on-premises using the Confluent Platform ZIP and TAR installation. Confluent Cloud Quick Start Get started with quickly with a fully-managed data streaming platform ZIP and TAR Installation Install Confluent Platform on-premises using ZIP and TAR archive Install Confluent CLI Manage Confluent Platform and Cloud using the command-line interface Confluent Platform Quick Start Use your laptop to quickly get started with Confluent Platform on-premises Get Started with Connect Get started and learn the Kafka Connect basics Cluster Types Learn about the Kafka cluster types available in Confluent Cloud Browse by Feature Find the guides, examples, and tutorials for Confluent features, including the Kafka consumer or the Kafka CLI tools. Kafka Streams Confluent Cloud | Confluent Platform Kafka Connect Confluent Cloud | Confluent Platform Schema Registry Confluent Cloud | Confluent Platform Apache Flink Confluent Cloud | Confluent Platform Networking Confluent Cloud Security Confluent Cloud | Confluent Platform Confluent CLI Confluent Cloud and Confluent Platform Kafka Clients Confluent Cloud | Confluent Platform Apache Kafka Learn more Kafka Consumer Confluent Cloud | Confluent Platform Kafka Producer Confluent Cloud | Confluent Platform Multi-Region Clusters Confluent Platform Stream Lineage Confluent Cloud Terraform Confluent Cloud Confluent for Kubernetes Confluent Platform Ansible Playbooks Confluent Platform Stream Governance Confluent Cloud | Confluent Platform Replicator Confluent Platform Cluster Linking Confluent Cloud | Confluent Platform Auto Data Balancer Confluent Platform WarpStream Learn more More Ways to Learn Explore the video courses available on Confluent Developer. Apache Kafka 101 Learn the fundamentals of Kafka with this video course Kafka Connect 101 Learn the fundamentals of Kafka Connect with this video course Apache Flink SQL Learn how you can use Flink SQL with with Confluent Cloud for Apache Flink

---

### Confluent Documentation | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/get-started/flink-sql-security.html

Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A fully-managed data streaming platform with a cloud-native Apache Kafka® engine for elastic scaling, enterprise-grade security, stream processing, and governance. Confluent Platform An enterprise-grade distribution of Apache Kafka available on-premises as self-managed software, with enterprise-grade security, stream processing, and governance. Apache Kafka An open-source distributed data streaming engine used by thousands of companies to build streaming data pipelines and applications, powering mission-critical operational and analytics use cases. What's New Try out the latest data streaming platform features, including Confluent Cloud for Apache Flink® and Confluent for VS Code. Confluent Tableflow Represent Apache Kafka® topics as open table formats like Apache Iceberg™ and Delta Lake Confluent Cloud for Apache Flink A cloud-native, serverless service for Apache Flink that enables simple, scalable, and secure stream processing that integrates seamlessly with Kafka Confluent for VS Code Build, manage, and monitor data streaming applications with Confluent for VS Code Release Notes for Confluent Cloud Discover the latest features Confluent Cloud APIs Use the API to manage your deployment Cloud Network Management Learn about the latest networking features Featured Content Get started with the fully-managed Confluent Cloud Quick Start or on-premises using the Confluent Platform ZIP and TAR installation. Confluent Cloud Quick Start Get started with quickly with a fully-managed data streaming platform ZIP and TAR Installation Install Confluent Platform on-premises using ZIP and TAR archive Install Confluent CLI Manage Confluent Platform and Cloud using the command-line interface Confluent Platform Quick Start Use your laptop to quickly get started with Confluent Platform on-premises Get Started with Connect Get started and learn the Kafka Connect basics Cluster Types Learn about the Kafka cluster types available in Confluent Cloud Browse by Feature Find the guides, examples, and tutorials for Confluent features, including the Kafka consumer or the Kafka CLI tools. Kafka Streams Confluent Cloud | Confluent Platform Kafka Connect Confluent Cloud | Confluent Platform Schema Registry Confluent Cloud | Confluent Platform Apache Flink Confluent Cloud | Confluent Platform Networking Confluent Cloud Security Confluent Cloud | Confluent Platform Confluent CLI Confluent Cloud and Confluent Platform Kafka Clients Confluent Cloud | Confluent Platform Apache Kafka Learn more Kafka Consumer Confluent Cloud | Confluent Platform Kafka Producer Confluent Cloud | Confluent Platform Multi-Region Clusters Confluent Platform Stream Lineage Confluent Cloud Terraform Confluent Cloud Confluent for Kubernetes Confluent Platform Ansible Playbooks Confluent Platform Stream Governance Confluent Cloud | Confluent Platform Replicator Confluent Platform Cluster Linking Confluent Cloud | Confluent Platform Auto Data Balancer Confluent Platform WarpStream Learn more More Ways to Learn Explore the video courses available on Confluent Developer. Apache Kafka 101 Learn the fundamentals of Kafka with this video course Kafka Connect 101 Learn the fundamentals of Kafka Connect with this video course Apache Flink SQL Learn how you can use Flink SQL with with Confluent Cloud for Apache Flink

---

### Confluent Documentation | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/get-started/flink-sql-private-networking.html

Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A fully-managed data streaming platform with a cloud-native Apache Kafka® engine for elastic scaling, enterprise-grade security, stream processing, and governance. Confluent Platform An enterprise-grade distribution of Apache Kafka available on-premises as self-managed software, with enterprise-grade security, stream processing, and governance. Apache Kafka An open-source distributed data streaming engine used by thousands of companies to build streaming data pipelines and applications, powering mission-critical operational and analytics use cases. What's New Try out the latest data streaming platform features, including Confluent Cloud for Apache Flink® and Confluent for VS Code. Confluent Tableflow Represent Apache Kafka® topics as open table formats like Apache Iceberg™ and Delta Lake Confluent Cloud for Apache Flink A cloud-native, serverless service for Apache Flink that enables simple, scalable, and secure stream processing that integrates seamlessly with Kafka Confluent for VS Code Build, manage, and monitor data streaming applications with Confluent for VS Code Release Notes for Confluent Cloud Discover the latest features Confluent Cloud APIs Use the API to manage your deployment Cloud Network Management Learn about the latest networking features Featured Content Get started with the fully-managed Confluent Cloud Quick Start or on-premises using the Confluent Platform ZIP and TAR installation. Confluent Cloud Quick Start Get started with quickly with a fully-managed data streaming platform ZIP and TAR Installation Install Confluent Platform on-premises using ZIP and TAR archive Install Confluent CLI Manage Confluent Platform and Cloud using the command-line interface Confluent Platform Quick Start Use your laptop to quickly get started with Confluent Platform on-premises Get Started with Connect Get started and learn the Kafka Connect basics Cluster Types Learn about the Kafka cluster types available in Confluent Cloud Browse by Feature Find the guides, examples, and tutorials for Confluent features, including the Kafka consumer or the Kafka CLI tools. Kafka Streams Confluent Cloud | Confluent Platform Kafka Connect Confluent Cloud | Confluent Platform Schema Registry Confluent Cloud | Confluent Platform Apache Flink Confluent Cloud | Confluent Platform Networking Confluent Cloud Security Confluent Cloud | Confluent Platform Confluent CLI Confluent Cloud and Confluent Platform Kafka Clients Confluent Cloud | Confluent Platform Apache Kafka Learn more Kafka Consumer Confluent Cloud | Confluent Platform Kafka Producer Confluent Cloud | Confluent Platform Multi-Region Clusters Confluent Platform Stream Lineage Confluent Cloud Terraform Confluent Cloud Confluent for Kubernetes Confluent Platform Ansible Playbooks Confluent Platform Stream Governance Confluent Cloud | Confluent Platform Replicator Confluent Platform Cluster Linking Confluent Cloud | Confluent Platform Auto Data Balancer Confluent Platform WarpStream Learn more More Ways to Learn Explore the video courses available on Confluent Developer. Apache Kafka 101 Learn the fundamentals of Kafka with this video course Kafka Connect 101 Learn the fundamentals of Kafka Connect with this video course Apache Flink SQL Learn how you can use Flink SQL with with Confluent Cloud for Apache Flink

---

### Confluent Documentation | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/get-started/flink-sql-rbac.html

Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A fully-managed data streaming platform with a cloud-native Apache Kafka® engine for elastic scaling, enterprise-grade security, stream processing, and governance. Confluent Platform An enterprise-grade distribution of Apache Kafka available on-premises as self-managed software, with enterprise-grade security, stream processing, and governance. Apache Kafka An open-source distributed data streaming engine used by thousands of companies to build streaming data pipelines and applications, powering mission-critical operational and analytics use cases. What's New Try out the latest data streaming platform features, including Confluent Cloud for Apache Flink® and Confluent for VS Code. Confluent Tableflow Represent Apache Kafka® topics as open table formats like Apache Iceberg™ and Delta Lake Confluent Cloud for Apache Flink A cloud-native, serverless service for Apache Flink that enables simple, scalable, and secure stream processing that integrates seamlessly with Kafka Confluent for VS Code Build, manage, and monitor data streaming applications with Confluent for VS Code Release Notes for Confluent Cloud Discover the latest features Confluent Cloud APIs Use the API to manage your deployment Cloud Network Management Learn about the latest networking features Featured Content Get started with the fully-managed Confluent Cloud Quick Start or on-premises using the Confluent Platform ZIP and TAR installation. Confluent Cloud Quick Start Get started with quickly with a fully-managed data streaming platform ZIP and TAR Installation Install Confluent Platform on-premises using ZIP and TAR archive Install Confluent CLI Manage Confluent Platform and Cloud using the command-line interface Confluent Platform Quick Start Use your laptop to quickly get started with Confluent Platform on-premises Get Started with Connect Get started and learn the Kafka Connect basics Cluster Types Learn about the Kafka cluster types available in Confluent Cloud Browse by Feature Find the guides, examples, and tutorials for Confluent features, including the Kafka consumer or the Kafka CLI tools. Kafka Streams Confluent Cloud | Confluent Platform Kafka Connect Confluent Cloud | Confluent Platform Schema Registry Confluent Cloud | Confluent Platform Apache Flink Confluent Cloud | Confluent Platform Networking Confluent Cloud Security Confluent Cloud | Confluent Platform Confluent CLI Confluent Cloud and Confluent Platform Kafka Clients Confluent Cloud | Confluent Platform Apache Kafka Learn more Kafka Consumer Confluent Cloud | Confluent Platform Kafka Producer Confluent Cloud | Confluent Platform Multi-Region Clusters Confluent Platform Stream Lineage Confluent Cloud Terraform Confluent Cloud Confluent for Kubernetes Confluent Platform Ansible Playbooks Confluent Platform Stream Governance Confluent Cloud | Confluent Platform Replicator Confluent Platform Cluster Linking Confluent Cloud | Confluent Platform Auto Data Balancer Confluent Platform WarpStream Learn more More Ways to Learn Explore the video courses available on Confluent Developer. Apache Kafka 101 Learn the fundamentals of Kafka with this video course Kafka Connect 101 Learn the fundamentals of Kafka Connect with this video course Apache Flink SQL Learn how you can use Flink SQL with with Confluent Cloud for Apache Flink

---

### Confluent Documentation | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/get-started/flink-sql-troubleshooting.html

Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A fully-managed data streaming platform with a cloud-native Apache Kafka® engine for elastic scaling, enterprise-grade security, stream processing, and governance. Confluent Platform An enterprise-grade distribution of Apache Kafka available on-premises as self-managed software, with enterprise-grade security, stream processing, and governance. Apache Kafka An open-source distributed data streaming engine used by thousands of companies to build streaming data pipelines and applications, powering mission-critical operational and analytics use cases. What's New Try out the latest data streaming platform features, including Confluent Cloud for Apache Flink® and Confluent for VS Code. Confluent Tableflow Represent Apache Kafka® topics as open table formats like Apache Iceberg™ and Delta Lake Confluent Cloud for Apache Flink A cloud-native, serverless service for Apache Flink that enables simple, scalable, and secure stream processing that integrates seamlessly with Kafka Confluent for VS Code Build, manage, and monitor data streaming applications with Confluent for VS Code Release Notes for Confluent Cloud Discover the latest features Confluent Cloud APIs Use the API to manage your deployment Cloud Network Management Learn about the latest networking features Featured Content Get started with the fully-managed Confluent Cloud Quick Start or on-premises using the Confluent Platform ZIP and TAR installation. Confluent Cloud Quick Start Get started with quickly with a fully-managed data streaming platform ZIP and TAR Installation Install Confluent Platform on-premises using ZIP and TAR archive Install Confluent CLI Manage Confluent Platform and Cloud using the command-line interface Confluent Platform Quick Start Use your laptop to quickly get started with Confluent Platform on-premises Get Started with Connect Get started and learn the Kafka Connect basics Cluster Types Learn about the Kafka cluster types available in Confluent Cloud Browse by Feature Find the guides, examples, and tutorials for Confluent features, including the Kafka consumer or the Kafka CLI tools. Kafka Streams Confluent Cloud | Confluent Platform Kafka Connect Confluent Cloud | Confluent Platform Schema Registry Confluent Cloud | Confluent Platform Apache Flink Confluent Cloud | Confluent Platform Networking Confluent Cloud Security Confluent Cloud | Confluent Platform Confluent CLI Confluent Cloud and Confluent Platform Kafka Clients Confluent Cloud | Confluent Platform Apache Kafka Learn more Kafka Consumer Confluent Cloud | Confluent Platform Kafka Producer Confluent Cloud | Confluent Platform Multi-Region Clusters Confluent Platform Stream Lineage Confluent Cloud Terraform Confluent Cloud Confluent for Kubernetes Confluent Platform Ansible Playbooks Confluent Platform Stream Governance Confluent Cloud | Confluent Platform Replicator Confluent Platform Cluster Linking Confluent Cloud | Confluent Platform Auto Data Balancer Confluent Platform WarpStream Learn more More Ways to Learn Explore the video courses available on Confluent Developer. Apache Kafka 101 Learn the fundamentals of Kafka with this video course Kafka Connect 101 Learn the fundamentals of Kafka Connect with this video course Apache Flink SQL Learn how you can use Flink SQL with with Confluent Cloud for Apache Flink

---

### Confluent Documentation | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/get-started/flink-sql-faq.html

Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A fully-managed data streaming platform with a cloud-native Apache Kafka® engine for elastic scaling, enterprise-grade security, stream processing, and governance. Confluent Platform An enterprise-grade distribution of Apache Kafka available on-premises as self-managed software, with enterprise-grade security, stream processing, and governance. Apache Kafka An open-source distributed data streaming engine used by thousands of companies to build streaming data pipelines and applications, powering mission-critical operational and analytics use cases. What's New Try out the latest data streaming platform features, including Confluent Cloud for Apache Flink® and Confluent for VS Code. Confluent Tableflow Represent Apache Kafka® topics as open table formats like Apache Iceberg™ and Delta Lake Confluent Cloud for Apache Flink A cloud-native, serverless service for Apache Flink that enables simple, scalable, and secure stream processing that integrates seamlessly with Kafka Confluent for VS Code Build, manage, and monitor data streaming applications with Confluent for VS Code Release Notes for Confluent Cloud Discover the latest features Confluent Cloud APIs Use the API to manage your deployment Cloud Network Management Learn about the latest networking features Featured Content Get started with the fully-managed Confluent Cloud Quick Start or on-premises using the Confluent Platform ZIP and TAR installation. Confluent Cloud Quick Start Get started with quickly with a fully-managed data streaming platform ZIP and TAR Installation Install Confluent Platform on-premises using ZIP and TAR archive Install Confluent CLI Manage Confluent Platform and Cloud using the command-line interface Confluent Platform Quick Start Use your laptop to quickly get started with Confluent Platform on-premises Get Started with Connect Get started and learn the Kafka Connect basics Cluster Types Learn about the Kafka cluster types available in Confluent Cloud Browse by Feature Find the guides, examples, and tutorials for Confluent features, including the Kafka consumer or the Kafka CLI tools. Kafka Streams Confluent Cloud | Confluent Platform Kafka Connect Confluent Cloud | Confluent Platform Schema Registry Confluent Cloud | Confluent Platform Apache Flink Confluent Cloud | Confluent Platform Networking Confluent Cloud Security Confluent Cloud | Confluent Platform Confluent CLI Confluent Cloud and Confluent Platform Kafka Clients Confluent Cloud | Confluent Platform Apache Kafka Learn more Kafka Consumer Confluent Cloud | Confluent Platform Kafka Producer Confluent Cloud | Confluent Platform Multi-Region Clusters Confluent Platform Stream Lineage Confluent Cloud Terraform Confluent Cloud Confluent for Kubernetes Confluent Platform Ansible Playbooks Confluent Platform Stream Governance Confluent Cloud | Confluent Platform Replicator Confluent Platform Cluster Linking Confluent Cloud | Confluent Platform Auto Data Balancer Confluent Platform WarpStream Learn more More Ways to Learn Explore the video courses available on Confluent Developer. Apache Kafka 101 Learn the fundamentals of Kafka with this video course Kafka Connect 101 Learn the fundamentals of Kafka Connect with this video course Apache Flink SQL Learn how you can use Flink SQL with with Confluent Cloud for Apache Flink

---

### Get Help with Confluent Cloud for Apache Flink | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/get-help.html

Get Help with Confluent Cloud for Apache Flink¶ You can request support in the Confluent Support Portal. You can access the portal directly, or you can navigate to it from the Confluent Cloud Console by selecting the Support menu identified by the help icon () in the upper-right and choosing Support portal. For more information, see Confluent Support for Confluent Cloud. Confluent Community Slack¶ There is a dedicated #flink channel in the Confluent Community Slack. Join with this link to ask questions, provide feedback, and engage with other users. Troubleshoot Flink in Confluent Cloud Console¶ If issues occur while running Flink in Cloud Console, consider generating a HAR file and uploading it to the Confluent Community Slack channel or sending it to the Support Portal. For more information, see Generate a HAR file for Troubleshooting on Confluent Cloud. Note This website includes content developed at the Apache Software Foundation under the terms of the Apache License v2.

---

### How-to Guides for Developing Flink Applications on Confluent Cloud for Apache Flink | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/how-to-guides/overview.html

How-to Guides for Confluent Cloud for Apache Flink¶ Discover how Confluent Cloud for Apache Flink® can help you accomplish common processing tasks such as joins and aggregations. This section provides step-by-step guidance on how to use Flink to process your data efficiently and effectively. Aggregate a Stream in a Tumbling Window Combine Streams and Track Most Recent Records Compare Current and Previous Values in a Data Stream Convert the Serialization Format of a Topic Create a User Defined Function Handle Multiple Event Types Process Schemaless Events Resolve Common SQL Query Problems Run a Snapshot Query Scan and Summarize Tables View Time Series Data Flink actions¶ Confluent Cloud for Apache Flink provides Flink Actions that enable you to perform specific data-processing tasks on topics with minimal configuration. These actions are designed to simplify common workloads by providing a user-friendly interface to configure and execute them. Create an Embedding: Convert data in a topic’s column into a vector embedding for AI model inference. Deduplicate Rows in a Table: Remove duplicate records from a topic based on specified fields, ensuring that only unique records are retained in the output topic. Mask Fields in a Table: Mask sensitive data in specified fields of a topic by replacing the original data with a static value. Transform a Topic: Change a topic’s properties by applying custom Flink SQL transformations. Related content¶ Video: How to Set Idle Timeouts Video: How to Analyze Data from a REST API with Flink SQL Video: How To Use Streaming Joins with Apache Flink Video: How to Visualize Real-Time Data from Apache Kafka using Apache Flink SQL and Streamlit Use Flink SQL with Kafka, Streamlit, and the Alpaca API Note This website includes content developed at the Apache Software Foundation under the terms of the Apache License v2.

---

### Confluent Documentation | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/how-to-guides/create-flink-sql-table.html

Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A fully-managed data streaming platform with a cloud-native Apache Kafka® engine for elastic scaling, enterprise-grade security, stream processing, and governance. Confluent Platform An enterprise-grade distribution of Apache Kafka available on-premises as self-managed software, with enterprise-grade security, stream processing, and governance. Apache Kafka An open-source distributed data streaming engine used by thousands of companies to build streaming data pipelines and applications, powering mission-critical operational and analytics use cases. What's New Try out the latest data streaming platform features, including Confluent Cloud for Apache Flink® and Confluent for VS Code. Confluent Tableflow Represent Apache Kafka® topics as open table formats like Apache Iceberg™ and Delta Lake Confluent Cloud for Apache Flink A cloud-native, serverless service for Apache Flink that enables simple, scalable, and secure stream processing that integrates seamlessly with Kafka Confluent for VS Code Build, manage, and monitor data streaming applications with Confluent for VS Code Release Notes for Confluent Cloud Discover the latest features Confluent Cloud APIs Use the API to manage your deployment Cloud Network Management Learn about the latest networking features Featured Content Get started with the fully-managed Confluent Cloud Quick Start or on-premises using the Confluent Platform ZIP and TAR installation. Confluent Cloud Quick Start Get started with quickly with a fully-managed data streaming platform ZIP and TAR Installation Install Confluent Platform on-premises using ZIP and TAR archive Install Confluent CLI Manage Confluent Platform and Cloud using the command-line interface Confluent Platform Quick Start Use your laptop to quickly get started with Confluent Platform on-premises Get Started with Connect Get started and learn the Kafka Connect basics Cluster Types Learn about the Kafka cluster types available in Confluent Cloud Browse by Feature Find the guides, examples, and tutorials for Confluent features, including the Kafka consumer or the Kafka CLI tools. Kafka Streams Confluent Cloud | Confluent Platform Kafka Connect Confluent Cloud | Confluent Platform Schema Registry Confluent Cloud | Confluent Platform Apache Flink Confluent Cloud | Confluent Platform Networking Confluent Cloud Security Confluent Cloud | Confluent Platform Confluent CLI Confluent Cloud and Confluent Platform Kafka Clients Confluent Cloud | Confluent Platform Apache Kafka Learn more Kafka Consumer Confluent Cloud | Confluent Platform Kafka Producer Confluent Cloud | Confluent Platform Multi-Region Clusters Confluent Platform Stream Lineage Confluent Cloud Terraform Confluent Cloud Confluent for Kubernetes Confluent Platform Ansible Playbooks Confluent Platform Stream Governance Confluent Cloud | Confluent Platform Replicator Confluent Platform Cluster Linking Confluent Cloud | Confluent Platform Auto Data Balancer Confluent Platform WarpStream Learn more More Ways to Learn Explore the video courses available on Confluent Developer. Apache Kafka 101 Learn the fundamentals of Kafka with this video course Kafka Connect 101 Learn the fundamentals of Kafka Connect with this video course Apache Flink SQL Learn how you can use Flink SQL with with Confluent Cloud for Apache Flink

---

### Confluent Documentation | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/how-to-guides/query-flink-sql-table.html

Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A fully-managed data streaming platform with a cloud-native Apache Kafka® engine for elastic scaling, enterprise-grade security, stream processing, and governance. Confluent Platform An enterprise-grade distribution of Apache Kafka available on-premises as self-managed software, with enterprise-grade security, stream processing, and governance. Apache Kafka An open-source distributed data streaming engine used by thousands of companies to build streaming data pipelines and applications, powering mission-critical operational and analytics use cases. What's New Try out the latest data streaming platform features, including Confluent Cloud for Apache Flink® and Confluent for VS Code. Confluent Tableflow Represent Apache Kafka® topics as open table formats like Apache Iceberg™ and Delta Lake Confluent Cloud for Apache Flink A cloud-native, serverless service for Apache Flink that enables simple, scalable, and secure stream processing that integrates seamlessly with Kafka Confluent for VS Code Build, manage, and monitor data streaming applications with Confluent for VS Code Release Notes for Confluent Cloud Discover the latest features Confluent Cloud APIs Use the API to manage your deployment Cloud Network Management Learn about the latest networking features Featured Content Get started with the fully-managed Confluent Cloud Quick Start or on-premises using the Confluent Platform ZIP and TAR installation. Confluent Cloud Quick Start Get started with quickly with a fully-managed data streaming platform ZIP and TAR Installation Install Confluent Platform on-premises using ZIP and TAR archive Install Confluent CLI Manage Confluent Platform and Cloud using the command-line interface Confluent Platform Quick Start Use your laptop to quickly get started with Confluent Platform on-premises Get Started with Connect Get started and learn the Kafka Connect basics Cluster Types Learn about the Kafka cluster types available in Confluent Cloud Browse by Feature Find the guides, examples, and tutorials for Confluent features, including the Kafka consumer or the Kafka CLI tools. Kafka Streams Confluent Cloud | Confluent Platform Kafka Connect Confluent Cloud | Confluent Platform Schema Registry Confluent Cloud | Confluent Platform Apache Flink Confluent Cloud | Confluent Platform Networking Confluent Cloud Security Confluent Cloud | Confluent Platform Confluent CLI Confluent Cloud and Confluent Platform Kafka Clients Confluent Cloud | Confluent Platform Apache Kafka Learn more Kafka Consumer Confluent Cloud | Confluent Platform Kafka Producer Confluent Cloud | Confluent Platform Multi-Region Clusters Confluent Platform Stream Lineage Confluent Cloud Terraform Confluent Cloud Confluent for Kubernetes Confluent Platform Ansible Playbooks Confluent Platform Stream Governance Confluent Cloud | Confluent Platform Replicator Confluent Platform Cluster Linking Confluent Cloud | Confluent Platform Auto Data Balancer Confluent Platform WarpStream Learn more More Ways to Learn Explore the video courses available on Confluent Developer. Apache Kafka 101 Learn the fundamentals of Kafka with this video course Kafka Connect 101 Learn the fundamentals of Kafka Connect with this video course Apache Flink SQL Learn how you can use Flink SQL with with Confluent Cloud for Apache Flink

---

### Confluent Documentation | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/how-to-guides/insert-into-flink-sql-table.html

Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A fully-managed data streaming platform with a cloud-native Apache Kafka® engine for elastic scaling, enterprise-grade security, stream processing, and governance. Confluent Platform An enterprise-grade distribution of Apache Kafka available on-premises as self-managed software, with enterprise-grade security, stream processing, and governance. Apache Kafka An open-source distributed data streaming engine used by thousands of companies to build streaming data pipelines and applications, powering mission-critical operational and analytics use cases. What's New Try out the latest data streaming platform features, including Confluent Cloud for Apache Flink® and Confluent for VS Code. Confluent Tableflow Represent Apache Kafka® topics as open table formats like Apache Iceberg™ and Delta Lake Confluent Cloud for Apache Flink A cloud-native, serverless service for Apache Flink that enables simple, scalable, and secure stream processing that integrates seamlessly with Kafka Confluent for VS Code Build, manage, and monitor data streaming applications with Confluent for VS Code Release Notes for Confluent Cloud Discover the latest features Confluent Cloud APIs Use the API to manage your deployment Cloud Network Management Learn about the latest networking features Featured Content Get started with the fully-managed Confluent Cloud Quick Start or on-premises using the Confluent Platform ZIP and TAR installation. Confluent Cloud Quick Start Get started with quickly with a fully-managed data streaming platform ZIP and TAR Installation Install Confluent Platform on-premises using ZIP and TAR archive Install Confluent CLI Manage Confluent Platform and Cloud using the command-line interface Confluent Platform Quick Start Use your laptop to quickly get started with Confluent Platform on-premises Get Started with Connect Get started and learn the Kafka Connect basics Cluster Types Learn about the Kafka cluster types available in Confluent Cloud Browse by Feature Find the guides, examples, and tutorials for Confluent features, including the Kafka consumer or the Kafka CLI tools. Kafka Streams Confluent Cloud | Confluent Platform Kafka Connect Confluent Cloud | Confluent Platform Schema Registry Confluent Cloud | Confluent Platform Apache Flink Confluent Cloud | Confluent Platform Networking Confluent Cloud Security Confluent Cloud | Confluent Platform Confluent CLI Confluent Cloud and Confluent Platform Kafka Clients Confluent Cloud | Confluent Platform Apache Kafka Learn more Kafka Consumer Confluent Cloud | Confluent Platform Kafka Producer Confluent Cloud | Confluent Platform Multi-Region Clusters Confluent Platform Stream Lineage Confluent Cloud Terraform Confluent Cloud Confluent for Kubernetes Confluent Platform Ansible Playbooks Confluent Platform Stream Governance Confluent Cloud | Confluent Platform Replicator Confluent Platform Cluster Linking Confluent Cloud | Confluent Platform Auto Data Balancer Confluent Platform WarpStream Learn more More Ways to Learn Explore the video courses available on Confluent Developer. Apache Kafka 101 Learn the fundamentals of Kafka with this video course Kafka Connect 101 Learn the fundamentals of Kafka Connect with this video course Apache Flink SQL Learn how you can use Flink SQL with with Confluent Cloud for Apache Flink

---

### Confluent Documentation | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/how-to-guides/windowing-aggregations.html

Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A fully-managed data streaming platform with a cloud-native Apache Kafka® engine for elastic scaling, enterprise-grade security, stream processing, and governance. Confluent Platform An enterprise-grade distribution of Apache Kafka available on-premises as self-managed software, with enterprise-grade security, stream processing, and governance. Apache Kafka An open-source distributed data streaming engine used by thousands of companies to build streaming data pipelines and applications, powering mission-critical operational and analytics use cases. What's New Try out the latest data streaming platform features, including Confluent Cloud for Apache Flink® and Confluent for VS Code. Confluent Tableflow Represent Apache Kafka® topics as open table formats like Apache Iceberg™ and Delta Lake Confluent Cloud for Apache Flink A cloud-native, serverless service for Apache Flink that enables simple, scalable, and secure stream processing that integrates seamlessly with Kafka Confluent for VS Code Build, manage, and monitor data streaming applications with Confluent for VS Code Release Notes for Confluent Cloud Discover the latest features Confluent Cloud APIs Use the API to manage your deployment Cloud Network Management Learn about the latest networking features Featured Content Get started with the fully-managed Confluent Cloud Quick Start or on-premises using the Confluent Platform ZIP and TAR installation. Confluent Cloud Quick Start Get started with quickly with a fully-managed data streaming platform ZIP and TAR Installation Install Confluent Platform on-premises using ZIP and TAR archive Install Confluent CLI Manage Confluent Platform and Cloud using the command-line interface Confluent Platform Quick Start Use your laptop to quickly get started with Confluent Platform on-premises Get Started with Connect Get started and learn the Kafka Connect basics Cluster Types Learn about the Kafka cluster types available in Confluent Cloud Browse by Feature Find the guides, examples, and tutorials for Confluent features, including the Kafka consumer or the Kafka CLI tools. Kafka Streams Confluent Cloud | Confluent Platform Kafka Connect Confluent Cloud | Confluent Platform Schema Registry Confluent Cloud | Confluent Platform Apache Flink Confluent Cloud | Confluent Platform Networking Confluent Cloud Security Confluent Cloud | Confluent Platform Confluent CLI Confluent Cloud and Confluent Platform Kafka Clients Confluent Cloud | Confluent Platform Apache Kafka Learn more Kafka Consumer Confluent Cloud | Confluent Platform Kafka Producer Confluent Cloud | Confluent Platform Multi-Region Clusters Confluent Platform Stream Lineage Confluent Cloud Terraform Confluent Cloud Confluent for Kubernetes Confluent Platform Ansible Playbooks Confluent Platform Stream Governance Confluent Cloud | Confluent Platform Replicator Confluent Platform Cluster Linking Confluent Cloud | Confluent Platform Auto Data Balancer Confluent Platform WarpStream Learn more More Ways to Learn Explore the video courses available on Confluent Developer. Apache Kafka 101 Learn the fundamentals of Kafka with this video course Kafka Connect 101 Learn the fundamentals of Kafka Connect with this video course Apache Flink SQL Learn how you can use Flink SQL with with Confluent Cloud for Apache Flink

---

### Confluent Documentation | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/how-to-guides/joins.html

Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A fully-managed data streaming platform with a cloud-native Apache Kafka® engine for elastic scaling, enterprise-grade security, stream processing, and governance. Confluent Platform An enterprise-grade distribution of Apache Kafka available on-premises as self-managed software, with enterprise-grade security, stream processing, and governance. Apache Kafka An open-source distributed data streaming engine used by thousands of companies to build streaming data pipelines and applications, powering mission-critical operational and analytics use cases. What's New Try out the latest data streaming platform features, including Confluent Cloud for Apache Flink® and Confluent for VS Code. Confluent Tableflow Represent Apache Kafka® topics as open table formats like Apache Iceberg™ and Delta Lake Confluent Cloud for Apache Flink A cloud-native, serverless service for Apache Flink that enables simple, scalable, and secure stream processing that integrates seamlessly with Kafka Confluent for VS Code Build, manage, and monitor data streaming applications with Confluent for VS Code Release Notes for Confluent Cloud Discover the latest features Confluent Cloud APIs Use the API to manage your deployment Cloud Network Management Learn about the latest networking features Featured Content Get started with the fully-managed Confluent Cloud Quick Start or on-premises using the Confluent Platform ZIP and TAR installation. Confluent Cloud Quick Start Get started with quickly with a fully-managed data streaming platform ZIP and TAR Installation Install Confluent Platform on-premises using ZIP and TAR archive Install Confluent CLI Manage Confluent Platform and Cloud using the command-line interface Confluent Platform Quick Start Use your laptop to quickly get started with Confluent Platform on-premises Get Started with Connect Get started and learn the Kafka Connect basics Cluster Types Learn about the Kafka cluster types available in Confluent Cloud Browse by Feature Find the guides, examples, and tutorials for Confluent features, including the Kafka consumer or the Kafka CLI tools. Kafka Streams Confluent Cloud | Confluent Platform Kafka Connect Confluent Cloud | Confluent Platform Schema Registry Confluent Cloud | Confluent Platform Apache Flink Confluent Cloud | Confluent Platform Networking Confluent Cloud Security Confluent Cloud | Confluent Platform Confluent CLI Confluent Cloud and Confluent Platform Kafka Clients Confluent Cloud | Confluent Platform Apache Kafka Learn more Kafka Consumer Confluent Cloud | Confluent Platform Kafka Producer Confluent Cloud | Confluent Platform Multi-Region Clusters Confluent Platform Stream Lineage Confluent Cloud Terraform Confluent Cloud Confluent for Kubernetes Confluent Platform Ansible Playbooks Confluent Platform Stream Governance Confluent Cloud | Confluent Platform Replicator Confluent Platform Cluster Linking Confluent Cloud | Confluent Platform Auto Data Balancer Confluent Platform WarpStream Learn more More Ways to Learn Explore the video courses available on Confluent Developer. Apache Kafka 101 Learn the fundamentals of Kafka with this video course Kafka Connect 101 Learn the fundamentals of Kafka Connect with this video course Apache Flink SQL Learn how you can use Flink SQL with with Confluent Cloud for Apache Flink

---

### Confluent Documentation | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/how-to-guides/time-processing.html

Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A fully-managed data streaming platform with a cloud-native Apache Kafka® engine for elastic scaling, enterprise-grade security, stream processing, and governance. Confluent Platform An enterprise-grade distribution of Apache Kafka available on-premises as self-managed software, with enterprise-grade security, stream processing, and governance. Apache Kafka An open-source distributed data streaming engine used by thousands of companies to build streaming data pipelines and applications, powering mission-critical operational and analytics use cases. What's New Try out the latest data streaming platform features, including Confluent Cloud for Apache Flink® and Confluent for VS Code. Confluent Tableflow Represent Apache Kafka® topics as open table formats like Apache Iceberg™ and Delta Lake Confluent Cloud for Apache Flink A cloud-native, serverless service for Apache Flink that enables simple, scalable, and secure stream processing that integrates seamlessly with Kafka Confluent for VS Code Build, manage, and monitor data streaming applications with Confluent for VS Code Release Notes for Confluent Cloud Discover the latest features Confluent Cloud APIs Use the API to manage your deployment Cloud Network Management Learn about the latest networking features Featured Content Get started with the fully-managed Confluent Cloud Quick Start or on-premises using the Confluent Platform ZIP and TAR installation. Confluent Cloud Quick Start Get started with quickly with a fully-managed data streaming platform ZIP and TAR Installation Install Confluent Platform on-premises using ZIP and TAR archive Install Confluent CLI Manage Confluent Platform and Cloud using the command-line interface Confluent Platform Quick Start Use your laptop to quickly get started with Confluent Platform on-premises Get Started with Connect Get started and learn the Kafka Connect basics Cluster Types Learn about the Kafka cluster types available in Confluent Cloud Browse by Feature Find the guides, examples, and tutorials for Confluent features, including the Kafka consumer or the Kafka CLI tools. Kafka Streams Confluent Cloud | Confluent Platform Kafka Connect Confluent Cloud | Confluent Platform Schema Registry Confluent Cloud | Confluent Platform Apache Flink Confluent Cloud | Confluent Platform Networking Confluent Cloud Security Confluent Cloud | Confluent Platform Confluent CLI Confluent Cloud and Confluent Platform Kafka Clients Confluent Cloud | Confluent Platform Apache Kafka Learn more Kafka Consumer Confluent Cloud | Confluent Platform Kafka Producer Confluent Cloud | Confluent Platform Multi-Region Clusters Confluent Platform Stream Lineage Confluent Cloud Terraform Confluent Cloud Confluent for Kubernetes Confluent Platform Ansible Playbooks Confluent Platform Stream Governance Confluent Cloud | Confluent Platform Replicator Confluent Platform Cluster Linking Confluent Cloud | Confluent Platform Auto Data Balancer Confluent Platform WarpStream Learn more More Ways to Learn Explore the video courses available on Confluent Developer. Apache Kafka 101 Learn the fundamentals of Kafka with this video course Kafka Connect 101 Learn the fundamentals of Kafka Connect with this video course Apache Flink SQL Learn how you can use Flink SQL with with Confluent Cloud for Apache Flink

---

### Confluent Documentation | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/how-to-guides/data-types.html

Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A fully-managed data streaming platform with a cloud-native Apache Kafka® engine for elastic scaling, enterprise-grade security, stream processing, and governance. Confluent Platform An enterprise-grade distribution of Apache Kafka available on-premises as self-managed software, with enterprise-grade security, stream processing, and governance. Apache Kafka An open-source distributed data streaming engine used by thousands of companies to build streaming data pipelines and applications, powering mission-critical operational and analytics use cases. What's New Try out the latest data streaming platform features, including Confluent Cloud for Apache Flink® and Confluent for VS Code. Confluent Tableflow Represent Apache Kafka® topics as open table formats like Apache Iceberg™ and Delta Lake Confluent Cloud for Apache Flink A cloud-native, serverless service for Apache Flink that enables simple, scalable, and secure stream processing that integrates seamlessly with Kafka Confluent for VS Code Build, manage, and monitor data streaming applications with Confluent for VS Code Release Notes for Confluent Cloud Discover the latest features Confluent Cloud APIs Use the API to manage your deployment Cloud Network Management Learn about the latest networking features Featured Content Get started with the fully-managed Confluent Cloud Quick Start or on-premises using the Confluent Platform ZIP and TAR installation. Confluent Cloud Quick Start Get started with quickly with a fully-managed data streaming platform ZIP and TAR Installation Install Confluent Platform on-premises using ZIP and TAR archive Install Confluent CLI Manage Confluent Platform and Cloud using the command-line interface Confluent Platform Quick Start Use your laptop to quickly get started with Confluent Platform on-premises Get Started with Connect Get started and learn the Kafka Connect basics Cluster Types Learn about the Kafka cluster types available in Confluent Cloud Browse by Feature Find the guides, examples, and tutorials for Confluent features, including the Kafka consumer or the Kafka CLI tools. Kafka Streams Confluent Cloud | Confluent Platform Kafka Connect Confluent Cloud | Confluent Platform Schema Registry Confluent Cloud | Confluent Platform Apache Flink Confluent Cloud | Confluent Platform Networking Confluent Cloud Security Confluent Cloud | Confluent Platform Confluent CLI Confluent Cloud and Confluent Platform Kafka Clients Confluent Cloud | Confluent Platform Apache Kafka Learn more Kafka Consumer Confluent Cloud | Confluent Platform Kafka Producer Confluent Cloud | Confluent Platform Multi-Region Clusters Confluent Platform Stream Lineage Confluent Cloud Terraform Confluent Cloud Confluent for Kubernetes Confluent Platform Ansible Playbooks Confluent Platform Stream Governance Confluent Cloud | Confluent Platform Replicator Confluent Platform Cluster Linking Confluent Cloud | Confluent Platform Auto Data Balancer Confluent Platform WarpStream Learn more More Ways to Learn Explore the video courses available on Confluent Developer. Apache Kafka 101 Learn the fundamentals of Kafka with this video course Kafka Connect 101 Learn the fundamentals of Kafka Connect with this video course Apache Flink SQL Learn how you can use Flink SQL with with Confluent Cloud for Apache Flink

---

### Confluent Documentation | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/how-to-guides/upsert-into-from-select-statement.html

Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A fully-managed data streaming platform with a cloud-native Apache Kafka® engine for elastic scaling, enterprise-grade security, stream processing, and governance. Confluent Platform An enterprise-grade distribution of Apache Kafka available on-premises as self-managed software, with enterprise-grade security, stream processing, and governance. Apache Kafka An open-source distributed data streaming engine used by thousands of companies to build streaming data pipelines and applications, powering mission-critical operational and analytics use cases. What's New Try out the latest data streaming platform features, including Confluent Cloud for Apache Flink® and Confluent for VS Code. Confluent Tableflow Represent Apache Kafka® topics as open table formats like Apache Iceberg™ and Delta Lake Confluent Cloud for Apache Flink A cloud-native, serverless service for Apache Flink that enables simple, scalable, and secure stream processing that integrates seamlessly with Kafka Confluent for VS Code Build, manage, and monitor data streaming applications with Confluent for VS Code Release Notes for Confluent Cloud Discover the latest features Confluent Cloud APIs Use the API to manage your deployment Cloud Network Management Learn about the latest networking features Featured Content Get started with the fully-managed Confluent Cloud Quick Start or on-premises using the Confluent Platform ZIP and TAR installation. Confluent Cloud Quick Start Get started with quickly with a fully-managed data streaming platform ZIP and TAR Installation Install Confluent Platform on-premises using ZIP and TAR archive Install Confluent CLI Manage Confluent Platform and Cloud using the command-line interface Confluent Platform Quick Start Use your laptop to quickly get started with Confluent Platform on-premises Get Started with Connect Get started and learn the Kafka Connect basics Cluster Types Learn about the Kafka cluster types available in Confluent Cloud Browse by Feature Find the guides, examples, and tutorials for Confluent features, including the Kafka consumer or the Kafka CLI tools. Kafka Streams Confluent Cloud | Confluent Platform Kafka Connect Confluent Cloud | Confluent Platform Schema Registry Confluent Cloud | Confluent Platform Apache Flink Confluent Cloud | Confluent Platform Networking Confluent Cloud Security Confluent Cloud | Confluent Platform Confluent CLI Confluent Cloud and Confluent Platform Kafka Clients Confluent Cloud | Confluent Platform Apache Kafka Learn more Kafka Consumer Confluent Cloud | Confluent Platform Kafka Producer Confluent Cloud | Confluent Platform Multi-Region Clusters Confluent Platform Stream Lineage Confluent Cloud Terraform Confluent Cloud Confluent for Kubernetes Confluent Platform Ansible Playbooks Confluent Platform Stream Governance Confluent Cloud | Confluent Platform Replicator Confluent Platform Cluster Linking Confluent Cloud | Confluent Platform Auto Data Balancer Confluent Platform WarpStream Learn more More Ways to Learn Explore the video courses available on Confluent Developer. Apache Kafka 101 Learn the fundamentals of Kafka with this video course Kafka Connect 101 Learn the fundamentals of Kafka Connect with this video course Apache Flink SQL Learn how you can use Flink SQL with with Confluent Cloud for Apache Flink

---

### Operate and Deploy Flink SQL Statements with Confluent Cloud for Apache Flink | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/operate-and-deploy/overview.html

Operate and Deploy Flink Statements with Confluent Cloud for Apache Flink¶ Confluent provides tools for operating Confluent Cloud for Apache Flink® in the Cloud Console, the Confluent CLI, the Confluent Terraform Provider, and the REST API: Deploy a Statement Billing Monitor Statements with Cloud Console CLI commands Terraform resources REST API RBAC Flink API Keys Note This website includes content developed at the Apache Software Foundation under the terms of the Apache License v2.

---

### Confluent Documentation | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/operate-and-deploy/flink-sql-compute-pools.html

Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A fully-managed data streaming platform with a cloud-native Apache Kafka® engine for elastic scaling, enterprise-grade security, stream processing, and governance. Confluent Platform An enterprise-grade distribution of Apache Kafka available on-premises as self-managed software, with enterprise-grade security, stream processing, and governance. Apache Kafka An open-source distributed data streaming engine used by thousands of companies to build streaming data pipelines and applications, powering mission-critical operational and analytics use cases. What's New Try out the latest data streaming platform features, including Confluent Cloud for Apache Flink® and Confluent for VS Code. Confluent Tableflow Represent Apache Kafka® topics as open table formats like Apache Iceberg™ and Delta Lake Confluent Cloud for Apache Flink A cloud-native, serverless service for Apache Flink that enables simple, scalable, and secure stream processing that integrates seamlessly with Kafka Confluent for VS Code Build, manage, and monitor data streaming applications with Confluent for VS Code Release Notes for Confluent Cloud Discover the latest features Confluent Cloud APIs Use the API to manage your deployment Cloud Network Management Learn about the latest networking features Featured Content Get started with the fully-managed Confluent Cloud Quick Start or on-premises using the Confluent Platform ZIP and TAR installation. Confluent Cloud Quick Start Get started with quickly with a fully-managed data streaming platform ZIP and TAR Installation Install Confluent Platform on-premises using ZIP and TAR archive Install Confluent CLI Manage Confluent Platform and Cloud using the command-line interface Confluent Platform Quick Start Use your laptop to quickly get started with Confluent Platform on-premises Get Started with Connect Get started and learn the Kafka Connect basics Cluster Types Learn about the Kafka cluster types available in Confluent Cloud Browse by Feature Find the guides, examples, and tutorials for Confluent features, including the Kafka consumer or the Kafka CLI tools. Kafka Streams Confluent Cloud | Confluent Platform Kafka Connect Confluent Cloud | Confluent Platform Schema Registry Confluent Cloud | Confluent Platform Apache Flink Confluent Cloud | Confluent Platform Networking Confluent Cloud Security Confluent Cloud | Confluent Platform Confluent CLI Confluent Cloud and Confluent Platform Kafka Clients Confluent Cloud | Confluent Platform Apache Kafka Learn more Kafka Consumer Confluent Cloud | Confluent Platform Kafka Producer Confluent Cloud | Confluent Platform Multi-Region Clusters Confluent Platform Stream Lineage Confluent Cloud Terraform Confluent Cloud Confluent for Kubernetes Confluent Platform Ansible Playbooks Confluent Platform Stream Governance Confluent Cloud | Confluent Platform Replicator Confluent Platform Cluster Linking Confluent Cloud | Confluent Platform Auto Data Balancer Confluent Platform WarpStream Learn more More Ways to Learn Explore the video courses available on Confluent Developer. Apache Kafka 101 Learn the fundamentals of Kafka with this video course Kafka Connect 101 Learn the fundamentals of Kafka Connect with this video course Apache Flink SQL Learn how you can use Flink SQL with with Confluent Cloud for Apache Flink

---

### Confluent Documentation | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/operate-and-deploy/flink-sql-statements.html

Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A fully-managed data streaming platform with a cloud-native Apache Kafka® engine for elastic scaling, enterprise-grade security, stream processing, and governance. Confluent Platform An enterprise-grade distribution of Apache Kafka available on-premises as self-managed software, with enterprise-grade security, stream processing, and governance. Apache Kafka An open-source distributed data streaming engine used by thousands of companies to build streaming data pipelines and applications, powering mission-critical operational and analytics use cases. What's New Try out the latest data streaming platform features, including Confluent Cloud for Apache Flink® and Confluent for VS Code. Confluent Tableflow Represent Apache Kafka® topics as open table formats like Apache Iceberg™ and Delta Lake Confluent Cloud for Apache Flink A cloud-native, serverless service for Apache Flink that enables simple, scalable, and secure stream processing that integrates seamlessly with Kafka Confluent for VS Code Build, manage, and monitor data streaming applications with Confluent for VS Code Release Notes for Confluent Cloud Discover the latest features Confluent Cloud APIs Use the API to manage your deployment Cloud Network Management Learn about the latest networking features Featured Content Get started with the fully-managed Confluent Cloud Quick Start or on-premises using the Confluent Platform ZIP and TAR installation. Confluent Cloud Quick Start Get started with quickly with a fully-managed data streaming platform ZIP and TAR Installation Install Confluent Platform on-premises using ZIP and TAR archive Install Confluent CLI Manage Confluent Platform and Cloud using the command-line interface Confluent Platform Quick Start Use your laptop to quickly get started with Confluent Platform on-premises Get Started with Connect Get started and learn the Kafka Connect basics Cluster Types Learn about the Kafka cluster types available in Confluent Cloud Browse by Feature Find the guides, examples, and tutorials for Confluent features, including the Kafka consumer or the Kafka CLI tools. Kafka Streams Confluent Cloud | Confluent Platform Kafka Connect Confluent Cloud | Confluent Platform Schema Registry Confluent Cloud | Confluent Platform Apache Flink Confluent Cloud | Confluent Platform Networking Confluent Cloud Security Confluent Cloud | Confluent Platform Confluent CLI Confluent Cloud and Confluent Platform Kafka Clients Confluent Cloud | Confluent Platform Apache Kafka Learn more Kafka Consumer Confluent Cloud | Confluent Platform Kafka Producer Confluent Cloud | Confluent Platform Multi-Region Clusters Confluent Platform Stream Lineage Confluent Cloud Terraform Confluent Cloud Confluent for Kubernetes Confluent Platform Ansible Playbooks Confluent Platform Stream Governance Confluent Cloud | Confluent Platform Replicator Confluent Platform Cluster Linking Confluent Cloud | Confluent Platform Auto Data Balancer Confluent Platform WarpStream Learn more More Ways to Learn Explore the video courses available on Confluent Developer. Apache Kafka 101 Learn the fundamentals of Kafka with this video course Kafka Connect 101 Learn the fundamentals of Kafka Connect with this video course Apache Flink SQL Learn how you can use Flink SQL with with Confluent Cloud for Apache Flink

---

### Confluent Documentation | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/operate-and-deploy/flink-sql-user-defined-functions.html

Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A fully-managed data streaming platform with a cloud-native Apache Kafka® engine for elastic scaling, enterprise-grade security, stream processing, and governance. Confluent Platform An enterprise-grade distribution of Apache Kafka available on-premises as self-managed software, with enterprise-grade security, stream processing, and governance. Apache Kafka An open-source distributed data streaming engine used by thousands of companies to build streaming data pipelines and applications, powering mission-critical operational and analytics use cases. What's New Try out the latest data streaming platform features, including Confluent Cloud for Apache Flink® and Confluent for VS Code. Confluent Tableflow Represent Apache Kafka® topics as open table formats like Apache Iceberg™ and Delta Lake Confluent Cloud for Apache Flink A cloud-native, serverless service for Apache Flink that enables simple, scalable, and secure stream processing that integrates seamlessly with Kafka Confluent for VS Code Build, manage, and monitor data streaming applications with Confluent for VS Code Release Notes for Confluent Cloud Discover the latest features Confluent Cloud APIs Use the API to manage your deployment Cloud Network Management Learn about the latest networking features Featured Content Get started with the fully-managed Confluent Cloud Quick Start or on-premises using the Confluent Platform ZIP and TAR installation. Confluent Cloud Quick Start Get started with quickly with a fully-managed data streaming platform ZIP and TAR Installation Install Confluent Platform on-premises using ZIP and TAR archive Install Confluent CLI Manage Confluent Platform and Cloud using the command-line interface Confluent Platform Quick Start Use your laptop to quickly get started with Confluent Platform on-premises Get Started with Connect Get started and learn the Kafka Connect basics Cluster Types Learn about the Kafka cluster types available in Confluent Cloud Browse by Feature Find the guides, examples, and tutorials for Confluent features, including the Kafka consumer or the Kafka CLI tools. Kafka Streams Confluent Cloud | Confluent Platform Kafka Connect Confluent Cloud | Confluent Platform Schema Registry Confluent Cloud | Confluent Platform Apache Flink Confluent Cloud | Confluent Platform Networking Confluent Cloud Security Confluent Cloud | Confluent Platform Confluent CLI Confluent Cloud and Confluent Platform Kafka Clients Confluent Cloud | Confluent Platform Apache Kafka Learn more Kafka Consumer Confluent Cloud | Confluent Platform Kafka Producer Confluent Cloud | Confluent Platform Multi-Region Clusters Confluent Platform Stream Lineage Confluent Cloud Terraform Confluent Cloud Confluent for Kubernetes Confluent Platform Ansible Playbooks Confluent Platform Stream Governance Confluent Cloud | Confluent Platform Replicator Confluent Platform Cluster Linking Confluent Cloud | Confluent Platform Auto Data Balancer Confluent Platform WarpStream Learn more More Ways to Learn Explore the video courses available on Confluent Developer. Apache Kafka 101 Learn the fundamentals of Kafka with this video course Kafka Connect 101 Learn the fundamentals of Kafka Connect with this video course Apache Flink SQL Learn how you can use Flink SQL with with Confluent Cloud for Apache Flink

---

### Confluent Documentation | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/operate-and-deploy/flink-sql-security.html

Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A fully-managed data streaming platform with a cloud-native Apache Kafka® engine for elastic scaling, enterprise-grade security, stream processing, and governance. Confluent Platform An enterprise-grade distribution of Apache Kafka available on-premises as self-managed software, with enterprise-grade security, stream processing, and governance. Apache Kafka An open-source distributed data streaming engine used by thousands of companies to build streaming data pipelines and applications, powering mission-critical operational and analytics use cases. What's New Try out the latest data streaming platform features, including Confluent Cloud for Apache Flink® and Confluent for VS Code. Confluent Tableflow Represent Apache Kafka® topics as open table formats like Apache Iceberg™ and Delta Lake Confluent Cloud for Apache Flink A cloud-native, serverless service for Apache Flink that enables simple, scalable, and secure stream processing that integrates seamlessly with Kafka Confluent for VS Code Build, manage, and monitor data streaming applications with Confluent for VS Code Release Notes for Confluent Cloud Discover the latest features Confluent Cloud APIs Use the API to manage your deployment Cloud Network Management Learn about the latest networking features Featured Content Get started with the fully-managed Confluent Cloud Quick Start or on-premises using the Confluent Platform ZIP and TAR installation. Confluent Cloud Quick Start Get started with quickly with a fully-managed data streaming platform ZIP and TAR Installation Install Confluent Platform on-premises using ZIP and TAR archive Install Confluent CLI Manage Confluent Platform and Cloud using the command-line interface Confluent Platform Quick Start Use your laptop to quickly get started with Confluent Platform on-premises Get Started with Connect Get started and learn the Kafka Connect basics Cluster Types Learn about the Kafka cluster types available in Confluent Cloud Browse by Feature Find the guides, examples, and tutorials for Confluent features, including the Kafka consumer or the Kafka CLI tools. Kafka Streams Confluent Cloud | Confluent Platform Kafka Connect Confluent Cloud | Confluent Platform Schema Registry Confluent Cloud | Confluent Platform Apache Flink Confluent Cloud | Confluent Platform Networking Confluent Cloud Security Confluent Cloud | Confluent Platform Confluent CLI Confluent Cloud and Confluent Platform Kafka Clients Confluent Cloud | Confluent Platform Apache Kafka Learn more Kafka Consumer Confluent Cloud | Confluent Platform Kafka Producer Confluent Cloud | Confluent Platform Multi-Region Clusters Confluent Platform Stream Lineage Confluent Cloud Terraform Confluent Cloud Confluent for Kubernetes Confluent Platform Ansible Playbooks Confluent Platform Stream Governance Confluent Cloud | Confluent Platform Replicator Confluent Platform Cluster Linking Confluent Cloud | Confluent Platform Auto Data Balancer Confluent Platform WarpStream Learn more More Ways to Learn Explore the video courses available on Confluent Developer. Apache Kafka 101 Learn the fundamentals of Kafka with this video course Kafka Connect 101 Learn the fundamentals of Kafka Connect with this video course Apache Flink SQL Learn how you can use Flink SQL with with Confluent Cloud for Apache Flink

---

### Confluent Documentation | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/operate-and-deploy/flink-sql-private-networking.html

Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A fully-managed data streaming platform with a cloud-native Apache Kafka® engine for elastic scaling, enterprise-grade security, stream processing, and governance. Confluent Platform An enterprise-grade distribution of Apache Kafka available on-premises as self-managed software, with enterprise-grade security, stream processing, and governance. Apache Kafka An open-source distributed data streaming engine used by thousands of companies to build streaming data pipelines and applications, powering mission-critical operational and analytics use cases. What's New Try out the latest data streaming platform features, including Confluent Cloud for Apache Flink® and Confluent for VS Code. Confluent Tableflow Represent Apache Kafka® topics as open table formats like Apache Iceberg™ and Delta Lake Confluent Cloud for Apache Flink A cloud-native, serverless service for Apache Flink that enables simple, scalable, and secure stream processing that integrates seamlessly with Kafka Confluent for VS Code Build, manage, and monitor data streaming applications with Confluent for VS Code Release Notes for Confluent Cloud Discover the latest features Confluent Cloud APIs Use the API to manage your deployment Cloud Network Management Learn about the latest networking features Featured Content Get started with the fully-managed Confluent Cloud Quick Start or on-premises using the Confluent Platform ZIP and TAR installation. Confluent Cloud Quick Start Get started with quickly with a fully-managed data streaming platform ZIP and TAR Installation Install Confluent Platform on-premises using ZIP and TAR archive Install Confluent CLI Manage Confluent Platform and Cloud using the command-line interface Confluent Platform Quick Start Use your laptop to quickly get started with Confluent Platform on-premises Get Started with Connect Get started and learn the Kafka Connect basics Cluster Types Learn about the Kafka cluster types available in Confluent Cloud Browse by Feature Find the guides, examples, and tutorials for Confluent features, including the Kafka consumer or the Kafka CLI tools. Kafka Streams Confluent Cloud | Confluent Platform Kafka Connect Confluent Cloud | Confluent Platform Schema Registry Confluent Cloud | Confluent Platform Apache Flink Confluent Cloud | Confluent Platform Networking Confluent Cloud Security Confluent Cloud | Confluent Platform Confluent CLI Confluent Cloud and Confluent Platform Kafka Clients Confluent Cloud | Confluent Platform Apache Kafka Learn more Kafka Consumer Confluent Cloud | Confluent Platform Kafka Producer Confluent Cloud | Confluent Platform Multi-Region Clusters Confluent Platform Stream Lineage Confluent Cloud Terraform Confluent Cloud Confluent for Kubernetes Confluent Platform Ansible Playbooks Confluent Platform Stream Governance Confluent Cloud | Confluent Platform Replicator Confluent Platform Cluster Linking Confluent Cloud | Confluent Platform Auto Data Balancer Confluent Platform WarpStream Learn more More Ways to Learn Explore the video courses available on Confluent Developer. Apache Kafka 101 Learn the fundamentals of Kafka with this video course Kafka Connect 101 Learn the fundamentals of Kafka Connect with this video course Apache Flink SQL Learn how you can use Flink SQL with with Confluent Cloud for Apache Flink

---

### Confluent Documentation | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/operate-and-deploy/flink-sql-rbac.html

Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A fully-managed data streaming platform with a cloud-native Apache Kafka® engine for elastic scaling, enterprise-grade security, stream processing, and governance. Confluent Platform An enterprise-grade distribution of Apache Kafka available on-premises as self-managed software, with enterprise-grade security, stream processing, and governance. Apache Kafka An open-source distributed data streaming engine used by thousands of companies to build streaming data pipelines and applications, powering mission-critical operational and analytics use cases. What's New Try out the latest data streaming platform features, including Confluent Cloud for Apache Flink® and Confluent for VS Code. Confluent Tableflow Represent Apache Kafka® topics as open table formats like Apache Iceberg™ and Delta Lake Confluent Cloud for Apache Flink A cloud-native, serverless service for Apache Flink that enables simple, scalable, and secure stream processing that integrates seamlessly with Kafka Confluent for VS Code Build, manage, and monitor data streaming applications with Confluent for VS Code Release Notes for Confluent Cloud Discover the latest features Confluent Cloud APIs Use the API to manage your deployment Cloud Network Management Learn about the latest networking features Featured Content Get started with the fully-managed Confluent Cloud Quick Start or on-premises using the Confluent Platform ZIP and TAR installation. Confluent Cloud Quick Start Get started with quickly with a fully-managed data streaming platform ZIP and TAR Installation Install Confluent Platform on-premises using ZIP and TAR archive Install Confluent CLI Manage Confluent Platform and Cloud using the command-line interface Confluent Platform Quick Start Use your laptop to quickly get started with Confluent Platform on-premises Get Started with Connect Get started and learn the Kafka Connect basics Cluster Types Learn about the Kafka cluster types available in Confluent Cloud Browse by Feature Find the guides, examples, and tutorials for Confluent features, including the Kafka consumer or the Kafka CLI tools. Kafka Streams Confluent Cloud | Confluent Platform Kafka Connect Confluent Cloud | Confluent Platform Schema Registry Confluent Cloud | Confluent Platform Apache Flink Confluent Cloud | Confluent Platform Networking Confluent Cloud Security Confluent Cloud | Confluent Platform Confluent CLI Confluent Cloud and Confluent Platform Kafka Clients Confluent Cloud | Confluent Platform Apache Kafka Learn more Kafka Consumer Confluent Cloud | Confluent Platform Kafka Producer Confluent Cloud | Confluent Platform Multi-Region Clusters Confluent Platform Stream Lineage Confluent Cloud Terraform Confluent Cloud Confluent for Kubernetes Confluent Platform Ansible Playbooks Confluent Platform Stream Governance Confluent Cloud | Confluent Platform Replicator Confluent Platform Cluster Linking Confluent Cloud | Confluent Platform Auto Data Balancer Confluent Platform WarpStream Learn more More Ways to Learn Explore the video courses available on Confluent Developer. Apache Kafka 101 Learn the fundamentals of Kafka with this video course Kafka Connect 101 Learn the fundamentals of Kafka Connect with this video course Apache Flink SQL Learn how you can use Flink SQL with with Confluent Cloud for Apache Flink

---

### Confluent Documentation | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/operate-and-deploy/flink-sql-troubleshooting.html

Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A fully-managed data streaming platform with a cloud-native Apache Kafka® engine for elastic scaling, enterprise-grade security, stream processing, and governance. Confluent Platform An enterprise-grade distribution of Apache Kafka available on-premises as self-managed software, with enterprise-grade security, stream processing, and governance. Apache Kafka An open-source distributed data streaming engine used by thousands of companies to build streaming data pipelines and applications, powering mission-critical operational and analytics use cases. What's New Try out the latest data streaming platform features, including Confluent Cloud for Apache Flink® and Confluent for VS Code. Confluent Tableflow Represent Apache Kafka® topics as open table formats like Apache Iceberg™ and Delta Lake Confluent Cloud for Apache Flink A cloud-native, serverless service for Apache Flink that enables simple, scalable, and secure stream processing that integrates seamlessly with Kafka Confluent for VS Code Build, manage, and monitor data streaming applications with Confluent for VS Code Release Notes for Confluent Cloud Discover the latest features Confluent Cloud APIs Use the API to manage your deployment Cloud Network Management Learn about the latest networking features Featured Content Get started with the fully-managed Confluent Cloud Quick Start or on-premises using the Confluent Platform ZIP and TAR installation. Confluent Cloud Quick Start Get started with quickly with a fully-managed data streaming platform ZIP and TAR Installation Install Confluent Platform on-premises using ZIP and TAR archive Install Confluent CLI Manage Confluent Platform and Cloud using the command-line interface Confluent Platform Quick Start Use your laptop to quickly get started with Confluent Platform on-premises Get Started with Connect Get started and learn the Kafka Connect basics Cluster Types Learn about the Kafka cluster types available in Confluent Cloud Browse by Feature Find the guides, examples, and tutorials for Confluent features, including the Kafka consumer or the Kafka CLI tools. Kafka Streams Confluent Cloud | Confluent Platform Kafka Connect Confluent Cloud | Confluent Platform Schema Registry Confluent Cloud | Confluent Platform Apache Flink Confluent Cloud | Confluent Platform Networking Confluent Cloud Security Confluent Cloud | Confluent Platform Confluent CLI Confluent Cloud and Confluent Platform Kafka Clients Confluent Cloud | Confluent Platform Apache Kafka Learn more Kafka Consumer Confluent Cloud | Confluent Platform Kafka Producer Confluent Cloud | Confluent Platform Multi-Region Clusters Confluent Platform Stream Lineage Confluent Cloud Terraform Confluent Cloud Confluent for Kubernetes Confluent Platform Ansible Playbooks Confluent Platform Stream Governance Confluent Cloud | Confluent Platform Replicator Confluent Platform Cluster Linking Confluent Cloud | Confluent Platform Auto Data Balancer Confluent Platform WarpStream Learn more More Ways to Learn Explore the video courses available on Confluent Developer. Apache Kafka 101 Learn the fundamentals of Kafka with this video course Kafka Connect 101 Learn the fundamentals of Kafka Connect with this video course Apache Flink SQL Learn how you can use Flink SQL with with Confluent Cloud for Apache Flink

---

### Confluent Documentation | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/operate-and-deploy/flink-sql-faq.html

Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A fully-managed data streaming platform with a cloud-native Apache Kafka® engine for elastic scaling, enterprise-grade security, stream processing, and governance. Confluent Platform An enterprise-grade distribution of Apache Kafka available on-premises as self-managed software, with enterprise-grade security, stream processing, and governance. Apache Kafka An open-source distributed data streaming engine used by thousands of companies to build streaming data pipelines and applications, powering mission-critical operational and analytics use cases. What's New Try out the latest data streaming platform features, including Confluent Cloud for Apache Flink® and Confluent for VS Code. Confluent Tableflow Represent Apache Kafka® topics as open table formats like Apache Iceberg™ and Delta Lake Confluent Cloud for Apache Flink A cloud-native, serverless service for Apache Flink that enables simple, scalable, and secure stream processing that integrates seamlessly with Kafka Confluent for VS Code Build, manage, and monitor data streaming applications with Confluent for VS Code Release Notes for Confluent Cloud Discover the latest features Confluent Cloud APIs Use the API to manage your deployment Cloud Network Management Learn about the latest networking features Featured Content Get started with the fully-managed Confluent Cloud Quick Start or on-premises using the Confluent Platform ZIP and TAR installation. Confluent Cloud Quick Start Get started with quickly with a fully-managed data streaming platform ZIP and TAR Installation Install Confluent Platform on-premises using ZIP and TAR archive Install Confluent CLI Manage Confluent Platform and Cloud using the command-line interface Confluent Platform Quick Start Use your laptop to quickly get started with Confluent Platform on-premises Get Started with Connect Get started and learn the Kafka Connect basics Cluster Types Learn about the Kafka cluster types available in Confluent Cloud Browse by Feature Find the guides, examples, and tutorials for Confluent features, including the Kafka consumer or the Kafka CLI tools. Kafka Streams Confluent Cloud | Confluent Platform Kafka Connect Confluent Cloud | Confluent Platform Schema Registry Confluent Cloud | Confluent Platform Apache Flink Confluent Cloud | Confluent Platform Networking Confluent Cloud Security Confluent Cloud | Confluent Platform Confluent CLI Confluent Cloud and Confluent Platform Kafka Clients Confluent Cloud | Confluent Platform Apache Kafka Learn more Kafka Consumer Confluent Cloud | Confluent Platform Kafka Producer Confluent Cloud | Confluent Platform Multi-Region Clusters Confluent Platform Stream Lineage Confluent Cloud Terraform Confluent Cloud Confluent for Kubernetes Confluent Platform Ansible Playbooks Confluent Platform Stream Governance Confluent Cloud | Confluent Platform Replicator Confluent Platform Cluster Linking Confluent Cloud | Confluent Platform Auto Data Balancer Confluent Platform WarpStream Learn more More Ways to Learn Explore the video courses available on Confluent Developer. Apache Kafka 101 Learn the fundamentals of Kafka with this video course Kafka Connect 101 Learn the fundamentals of Kafka Connect with this video course Apache Flink SQL Learn how you can use Flink SQL with with Confluent Cloud for Apache Flink

---

### Flink SQL and Table API Reference in Confluent Cloud for Apache Flink | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/reference/overview.html

Flink SQL and Table API Reference in Confluent Cloud for Apache Flink¶ This section describes the SQL language support in Confluent Cloud for Apache Flink®, including Data Definition Language (DDL) statements, Data Manipulation Language (DML) statements, built-in functions, and the Table API. Apache Flink® SQL is based on Apache Calcite, which implements the SQL standard. Data Types¶ Flink SQL has a rich set of native data types that you can use in SQL statements and queries. Data Types Serialize and deserialize data¶ Data Type Mappings Reserved keywords¶ Some string combinations are reserved as keywords for future use. Flink SQL Reserved Keywords Related content¶ Stream Processing Concepts Time and Watermarks Note This website includes content developed at the Apache Software Foundation under the terms of the Apache License v2.

---

### SQL Statements in Confluent Cloud for Apache Flink | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/reference/statements/overview.html

DDL Statements in Confluent Cloud for Apache Flink¶ In Confluent Cloud for Apache Flink®, a statement is a high-level resource that’s created when you enter a SQL query. Data Definition Language (DDL) statements are imperative verbs that define metadata in Flink SQL by adding, changing, or deleting tables. Unlike Data Manipulation Language (DML) statements, DDL statements modify only metadata and don’t change data. When you want to change data, use DML statements. For valid lexical structure of statements, see Flink SQL Syntax in Confluent Cloud for Apache Flink. Available DDL statements¶ These are the available DDL statements in Confluent Cloud for Flink SQL. ALTER ALTER TABLE: Change properties of an existing table. ALTER MODEL: Rename an AI model or change model options. CREATE CREATE TABLE: Register a table into the current or specified catalog (Confluent Cloud environment). CREATE FUNCTION: Register a user-defined function (UDF) in the current database (Apache Kafka® cluster). CREATE MODEL: Create a new AI model. DESCRIBE DESCRIBE: Show properties of a table, AI model, or UDF. DROP DROP MODEL: Remove an AI model. DROP TABLE: Remove a table. DROP VIEW: Remove a view from a catalog. EXPLAIN EXPLAIN: View the query plan of a Flink SQL statement. RESET RESET: Reset the Flink SQL shell configuration to default settings. SET SET: Modify or list the Flink SQL shell configuration. SHOW SHOW CATALOGS: List all catalogs. SHOW CREATE MODEL: Show details about an AI inference model. SHOW CREATE TABLE: Show details about a table. SHOW CURRENT CATALOG: Show the current catalog. SHOW CURRENT DATABASE: Show the current database. SHOW DATABASES: List all databases in the current catalog. SHOW FUNCTIONS: List all functions in the current catalog and database. SHOW JOBS: List the status of all statements in the current catalog. SHOW MODELS: List all AI models that are registered in the current catalog. SHOW TABLES: List all tables for the current database. USE USE CATALOG: Set the current catalog. USE [database_name]: Set the current database. Related content¶ Flink SQL Syntax Flink SQL Queries Stream Processing Concepts Built-in Functions Note This website includes content developed at the Apache Software Foundation under the terms of the Apache License v2.

---

### SQL Functions in Confluent Cloud for Apache Flink | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/reference/functions/overview.html

Flink SQL Functions in Confluent Cloud for Apache Flink¶ Confluent Cloud for Apache Flink® enables you to do data transformations and other operations with the following built-in functions. Aggregate Functions Collection Functions Comparison Functions Conditional Functions Datetime Functions Hash Functions JSON Functions Model Inference Functions Numeric Functions String Functions Table API Functions Related content¶ User-defined Functions Create a User Defined Function Flink SQL Queries DDL Statements

---

### Confluent Documentation | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/reference/data-types.html

Confluent Documentation¶ Confluent Documentation Find the guides, samples, tutorials, API, and CLI reference to get started with the streaming data platform based on Apache Kafka®. Confluent Cloud A fully-managed data streaming platform with a cloud-native Apache Kafka® engine for elastic scaling, enterprise-grade security, stream processing, and governance. Confluent Platform An enterprise-grade distribution of Apache Kafka available on-premises as self-managed software, with enterprise-grade security, stream processing, and governance. Apache Kafka An open-source distributed data streaming engine used by thousands of companies to build streaming data pipelines and applications, powering mission-critical operational and analytics use cases. What's New Try out the latest data streaming platform features, including Confluent Cloud for Apache Flink® and Confluent for VS Code. Confluent Tableflow Represent Apache Kafka® topics as open table formats like Apache Iceberg™ and Delta Lake Confluent Cloud for Apache Flink A cloud-native, serverless service for Apache Flink that enables simple, scalable, and secure stream processing that integrates seamlessly with Kafka Confluent for VS Code Build, manage, and monitor data streaming applications with Confluent for VS Code Release Notes for Confluent Cloud Discover the latest features Confluent Cloud APIs Use the API to manage your deployment Cloud Network Management Learn about the latest networking features Featured Content Get started with the fully-managed Confluent Cloud Quick Start or on-premises using the Confluent Platform ZIP and TAR installation. Confluent Cloud Quick Start Get started with quickly with a fully-managed data streaming platform ZIP and TAR Installation Install Confluent Platform on-premises using ZIP and TAR archive Install Confluent CLI Manage Confluent Platform and Cloud using the command-line interface Confluent Platform Quick Start Use your laptop to quickly get started with Confluent Platform on-premises Get Started with Connect Get started and learn the Kafka Connect basics Cluster Types Learn about the Kafka cluster types available in Confluent Cloud Browse by Feature Find the guides, examples, and tutorials for Confluent features, including the Kafka consumer or the Kafka CLI tools. Kafka Streams Confluent Cloud | Confluent Platform Kafka Connect Confluent Cloud | Confluent Platform Schema Registry Confluent Cloud | Confluent Platform Apache Flink Confluent Cloud | Confluent Platform Networking Confluent Cloud Security Confluent Cloud | Confluent Platform Confluent CLI Confluent Cloud and Confluent Platform Kafka Clients Confluent Cloud | Confluent Platform Apache Kafka Learn more Kafka Consumer Confluent Cloud | Confluent Platform Kafka Producer Confluent Cloud | Confluent Platform Multi-Region Clusters Confluent Platform Stream Lineage Confluent Cloud Terraform Confluent Cloud Confluent for Kubernetes Confluent Platform Ansible Playbooks Confluent Platform Stream Governance Confluent Cloud | Confluent Platform Replicator Confluent Platform Cluster Linking Confluent Cloud | Confluent Platform Auto Data Balancer Confluent Platform WarpStream Learn more More Ways to Learn Explore the video courses available on Confluent Developer. Apache Kafka 101 Learn the fundamentals of Kafka with this video course Kafka Connect 101 Learn the fundamentals of Kafka Connect with this video course Apache Flink SQL Learn how you can use Flink SQL with with Confluent Cloud for Apache Flink

---

### Flink SQL Syntax in Confluent Cloud for Apache Flink | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/reference/sql-syntax.html

Flink SQL Syntax in Confluent Cloud for Apache Flink¶ SQL is a domain-specific language for managing and manipulating data. It’s used primarily to work with structured data, where the types and relationships across entities are well-defined. Originally adopted for relational databases, SQL is rapidly becoming the language of choice for stream processing. It’s declarative, expressive, and ubiquitous. The American National Standards Institute (ANSI) maintains a standard for the specification of SQL. Flink SQL is compliant with ANSI SQL 2011. Beyond the standard, there are many flavors and extensions to SQL so that it can express programs beyond what’s possible with the SQL 2011 grammar. Lexical structure¶ The grammar of Apache Flink® parses SQL using Apache Calcite, which supports standard ANSI SQL. Syntax¶ Flink SQL inputs are made up of a series of statements. Each statement is made up of a series of tokens and ends in a semicolon (;). The tokens that apply depend on the statement being invoked. A token is any keyword, identifier, backticked identifier, literal, or special character. By convention, tokens are separated by whitespace, unless there is no ambiguity in the grammar. This happens when tokens flank a special character. The following example statements are syntactically valid Flink SQL input: -- Create a users table. CREATE TABLE users ( user_id STRING, registertime BIGINT, gender STRING, regionid STRING ); -- Populate the table with mock users data. INSERT INTO users VALUES ('Thomas A. Anderson', 1677260724, 'male', 'Region_4'), ('Trinity', 1677260733, 'female', 'Region_4'), ('Morpheus', 1677260742, 'male', 'Region_8'); SELECT * FROM users; Keywords¶ Some tokens, such as SELECT, INSERT, and CREATE, are keywords. Keywords are reserved tokens that have a specific meaning in Flink’s syntax. They control their surrounding allowable tokens and execution semantics. Keywords are case insensitive, meaning SELECT and select are equivalent. You can’t create an identifier that is already a reserved word, unless you use backticked identifiers, for example, `table`. For a complete list of keywords, see Flink SQL Reserved Keywords. Identifiers¶ Identifiers are symbols that represent user-defined entities, like tables, columns, and other objects. For example, if you have a table named t1, t1 is an identifier for that table. By default, identifiers are case-sensitive, meaning t1 and T1 refer to different tables. Unless an identifier is backticked, it may be composed only of characters that are a letter, number, or underscore. There is no imposed limit on the number of characters. To make it possible to use any character in an identifier, you can enclose it in backtick characters (`) when you declare and use it. A backticked identifier is useful when you don’t control the data, so it might have special characters, or even keywords. If you want to use one of the keyword strings as an identifier, enclose them with backticks, for example: `value` `count` When you use backticked identifiers, Flink SQL captures the case exactly, and any future references to the identifier are case-sensitive. For example, if you declare the following table: CREATE TABLE `t1` ( id VARCHAR, `@MY-identifier-table-column!` INT); You must select from it by backticking the table name and column name and using the original casing: SELECT `@MY-identifier-table-column!` FROM `t1`; If you use an invalid identifier without enclosing it in backticks, you receive a SQL parse failed error. For example, the following SQL query tries to read records from a table named table-with-dashes, but the dash character (-) is not valid in an identifier. SELECT * FROM table-with-dashes; The error output resembles: SQL parse failed. Encountered "-" at line 1, column 20. You can fix the error by enclosing the identifier with backticks: SELECT * FROM `table-with-dashes`; Constants¶ There are three implicitly typed constants, or literals, in Flink SQL: strings, numbers, and booleans. String constants¶ A string constant is an arbitrary series of characters surrounded by single quotes ('), like 'Hello world'. To include a quote inside of a string literal, escape the quote by prefixing it with another quote, for example, 'You can call me ''Stuart'', or Stu.' Numeric constants¶ Numeric constants are accepted in the following forms: digits digits.[digits][e[+-]digits] [digits].digits[e[+-]digits] digitse[+-]digits where digits is one or more single-digit integers (0 through 9). At least one digit must be present before or after the decimal point, if there is one. At least one digit must follow the exponent symbol e, if there is one. No spaces, underscores, or any other characters are allowed in the constant. Numeric constants may also have a + or - prefix, but this is considered to be a function applied to the constant, not the constant itself. Here are some examples of valid numeric constants: 5 7.2 0.0087 1. .5 1e-3 1.332434e+2 +100 -250 Boolean constants¶ A boolean constant is represented as either the identifier true or false. Boolean constants are not case-sensitive, which means that true evaluates to the same value as TRUE. Operators¶ Operators are infix functions composed of special characters. Flink SQL doesn’t allow you to add user-space operators. For a complete list of operators, see Comparison Functions in Confluent Cloud for Apache Flink. Special characters¶ Some characters have a particular meaning that doesn’t correspond to an operator. The following list describes the special characters and their purposes. Parentheses (()) retain their usual meaning in programming languages for grouping expressions and controlling the order of evaluation. Brackets ([]) are used to work with arrays, both in their construction and subscript access. They also allow you to key into maps. Commas (,) delineate a discrete list of entities. The semi-colon (;) terminates a SQL statement. The asterisk (*), when used in particular syntax, is used as an “all” qualifier. This is seen most commonly in a SELECT command to retrieve all columns. The period (.) accesses a column in a table or a field in a struct data type. Comments¶ A comment is a string beginning with two dashes. It includes all of the content from the dashes to the end of the line: -- Here is a comment. You can also span a comment over multiple lines by using C-style syntax: /* Here is another comment. */ Lexical precedence¶ Operators are evaluated using the following order of precedence: *, /, % +, - =, >, <, >=, <=, <>, != NOT AND BETWEEN, LIKE, OR In an expression, when two operators have the same precedence level, they’re evaluated left-to-right, based on their position. You can enclose an expression in parentheses to force precedence or clarify precedence, for example, (5 + 2) * 3. Related content¶ Flink SQL Reserved Keywords Data Types Flink SQL Queries DDL Statements in Confluent Cloud for Apache Flink Note This website includes content developed at the Apache Software Foundation under the terms of the Apache License v2.

#### Code Examples

```sql
-- Create a users table.
CREATE TABLE users (
  user_id STRING,
  registertime BIGINT,
  gender STRING,
  regionid STRING
);

-- Populate the table with mock users data.
INSERT INTO users VALUES
  ('Thomas A. Anderson', 1677260724, 'male', 'Region_4'),
  ('Trinity', 1677260733, 'female', 'Region_4'),
  ('Morpheus', 1677260742, 'male', 'Region_8');

SELECT * FROM users;
```

```sql
CREATE TABLE `t1` (
  id VARCHAR,
  `@MY-identifier-table-column!` INT);
```

```sql
SELECT `@MY-identifier-table-column!` FROM `t1`;
```

```sql
SQL parse failed
```

```sql
table-with-dashes
```

```sql
SELECT * FROM table-with-dashes;
```

```sql
SQL parse failed. Encountered "-" at line 1, column 20.
```

```sql
SELECT * FROM `table-with-dashes`;
```

```sql
'Hello world'
```

```sql
'You can call me ''Stuart'', or Stu.'
```

```sql
1.332434e+2
```

```sql
-- Here is a comment.
```

```sql
/* Here is
   another comment.
*/
```

```sql
(5 + 2) * 3
```

---

### Predefined RBAC roles in Confluent Cloud | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/security/access-control/rbac/predefined-rbac-roles.html#flinkdeveloper-role

Predefined RBAC Roles in Confluent Cloud¶ You can use Confluent Cloud RBAC roles to control access to an organization, its environments, the clusters within each environment, and the Kafka resources on those clusters. A principal can be assigned multiple roles. Note the following: When SSO is enabled for an organization, a default group mapping (all-sso-users) is applied to all SSO user accounts and binds them to two predefined RBAC roles (FlinkDeveloper and DataDiscovery) that provide default user permissions to access Confluent Cloud resources in an SSO-enabled organization. These default user permissions can be customized by administrators. For more information, see Default user permissions. To use the Confluent Cloud Console or Confluent CLI, a user must be assigned at least one role. To allow a principal to inspect topics and view messages using the Confluent Cloud Console, you must minimally grant that account the DeveloperRead role. RBAC permissions for operations on granular Kafka resources (topics, consumer groups, and transactional IDs) are supported only on Standard, Enterprise, Dedicated, and Freight clusters. If a user or service account is deleted, all API keys and role bindings associated with that user or service account are also deleted. Limitations¶ Listed here are temporary limitations for Confluent Cloud resources and role bindings that you need to be aware of. Consumers and producers Any principal granted access to a Kafka cluster can view all of the consumers and producers of the cluster. This is subject to change and you must not rely on this access. To enable viewing consumers and producers, grant the Operator role. If you remove role bindings for a principal with an Organization, Environment, or Cluster scope that includes ksqlDB resource access, the principal’s API keys are not automatically deleted. A principal with an administrator role binding (OrganizationAdmin, EnvironmentAdmin, or CloudClusterAdmin) can create an API key for a ksqlDB cluster while they are an administrator and continue using that key even after their administrator role binding is removed. The API key continues allowing full access to the ksqlDB clusters. As a best practice, when you remove role bindings, make sure to remove any corresponding API keys. To learn more, see Delete unneeded API keys and service accounts. FlinkDeveloper To grant the FlinkDeveloper role on an environment, you can use the Confluent Cloud Console, Confluent CLI, or the REST API. KsqlAdmin Principals granted the KsqlAdmin role cannot create ksqlDB clusters, but can list and terminate the specific ksqlDB cluster the role is granted to. Principals with the KsqlAdmin role for a ksqlDB cluster has full access to all resources on the cluster, like streams and persistent queries. The KsqlAdmin role can create API keys that can access the ksqlDB cluster as long as the role has access to those clusters. When the role binding is removed, access is revoked. The KsqlAdmin role can be granted only by accounts that have the CloudClusterAdmin, EnvironmentAdmin, or OrganizationAdmin role. When a ksqlDB cluster is deleted, all KsqlAdmin role bindings assigned to that cluster are deleted. The KsqlAdmin role cannot grant access to a Kafka cluster or the ability to view Kafka ACLs or role bindings. Use other RBAC roles to grant access to Kafka clusters and resources. Predefined roles¶ Use predefined RBAC roles to grant principals granular access permissions to specific Confluent Cloud resources. A predefined role is a Confluent-defined job function assigned a set of permissions required to perform specific operations on Confluent resources bound to a principal and Confluent resources. The roles are updated as necessary when new resources or features are added to Confluent Cloud. When you assign a role to a principal, the role is bound to both the principal and the resources. For details about role bindings, see Manage RBAC role bindings on Confluent Cloud. Role categories¶ To help you focus on the roles that are relevant to your needs, here are listings of roles by category: Administration roles¶ AccountAdmin BillingAdmin CloudClusterAdmin ConnectManager DataSteward EnvironmentAdmin FlinkAdmin KsqlAdmin NetworkAdmin OrganizationAdmin ResourceKeyAdmin ResourceOwner Application development roles¶ DeveloperManage DeveloperRead DeveloperWrite FlinkDeveloper MetricsViewer Operator ResourceOwner The predefined RBAC roles available in Confluent Cloud are listed alphabetically. AccountAdmin¶ Principals granted the AccountAdmin role can: Describe, Create, Delete, or Invite user accounts within their organization. By itself, this role cannot view or alter the roles of other user accounts. To view or alter the roles of other user accounts, you must additionally be granted other roles. Describe, Alter, Create, or Delete service accounts within their organization. By itself, this role cannot view or alter the roles of other service accounts. To view or alter the roles of other user accounts, you must additionally be granted other roles. Alter, Create, Delete, and Describe operations for identity pool (IdentityPool) and group mapping (GroupMapping) resources. When combined with other role bindings, this role can manage the end-to-end provisioning of user and service accounts for Confluent Cloud resources they have access to. You can use the AccountAdmin role to create a dedicated account administrator for user and service accounts, but their access is limited based on the roles (other than AccountAdmin) that are granted to them. Usage example Instead of requiring the user with full access to all Confluent Cloud resources for your organization (OrganizationAdmin), you can grant someone managing one of your Kafka clusters the AccountAdmin role. This lets you create a dedicated account administrator for that cluster, who can manage the end-to-end provisioning of user and service accounts for that cluster, including inviting users, granting access, and creating service accounts. This delegation of responsibility can help you manage access to your resources more effectively while maintaining security. The following table shows the operations that can be performed on user and service accounts by a principal with only the AccountAdmin role, only the CloudClusterAdmin role, or both roles combined. The effective permissions are the union of the permissions granted by the two roles. Role User account Service account AccountAdmin Create, Delete Alter, Create, Delete CloudClusterAdmin Alter Access, Create, Describe, Invite Alter Access, Describe, Describe Access Combined Alter, Alter Access, Create, Delete, Describe, Describe Access, Invite Alter, Alter Access, Create, Delete, Describe, Describe Access Allow a cloud cluster administrator (with the CloudClusterAdmin role) to manage users and service accounts for their cluster. Assigner¶ Principals granted the Assigner role can: Assign a specific service account to perform a task within Confluent Cloud, such as executing a Flink SQL statement View the metadata about a service account. The Assigner role is bound on a specific service account using the specified ID. Usage example Marcus, an OrganizationAdmin, creates a service account called MarcusProjectSA. Marcus grants data access permissions to his new MarcusProjectSA service account. Marcus grants the Assigner role on his service account to Alice. Alice runs a Flink SQL statement, assigning the MarcusProjectSA service account to run it. BillingAdmin¶ Principals granted the BillingAdmin role have the following access permissions: View billing information: promotion code claims, invoices, and payment information (next scheduled payment and payment method). Update billing information: promotion code claim, payment method, and email address. View support plan information. View resources in the organization without access to data. Access the Costs API (billing/v1/costs). Use the Confluent Cloud Console to perform any of the following actions: View Monthly usage for selected month and year. View Charges by environment for selected environment. View or download invoices. View and copy Cloud Organization ID. View Payments, Next scheduled payment, Payment method, and Email address. Update Payment method and Email address. CloudClusterAdmin¶ Principals granted the CloudClusterAdmin role in Confluent Cloud have the following access permissions: Full access to manage the specified cluster. Cannot view Data portal or resources in the Global search bar on the Cloud Console UI. Cannot view or manage any other clusters, environments, or networking configurations for which they are not assigned a role. For temporary limitations that you should be aware of, review Limitations. The CloudClusterAdmin role allows principals to perform the following actions within clusters specified for the role and for service accounts in the organization: Grant and revoke access within the cluster. Full access to manage access control lists (ACLs) for the cluster. Service accounts: Create and Describe (list and view). Identity pools (OAuth and certificate): Create and Describe (list and view). Invite users. Create and configure ksqlDB clusters. Create and configure connectors. Create, view, update, and list connections to external services. View and manipulate topics through the Confluent Cloud Console and Confluent CLI. Create and manage Kafka resources, including topics, consumer groups, and transactional IDs. Reset consumer offsets. For more information, see Reset Consumer Offsets in Confluent Cloud. Assign role bindings on Kafka resources, including topics, consumer groups, and transactional IDs, on Standard, Enterprise, Dedicated, and Freight Kafka clusters, but not on Basic Kafka clusters. For AI model inference for Apache Flink, create, drop, list, and describe models, invoke models for prediction, and grant permissions on models. Also, see RBAC for model inference. No access to view or manage cluster networking configurations. No access to view or manage schema subjects. ConnectManager¶ Principals granted the ConnectManager role on a connector have the following permissions: Connector Operations View and describe connector details. Pause, restart, and resume connector operations. Read the configuration for the connector. Monitoring Monitor connector status and throughput metrics. View the cluster containing the connector. DataDiscovery¶ The DataDiscovery role is intended for users who need to discover and understand data in Confluent Cloud, but do not need to manage or evolve schemas, or manage business metadata. This role grants access permissions specific to Schema Registry, Stream Catalog, and Stream Lineage resources. All SSO user accounts are included in the default all-sso-users group mapping that binds them to the DataDiscovery and FlinkDeveloper roles and provides the users with default user permissions to access Confluent Cloud resources across any environments in an SSO-enabled organization. For more information, see Default user permissions. Principals granted the DataDiscovery role in Confluent Cloud have the following access. Allowed operations for Schema Registry include: Read access to all schema subjects within an organization or environment. Read access to schema compatibility For more RBAC details, see Access control (RBAC) for Confluent Cloud Schema Registry. Allowed operations for Stream Catalog include: Read access to Stream Catalog tags and business metadata within an organization or environment. Tags and business metadata can apply to schemas, records, fields, topics, connectors, clusters, environments, pipelines, and Apache Flink® compute pools. Read access to Stream Catalog search. This allows searching on schemas, schema subjects, records, fields, topics, tags, connectors, clusters, environments, pipelines, and Flink compute pools. For details, see Search entities and tags. For more RBAC details, see Access control (RBAC) for Stream Catalog and Stream Catalog business metadata: Allowed operations for Stream Lineage: Read access to Stream Lineage views. For more RBAC details, see Access control (RBAC) for Stream Lineage. To perform the operations above, the DataDiscovery role grants view access to all topics within all environments in an organization, but does not grant read access to any data being produced or consumed. DataSteward¶ The DataSteward role grants access permissions specific to Schema Registry, Stream Catalog, and Stream Lineage resources. For Stream Catalog, tags and business metadata can apply to a schema, record, field, or topic. Principals granted the DataSteward role in Confluent Cloud have the following access permissions on Schema Registry subjects, schema compatibility, and Stream Catalog business metadata: Read, write (create or evolve schema) access to all schema subjects within an environment. Can delete schema subject. Read, write access to Schema Registry compatibility. Can create, manage, read, write, and delete Stream Catalog business metadata. Can create, manage, read, write, and delete tags. Read access to Stream Catalog metadata. Read access to Stream Catalog search (permissions to search the catalog). Read access to Stream Lineage views. View access to all topics within an environment. No access to data being produced or consumed within a topic. DeveloperManage¶ Principals granted the DeveloperManage role in Confluent Cloud have the following access permissions for the specified resource (topic, consumer group, or transactional ID): Create and delete the resource (with the exception of schema subjects). No access to read, modify, or delete schemas. Can change global and subject level compatibility mode. For AI model inference for Apache Flink, create, drop, list, and describe models. Also, see RBAC for model inference. Cannot view Data portal or resources in the Global search bar on the Cloud Console. Cannot change the configuration of the resource. Updating the configuration requires the CloudClusterAdmin. Cannot manage access to the resource. No access to Stream Lineage views. For temporary limitations that you should be aware of, review Limitations. You can restrict the scope of the access permissions to a specified resource, all resources of a specific type, or all resources of a specific type whose names start with a given prefix. DeveloperRead¶ Principals granted the DeveloperRead role in Confluent Cloud have the following access permissions for the specified resource (topic, schema subject, consumer group, or transactional ID): For AI model inference for Apache Flink, list/describe models and invoke models for prediction. Also, see RBAC for model inference. Read-only access to the resource When granted read-only access on a topic, read permission is also required on on a consumer group in order to subscribe to the topic. This is not necessary if the consumer does manual partition assignment. See also: Consumer groups. When consuming from a topic using the Confluent CLI, the CLI will choose a consumer group name starting with confluent_cli_consumer_ by default, so the principal requires a DeveloperRead role binding on that prefix. Alternately, you can specify a consumer group with the --group flag and give a DeveloperRead role binding on your chosen group name. See confluent kafka topic consume. Cannot view Data portal or resources in the Global search bar on the Cloud Console UI. No access to Stream Lineage views. Reset a consumer offset when this role is scoped to the topic and the principal also has the ResourceOwner role scoped to the consumer group. For more information, see Reset Consumer Offsets in Confluent Cloud. For temporary limitations that you should be aware of, review Limitations. You can restrict the scope of the access permissions to a specified resource, all resources of a specific type, or all resources of a specific type whose names start with a given prefix. DeveloperWrite¶ Principals granted the DeveloperWrite role in Confluent Cloud have the following access permissions for the specified resource (topic, schema subject, or transactional ID): For AI model inference for Apache Flink, create and list/describe models, invoke models for prediction. Also, see RBAC for model inference. Write-only access to the resource For temporary limitations that you should be aware of, review Limitations. Cannot view Data portal or resources in the Global search bar on the Cloud Console UI. No access to Stream Lineage views You can restrict the scope of the access permissions to: a specified resource all resources of a specific type whose names start with a given prefix all resources of a specific type EnvironmentAdmin¶ Principals granted the EnvironmentAdmin role in Confluent Cloud have the following access permissions: Full access to manage all resources for the specified environment. Cannot view or manage any environments or resources in other environments in which they are not assigned a role. For details about how to create or rename an environment, refer to Environments on Confluent Cloud. The EnvironmentAdmin role allows principals to perform the following actions within the environment specified for the role: Grant and revoke access within their environment. Service accounts: Create and Describe (list and view). Identity pools (OAuth and certificate): Create and Describe (list and view). Create, describe, and delete BYOK keys. Create, view, update, and list connections to external services. Invite users. Enable or disable Schema Registry and manage schemas. Create and manage clusters. Manage cluster networking configurations for all clusters. Create and manage Kafka resources, including topics, consumer groups, and transactional IDs. Reset consumer offsets. For more information, see Reset Consumer Offsets in Confluent Cloud. Manage all Flink resources within the environment, including creating and managing compute pools, and creating and running Flink SQL statements. For AI model inference for Apache Flink, create, drop, list, and describe models, invoke models for prediction, and grant permissions on models. Also, see RBAC for model inference. View the result sets of statements run with their own permissions or that were run using a service account on which the principal has the Assigner role. Assign role bindings on Kafka resources, including topics, consumer groups, and transactional IDs, on Standard, Enterprise, Dedicated, and Freight Kafka clusters, but not on Basic Kafka clusters. Full access to Stream Lineage views. Create and manage ksqlDB clusters. Create and manage connectors. FlinkAdmin¶ The FlinkAdmin role allows full access to Flink resources in an environment. It should be given to users who will manage the compute pools used by Flink in an environment. The FlinkAdmin role grants permissions for the following operations: Manage compute pools in the environment, including creating, deleting, and altering their settings. View and list user and service accounts in the organization. Service accounts: Describe (list and view). Identity pools (OAuth and certificate): Describe (list and view). Create and run Flink SQL statements using any compute pool in the environment. View the result sets of statements run with their own permissions or that were run using a service account on which the principal has the Assigner role. Important Flink SQL statements are not private within an environment. All principals with FlinkDeveloper or FlinkAdmin role bindings can see any statements in plaintext. Confluent recommends creating a separate environment for users who run sensitive queries. Stop or delete statements in the environment. Create Flink SQL workspaces in the environment. View, alter, or delete Flink SQL workspaces created by a principal. Create, view, update, delete, and list artifacts in the environment. Create, view, delete, and list user defined functions in the environment. Create, view, update, and list connections to external services. FlinkDeveloper¶ The FlinkDeveloper role grants limited access to Flink resources in an organization or environment. You should grant this role to principals who run Flink statements, but do not manage compute pools. All SSO user accounts are included in the default all-sso-users group mapping that binds them to the DataDiscovery and FlinkDeveloper roles and provides the users with default user permissions to access Confluent Cloud resources across any environments in an SSO-enabled organization. For more information, see Default user permissions. The FlinkDeveloper role can perform the following operations: If the role is bound to the environment, create and run Flink SQL statements using any compute pool in the environment. If the role is bound to the environment, stop and delete Flink SQL statements that are running on any compute pool in the environment. View the result sets of statements run with their own permissions or run using a service account that the principal has the Assigner role. Create Flink SQL workspaces in the environment. Create, view, update, and list connections to external services. View, alter, or delete Flink SQL workspaces by the principal. Create, view, update, delete, and list artifacts in the environment. Create, view, delete, and list user defined functions in the environment. KsqlAdmin¶ The KsqlAdmin role grants access permissions to a specific ksqlDB cluster. Also, a KsqlAdmin can view the associated Organization, Environment, Kafka Cluster but not their resources. Principals granted the KsqlAdmin role have the following access permissions on the ksqlDB cluster: List the cluster. Terminate the cluster. Create, manage, read, write, and delete ksqlDB streams, tables, and queries. Service accounts: Create and Describe (list and view). Identity pools (OAuth and certificate): Create and Describe (list and view). For more information, see Grant Role-Based Access to ksqlDB on Confluent Cloud. MetricsViewer¶ The MetricsViewer role is intended to allow service accounts to access the Confluent Cloud Metrics in order to integrate with customer metrics platforms. For users who want interactive access to metrics, use the Operator role. Principals granted the MetricsViewer role in Confluent Cloud have the following access permissions: Access to the Confluent Cloud Metrics is applied at the Organization, Environment, or Cluster scope. Can import metrics into third-party metrics platforms Full access to Stream Lineage views For details about how to assign the MetricsViewer role binding to a service account, see Add the MetricsViewer role to a new service account. NetworkAdmin¶ The NetworkAdmin role enables network administrators to manage the lifecycle of Confluent Cloud networks and network connections, and to provision access for dedicated clusters in Confluent Cloud networks. Users assigned the NetworkAdmin role can: Create and provision Confluent Cloud networks and network connections for all environments in an organization and make those networks available to the platform or infrastructure teams within the organization to deploy Enterprise and Dedicated Kafka Confluent Cloud clusters. Full access to Stream Lineage views. Users assigned the NetworkAdmin role (typically, a dedicated network administrator) cannot access cluster resource data, such as topics, consumer groups, connectors, ksqlDB clusters, and Schema Registry. Operator¶ Note As of April 11, 2025, Confluent removed permissions to pause and resume connectors from the Operator role in Confluent Cloud, aligning with broad customer feedback. This change restores to administrators a read-only Operator role that can view metadata and monitor resources without the right to modify resources. You can grant users the ConnectManager role to monitor, pause, and resume connectors in your clusters. Principals granted the Operator role in Confluent Cloud have the following access permissions: Describe resources within the organization. This role can view basic information, or metadata, about the resources in the organization, including environments, clusters, connectors, topics, and consumer groups, but not including ksqlDB. Cannot list schema subjects. Subject names and associated metadata are available through the GraphQL APIs or searches on the Confluent Cloud Console. To learn more about RBAC and Schema Registry, see Access control (RBAC) for Confluent Cloud Schema Registry. Describe topics and consumer groups, but cannot read the messages in the topics. Describe API keys (for Kafka, Schema Registry, Flink, and ksqlDB) that are not owned by the principal. Describe, Create, Alter, or Delete API keys (for Kafka, Schema Registry, Flink, and ksqlDB) that are owned by the principal. Describe managed connectors. Describe and view pipelines. View Stream Lineage. View metrics for clusters (Kafka, Schema Registry, and KSQL) and connectors. Monitor the health of applications and clusters, including monitoring uptime. You can bind the Operator role to the organization, an environment, or a cluster. OrganizationAdmin¶ Principals granted the OrganizationAdmin role in Confluent Cloud have the following access permissions: Full access to manage all resources within an organization (including environments, clusters, compute pools, Flink SQL statements, Flink workspaces, topics, consumer groups, transactional IDs, ksqlDB clusters, connectors, schema registries, role bindings, and ACLs). For AI model inference for Apache Flink, create, drop, list/describe models, invoke models for prediction, and grant permissions on models. Also, see RBAC for model inference. The first user account created in an organization is automatically granted the OrganizationAdmin role, which allows that user to start granting the appropriate roles to other principals in the organization. The OrganizationAdmin role allows principals to perform the following actions: Grant and revoke access throughout the organization. Create, describe, and delete BYOK keys. Manage SSO group mappings (Create, Update, Delete, Describe, and List). For details, see Manage Group Mappings on Confluent Cloud. Invite and delete users. To delete a user or service account, you requires a role binding with the OrganizationAdmin or AccountAdmin role. You cannot remove your own OrganizationAdmin role binding. If you need to give up your administrative privileges, you must request that another user with the OrganizationAdmin role remove it for you. Each organization must have at least one user account assigned the OrganizationAdmin role. Identity pools (OAuth and certificate): Create, describe (list and view), update (alter), and delete. Service accounts: Create, describe (list and view), update (alter), and delete. Manage billing and support plans. Enable or disable Schema Registry and manage schemas. Create and manage environments. Create and manage clusters. Manage cluster network configurations for all clusters across all environments. Access and manage audit logs. Create and manage Kafka resources, including topics, consumer groups, and transactional IDs. Reset consumer offsets. For more information, see Reset Consumer Offsets in Confluent Cloud. Manage all Flink resources within the environment, including creating and managing compute pools, and creating and running Flink SQL statements. View the result sets of statements run with their own permissions or that were run using any service account. Assign role bindings on Kafka resources, including topics, consumer groups, and transactional IDs, on Standard, Enterprise, Dedicated, and Freight Kafka clusters, but not on Basic Kafka clusters. Full access to Stream Lineage views. Create and manage ksqlDB clusters. Create and manage connectors. Create, view, update, and list connections to external services. Notification of new OrganizationAdmin role bindings¶ When a new principal is granted the privileged OrganizationAdmin role binding, the following actions occur: An email notification is sent to all users with the OrganizationAdmin role-binding in the organization informing them that a new principal has been granted the OrganizationAdmin role. This email alerts existing administrators in case the operation was unintended. A Confluent Cloud audit log entry is added, triggered by the BindRoleForPrincipal auditable event method. Any user with the OrganizationAdmin role can review the log entry record to see the principal who granted the OrganizationAdmin role and the principal who received the role, and when the role binding was added. Note that Confluent Cloud audit logs are available only for Standard, Enterprise, Dedicated, and Freight clusters. ResourceKeyAdmin¶ Principals granted the ResourceKeyAdmin role can manage API keys for Kafka, Schema Registry, and ksqlDB clusters across their organization for service accounts. Only the OrganizationAdmin can manage API keys for Confluent Cloud resources. The ResourceKeyAdmin role grants permissions for the following operations: Alter, Create, Delete, and Describe API keys resource-scoped to a Kafka cluster, Schema Registry cluster, Flink region, or ksqlDB cluster. Note that the ResourceKeyAdmin role cannot create API keys for itself, but can create API keys for other service accounts. ResourceOwner¶ Principals granted the ResourceOwner role in Confluent Cloud have the following access permissions for the specified resource: topic, consumer group, transactional ID, connector, schema subject, pipeline, service account, identity pool, or group mapping: Read and write access to the resource. For AI model inference for Apache Flink, create, drop, list, and describe models, invoke models for prediction, and grant permissions on models. Also, see RBAC for model inference. Can update, delete, and manage access on the target resource. Create new workload identities for service accounts, OAuth identity pools, and certificate identity pools. For details, see Manage Workload Identities. For a existing resource, grant and revoke access permissions for the resource. Cannot create resources, unless granted to a prefix of the resource. Change the configuration of the resource. Reset consumer offsets when scoped to both the topic and consumer group. For more information, see Reset Consumer Offsets in Confluent Cloud. Manage RBAC permissions on the resource. The ResourceOwner role cannot manage ACLs. Manage API keys for clusters (Kafka, Schema Registry, Flink, and ksqlDB) owned by the principal. No access to Stream Lineage views. Cannot view Data portal or resources in the Global search bar on the Cloud Console UI. For temporary limitations that you should be aware of, review Limitations. You can restrict the scope of the access permissions to a specified resource, all resources of a specific type, or all resources of a specific type whose names start with a given prefix, or all resources of a specific type. Related content¶ Role-Based Access Control (overview) Manage RBAC role bindings Manage Workload Identities RBAC for components Use ACLs with RBAC Confluent Cloud Security: RBAC and ACL module [Confluent Developer]

#### Code Examples

```sql
all-sso-users
```

```sql
IdentityPool
```

```sql
GroupMapping
```

```sql
MarcusProjectSA
```

```sql
MarcusProjectSA
```

```sql
MarcusProjectSA
```

```sql
all-sso-users
```

```sql
confluent_cli_consumer_
```

```sql
all-sso-users
```

---

### Predefined RBAC roles in Confluent Cloud | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/security/access-control/rbac/predefined-rbac-roles.html#flinkadmin-role

Predefined RBAC Roles in Confluent Cloud¶ You can use Confluent Cloud RBAC roles to control access to an organization, its environments, the clusters within each environment, and the Kafka resources on those clusters. A principal can be assigned multiple roles. Note the following: When SSO is enabled for an organization, a default group mapping (all-sso-users) is applied to all SSO user accounts and binds them to two predefined RBAC roles (FlinkDeveloper and DataDiscovery) that provide default user permissions to access Confluent Cloud resources in an SSO-enabled organization. These default user permissions can be customized by administrators. For more information, see Default user permissions. To use the Confluent Cloud Console or Confluent CLI, a user must be assigned at least one role. To allow a principal to inspect topics and view messages using the Confluent Cloud Console, you must minimally grant that account the DeveloperRead role. RBAC permissions for operations on granular Kafka resources (topics, consumer groups, and transactional IDs) are supported only on Standard, Enterprise, Dedicated, and Freight clusters. If a user or service account is deleted, all API keys and role bindings associated with that user or service account are also deleted. Limitations¶ Listed here are temporary limitations for Confluent Cloud resources and role bindings that you need to be aware of. Consumers and producers Any principal granted access to a Kafka cluster can view all of the consumers and producers of the cluster. This is subject to change and you must not rely on this access. To enable viewing consumers and producers, grant the Operator role. If you remove role bindings for a principal with an Organization, Environment, or Cluster scope that includes ksqlDB resource access, the principal’s API keys are not automatically deleted. A principal with an administrator role binding (OrganizationAdmin, EnvironmentAdmin, or CloudClusterAdmin) can create an API key for a ksqlDB cluster while they are an administrator and continue using that key even after their administrator role binding is removed. The API key continues allowing full access to the ksqlDB clusters. As a best practice, when you remove role bindings, make sure to remove any corresponding API keys. To learn more, see Delete unneeded API keys and service accounts. FlinkDeveloper To grant the FlinkDeveloper role on an environment, you can use the Confluent Cloud Console, Confluent CLI, or the REST API. KsqlAdmin Principals granted the KsqlAdmin role cannot create ksqlDB clusters, but can list and terminate the specific ksqlDB cluster the role is granted to. Principals with the KsqlAdmin role for a ksqlDB cluster has full access to all resources on the cluster, like streams and persistent queries. The KsqlAdmin role can create API keys that can access the ksqlDB cluster as long as the role has access to those clusters. When the role binding is removed, access is revoked. The KsqlAdmin role can be granted only by accounts that have the CloudClusterAdmin, EnvironmentAdmin, or OrganizationAdmin role. When a ksqlDB cluster is deleted, all KsqlAdmin role bindings assigned to that cluster are deleted. The KsqlAdmin role cannot grant access to a Kafka cluster or the ability to view Kafka ACLs or role bindings. Use other RBAC roles to grant access to Kafka clusters and resources. Predefined roles¶ Use predefined RBAC roles to grant principals granular access permissions to specific Confluent Cloud resources. A predefined role is a Confluent-defined job function assigned a set of permissions required to perform specific operations on Confluent resources bound to a principal and Confluent resources. The roles are updated as necessary when new resources or features are added to Confluent Cloud. When you assign a role to a principal, the role is bound to both the principal and the resources. For details about role bindings, see Manage RBAC role bindings on Confluent Cloud. Role categories¶ To help you focus on the roles that are relevant to your needs, here are listings of roles by category: Administration roles¶ AccountAdmin BillingAdmin CloudClusterAdmin ConnectManager DataSteward EnvironmentAdmin FlinkAdmin KsqlAdmin NetworkAdmin OrganizationAdmin ResourceKeyAdmin ResourceOwner Application development roles¶ DeveloperManage DeveloperRead DeveloperWrite FlinkDeveloper MetricsViewer Operator ResourceOwner The predefined RBAC roles available in Confluent Cloud are listed alphabetically. AccountAdmin¶ Principals granted the AccountAdmin role can: Describe, Create, Delete, or Invite user accounts within their organization. By itself, this role cannot view or alter the roles of other user accounts. To view or alter the roles of other user accounts, you must additionally be granted other roles. Describe, Alter, Create, or Delete service accounts within their organization. By itself, this role cannot view or alter the roles of other service accounts. To view or alter the roles of other user accounts, you must additionally be granted other roles. Alter, Create, Delete, and Describe operations for identity pool (IdentityPool) and group mapping (GroupMapping) resources. When combined with other role bindings, this role can manage the end-to-end provisioning of user and service accounts for Confluent Cloud resources they have access to. You can use the AccountAdmin role to create a dedicated account administrator for user and service accounts, but their access is limited based on the roles (other than AccountAdmin) that are granted to them. Usage example Instead of requiring the user with full access to all Confluent Cloud resources for your organization (OrganizationAdmin), you can grant someone managing one of your Kafka clusters the AccountAdmin role. This lets you create a dedicated account administrator for that cluster, who can manage the end-to-end provisioning of user and service accounts for that cluster, including inviting users, granting access, and creating service accounts. This delegation of responsibility can help you manage access to your resources more effectively while maintaining security. The following table shows the operations that can be performed on user and service accounts by a principal with only the AccountAdmin role, only the CloudClusterAdmin role, or both roles combined. The effective permissions are the union of the permissions granted by the two roles. Role User account Service account AccountAdmin Create, Delete Alter, Create, Delete CloudClusterAdmin Alter Access, Create, Describe, Invite Alter Access, Describe, Describe Access Combined Alter, Alter Access, Create, Delete, Describe, Describe Access, Invite Alter, Alter Access, Create, Delete, Describe, Describe Access Allow a cloud cluster administrator (with the CloudClusterAdmin role) to manage users and service accounts for their cluster. Assigner¶ Principals granted the Assigner role can: Assign a specific service account to perform a task within Confluent Cloud, such as executing a Flink SQL statement View the metadata about a service account. The Assigner role is bound on a specific service account using the specified ID. Usage example Marcus, an OrganizationAdmin, creates a service account called MarcusProjectSA. Marcus grants data access permissions to his new MarcusProjectSA service account. Marcus grants the Assigner role on his service account to Alice. Alice runs a Flink SQL statement, assigning the MarcusProjectSA service account to run it. BillingAdmin¶ Principals granted the BillingAdmin role have the following access permissions: View billing information: promotion code claims, invoices, and payment information (next scheduled payment and payment method). Update billing information: promotion code claim, payment method, and email address. View support plan information. View resources in the organization without access to data. Access the Costs API (billing/v1/costs). Use the Confluent Cloud Console to perform any of the following actions: View Monthly usage for selected month and year. View Charges by environment for selected environment. View or download invoices. View and copy Cloud Organization ID. View Payments, Next scheduled payment, Payment method, and Email address. Update Payment method and Email address. CloudClusterAdmin¶ Principals granted the CloudClusterAdmin role in Confluent Cloud have the following access permissions: Full access to manage the specified cluster. Cannot view Data portal or resources in the Global search bar on the Cloud Console UI. Cannot view or manage any other clusters, environments, or networking configurations for which they are not assigned a role. For temporary limitations that you should be aware of, review Limitations. The CloudClusterAdmin role allows principals to perform the following actions within clusters specified for the role and for service accounts in the organization: Grant and revoke access within the cluster. Full access to manage access control lists (ACLs) for the cluster. Service accounts: Create and Describe (list and view). Identity pools (OAuth and certificate): Create and Describe (list and view). Invite users. Create and configure ksqlDB clusters. Create and configure connectors. Create, view, update, and list connections to external services. View and manipulate topics through the Confluent Cloud Console and Confluent CLI. Create and manage Kafka resources, including topics, consumer groups, and transactional IDs. Reset consumer offsets. For more information, see Reset Consumer Offsets in Confluent Cloud. Assign role bindings on Kafka resources, including topics, consumer groups, and transactional IDs, on Standard, Enterprise, Dedicated, and Freight Kafka clusters, but not on Basic Kafka clusters. For AI model inference for Apache Flink, create, drop, list, and describe models, invoke models for prediction, and grant permissions on models. Also, see RBAC for model inference. No access to view or manage cluster networking configurations. No access to view or manage schema subjects. ConnectManager¶ Principals granted the ConnectManager role on a connector have the following permissions: Connector Operations View and describe connector details. Pause, restart, and resume connector operations. Read the configuration for the connector. Monitoring Monitor connector status and throughput metrics. View the cluster containing the connector. DataDiscovery¶ The DataDiscovery role is intended for users who need to discover and understand data in Confluent Cloud, but do not need to manage or evolve schemas, or manage business metadata. This role grants access permissions specific to Schema Registry, Stream Catalog, and Stream Lineage resources. All SSO user accounts are included in the default all-sso-users group mapping that binds them to the DataDiscovery and FlinkDeveloper roles and provides the users with default user permissions to access Confluent Cloud resources across any environments in an SSO-enabled organization. For more information, see Default user permissions. Principals granted the DataDiscovery role in Confluent Cloud have the following access. Allowed operations for Schema Registry include: Read access to all schema subjects within an organization or environment. Read access to schema compatibility For more RBAC details, see Access control (RBAC) for Confluent Cloud Schema Registry. Allowed operations for Stream Catalog include: Read access to Stream Catalog tags and business metadata within an organization or environment. Tags and business metadata can apply to schemas, records, fields, topics, connectors, clusters, environments, pipelines, and Apache Flink® compute pools. Read access to Stream Catalog search. This allows searching on schemas, schema subjects, records, fields, topics, tags, connectors, clusters, environments, pipelines, and Flink compute pools. For details, see Search entities and tags. For more RBAC details, see Access control (RBAC) for Stream Catalog and Stream Catalog business metadata: Allowed operations for Stream Lineage: Read access to Stream Lineage views. For more RBAC details, see Access control (RBAC) for Stream Lineage. To perform the operations above, the DataDiscovery role grants view access to all topics within all environments in an organization, but does not grant read access to any data being produced or consumed. DataSteward¶ The DataSteward role grants access permissions specific to Schema Registry, Stream Catalog, and Stream Lineage resources. For Stream Catalog, tags and business metadata can apply to a schema, record, field, or topic. Principals granted the DataSteward role in Confluent Cloud have the following access permissions on Schema Registry subjects, schema compatibility, and Stream Catalog business metadata: Read, write (create or evolve schema) access to all schema subjects within an environment. Can delete schema subject. Read, write access to Schema Registry compatibility. Can create, manage, read, write, and delete Stream Catalog business metadata. Can create, manage, read, write, and delete tags. Read access to Stream Catalog metadata. Read access to Stream Catalog search (permissions to search the catalog). Read access to Stream Lineage views. View access to all topics within an environment. No access to data being produced or consumed within a topic. DeveloperManage¶ Principals granted the DeveloperManage role in Confluent Cloud have the following access permissions for the specified resource (topic, consumer group, or transactional ID): Create and delete the resource (with the exception of schema subjects). No access to read, modify, or delete schemas. Can change global and subject level compatibility mode. For AI model inference for Apache Flink, create, drop, list, and describe models. Also, see RBAC for model inference. Cannot view Data portal or resources in the Global search bar on the Cloud Console. Cannot change the configuration of the resource. Updating the configuration requires the CloudClusterAdmin. Cannot manage access to the resource. No access to Stream Lineage views. For temporary limitations that you should be aware of, review Limitations. You can restrict the scope of the access permissions to a specified resource, all resources of a specific type, or all resources of a specific type whose names start with a given prefix. DeveloperRead¶ Principals granted the DeveloperRead role in Confluent Cloud have the following access permissions for the specified resource (topic, schema subject, consumer group, or transactional ID): For AI model inference for Apache Flink, list/describe models and invoke models for prediction. Also, see RBAC for model inference. Read-only access to the resource When granted read-only access on a topic, read permission is also required on on a consumer group in order to subscribe to the topic. This is not necessary if the consumer does manual partition assignment. See also: Consumer groups. When consuming from a topic using the Confluent CLI, the CLI will choose a consumer group name starting with confluent_cli_consumer_ by default, so the principal requires a DeveloperRead role binding on that prefix. Alternately, you can specify a consumer group with the --group flag and give a DeveloperRead role binding on your chosen group name. See confluent kafka topic consume. Cannot view Data portal or resources in the Global search bar on the Cloud Console UI. No access to Stream Lineage views. Reset a consumer offset when this role is scoped to the topic and the principal also has the ResourceOwner role scoped to the consumer group. For more information, see Reset Consumer Offsets in Confluent Cloud. For temporary limitations that you should be aware of, review Limitations. You can restrict the scope of the access permissions to a specified resource, all resources of a specific type, or all resources of a specific type whose names start with a given prefix. DeveloperWrite¶ Principals granted the DeveloperWrite role in Confluent Cloud have the following access permissions for the specified resource (topic, schema subject, or transactional ID): For AI model inference for Apache Flink, create and list/describe models, invoke models for prediction. Also, see RBAC for model inference. Write-only access to the resource For temporary limitations that you should be aware of, review Limitations. Cannot view Data portal or resources in the Global search bar on the Cloud Console UI. No access to Stream Lineage views You can restrict the scope of the access permissions to: a specified resource all resources of a specific type whose names start with a given prefix all resources of a specific type EnvironmentAdmin¶ Principals granted the EnvironmentAdmin role in Confluent Cloud have the following access permissions: Full access to manage all resources for the specified environment. Cannot view or manage any environments or resources in other environments in which they are not assigned a role. For details about how to create or rename an environment, refer to Environments on Confluent Cloud. The EnvironmentAdmin role allows principals to perform the following actions within the environment specified for the role: Grant and revoke access within their environment. Service accounts: Create and Describe (list and view). Identity pools (OAuth and certificate): Create and Describe (list and view). Create, describe, and delete BYOK keys. Create, view, update, and list connections to external services. Invite users. Enable or disable Schema Registry and manage schemas. Create and manage clusters. Manage cluster networking configurations for all clusters. Create and manage Kafka resources, including topics, consumer groups, and transactional IDs. Reset consumer offsets. For more information, see Reset Consumer Offsets in Confluent Cloud. Manage all Flink resources within the environment, including creating and managing compute pools, and creating and running Flink SQL statements. For AI model inference for Apache Flink, create, drop, list, and describe models, invoke models for prediction, and grant permissions on models. Also, see RBAC for model inference. View the result sets of statements run with their own permissions or that were run using a service account on which the principal has the Assigner role. Assign role bindings on Kafka resources, including topics, consumer groups, and transactional IDs, on Standard, Enterprise, Dedicated, and Freight Kafka clusters, but not on Basic Kafka clusters. Full access to Stream Lineage views. Create and manage ksqlDB clusters. Create and manage connectors. FlinkAdmin¶ The FlinkAdmin role allows full access to Flink resources in an environment. It should be given to users who will manage the compute pools used by Flink in an environment. The FlinkAdmin role grants permissions for the following operations: Manage compute pools in the environment, including creating, deleting, and altering their settings. View and list user and service accounts in the organization. Service accounts: Describe (list and view). Identity pools (OAuth and certificate): Describe (list and view). Create and run Flink SQL statements using any compute pool in the environment. View the result sets of statements run with their own permissions or that were run using a service account on which the principal has the Assigner role. Important Flink SQL statements are not private within an environment. All principals with FlinkDeveloper or FlinkAdmin role bindings can see any statements in plaintext. Confluent recommends creating a separate environment for users who run sensitive queries. Stop or delete statements in the environment. Create Flink SQL workspaces in the environment. View, alter, or delete Flink SQL workspaces created by a principal. Create, view, update, delete, and list artifacts in the environment. Create, view, delete, and list user defined functions in the environment. Create, view, update, and list connections to external services. FlinkDeveloper¶ The FlinkDeveloper role grants limited access to Flink resources in an organization or environment. You should grant this role to principals who run Flink statements, but do not manage compute pools. All SSO user accounts are included in the default all-sso-users group mapping that binds them to the DataDiscovery and FlinkDeveloper roles and provides the users with default user permissions to access Confluent Cloud resources across any environments in an SSO-enabled organization. For more information, see Default user permissions. The FlinkDeveloper role can perform the following operations: If the role is bound to the environment, create and run Flink SQL statements using any compute pool in the environment. If the role is bound to the environment, stop and delete Flink SQL statements that are running on any compute pool in the environment. View the result sets of statements run with their own permissions or run using a service account that the principal has the Assigner role. Create Flink SQL workspaces in the environment. Create, view, update, and list connections to external services. View, alter, or delete Flink SQL workspaces by the principal. Create, view, update, delete, and list artifacts in the environment. Create, view, delete, and list user defined functions in the environment. KsqlAdmin¶ The KsqlAdmin role grants access permissions to a specific ksqlDB cluster. Also, a KsqlAdmin can view the associated Organization, Environment, Kafka Cluster but not their resources. Principals granted the KsqlAdmin role have the following access permissions on the ksqlDB cluster: List the cluster. Terminate the cluster. Create, manage, read, write, and delete ksqlDB streams, tables, and queries. Service accounts: Create and Describe (list and view). Identity pools (OAuth and certificate): Create and Describe (list and view). For more information, see Grant Role-Based Access to ksqlDB on Confluent Cloud. MetricsViewer¶ The MetricsViewer role is intended to allow service accounts to access the Confluent Cloud Metrics in order to integrate with customer metrics platforms. For users who want interactive access to metrics, use the Operator role. Principals granted the MetricsViewer role in Confluent Cloud have the following access permissions: Access to the Confluent Cloud Metrics is applied at the Organization, Environment, or Cluster scope. Can import metrics into third-party metrics platforms Full access to Stream Lineage views For details about how to assign the MetricsViewer role binding to a service account, see Add the MetricsViewer role to a new service account. NetworkAdmin¶ The NetworkAdmin role enables network administrators to manage the lifecycle of Confluent Cloud networks and network connections, and to provision access for dedicated clusters in Confluent Cloud networks. Users assigned the NetworkAdmin role can: Create and provision Confluent Cloud networks and network connections for all environments in an organization and make those networks available to the platform or infrastructure teams within the organization to deploy Enterprise and Dedicated Kafka Confluent Cloud clusters. Full access to Stream Lineage views. Users assigned the NetworkAdmin role (typically, a dedicated network administrator) cannot access cluster resource data, such as topics, consumer groups, connectors, ksqlDB clusters, and Schema Registry. Operator¶ Note As of April 11, 2025, Confluent removed permissions to pause and resume connectors from the Operator role in Confluent Cloud, aligning with broad customer feedback. This change restores to administrators a read-only Operator role that can view metadata and monitor resources without the right to modify resources. You can grant users the ConnectManager role to monitor, pause, and resume connectors in your clusters. Principals granted the Operator role in Confluent Cloud have the following access permissions: Describe resources within the organization. This role can view basic information, or metadata, about the resources in the organization, including environments, clusters, connectors, topics, and consumer groups, but not including ksqlDB. Cannot list schema subjects. Subject names and associated metadata are available through the GraphQL APIs or searches on the Confluent Cloud Console. To learn more about RBAC and Schema Registry, see Access control (RBAC) for Confluent Cloud Schema Registry. Describe topics and consumer groups, but cannot read the messages in the topics. Describe API keys (for Kafka, Schema Registry, Flink, and ksqlDB) that are not owned by the principal. Describe, Create, Alter, or Delete API keys (for Kafka, Schema Registry, Flink, and ksqlDB) that are owned by the principal. Describe managed connectors. Describe and view pipelines. View Stream Lineage. View metrics for clusters (Kafka, Schema Registry, and KSQL) and connectors. Monitor the health of applications and clusters, including monitoring uptime. You can bind the Operator role to the organization, an environment, or a cluster. OrganizationAdmin¶ Principals granted the OrganizationAdmin role in Confluent Cloud have the following access permissions: Full access to manage all resources within an organization (including environments, clusters, compute pools, Flink SQL statements, Flink workspaces, topics, consumer groups, transactional IDs, ksqlDB clusters, connectors, schema registries, role bindings, and ACLs). For AI model inference for Apache Flink, create, drop, list/describe models, invoke models for prediction, and grant permissions on models. Also, see RBAC for model inference. The first user account created in an organization is automatically granted the OrganizationAdmin role, which allows that user to start granting the appropriate roles to other principals in the organization. The OrganizationAdmin role allows principals to perform the following actions: Grant and revoke access throughout the organization. Create, describe, and delete BYOK keys. Manage SSO group mappings (Create, Update, Delete, Describe, and List). For details, see Manage Group Mappings on Confluent Cloud. Invite and delete users. To delete a user or service account, you requires a role binding with the OrganizationAdmin or AccountAdmin role. You cannot remove your own OrganizationAdmin role binding. If you need to give up your administrative privileges, you must request that another user with the OrganizationAdmin role remove it for you. Each organization must have at least one user account assigned the OrganizationAdmin role. Identity pools (OAuth and certificate): Create, describe (list and view), update (alter), and delete. Service accounts: Create, describe (list and view), update (alter), and delete. Manage billing and support plans. Enable or disable Schema Registry and manage schemas. Create and manage environments. Create and manage clusters. Manage cluster network configurations for all clusters across all environments. Access and manage audit logs. Create and manage Kafka resources, including topics, consumer groups, and transactional IDs. Reset consumer offsets. For more information, see Reset Consumer Offsets in Confluent Cloud. Manage all Flink resources within the environment, including creating and managing compute pools, and creating and running Flink SQL statements. View the result sets of statements run with their own permissions or that were run using any service account. Assign role bindings on Kafka resources, including topics, consumer groups, and transactional IDs, on Standard, Enterprise, Dedicated, and Freight Kafka clusters, but not on Basic Kafka clusters. Full access to Stream Lineage views. Create and manage ksqlDB clusters. Create and manage connectors. Create, view, update, and list connections to external services. Notification of new OrganizationAdmin role bindings¶ When a new principal is granted the privileged OrganizationAdmin role binding, the following actions occur: An email notification is sent to all users with the OrganizationAdmin role-binding in the organization informing them that a new principal has been granted the OrganizationAdmin role. This email alerts existing administrators in case the operation was unintended. A Confluent Cloud audit log entry is added, triggered by the BindRoleForPrincipal auditable event method. Any user with the OrganizationAdmin role can review the log entry record to see the principal who granted the OrganizationAdmin role and the principal who received the role, and when the role binding was added. Note that Confluent Cloud audit logs are available only for Standard, Enterprise, Dedicated, and Freight clusters. ResourceKeyAdmin¶ Principals granted the ResourceKeyAdmin role can manage API keys for Kafka, Schema Registry, and ksqlDB clusters across their organization for service accounts. Only the OrganizationAdmin can manage API keys for Confluent Cloud resources. The ResourceKeyAdmin role grants permissions for the following operations: Alter, Create, Delete, and Describe API keys resource-scoped to a Kafka cluster, Schema Registry cluster, Flink region, or ksqlDB cluster. Note that the ResourceKeyAdmin role cannot create API keys for itself, but can create API keys for other service accounts. ResourceOwner¶ Principals granted the ResourceOwner role in Confluent Cloud have the following access permissions for the specified resource: topic, consumer group, transactional ID, connector, schema subject, pipeline, service account, identity pool, or group mapping: Read and write access to the resource. For AI model inference for Apache Flink, create, drop, list, and describe models, invoke models for prediction, and grant permissions on models. Also, see RBAC for model inference. Can update, delete, and manage access on the target resource. Create new workload identities for service accounts, OAuth identity pools, and certificate identity pools. For details, see Manage Workload Identities. For a existing resource, grant and revoke access permissions for the resource. Cannot create resources, unless granted to a prefix of the resource. Change the configuration of the resource. Reset consumer offsets when scoped to both the topic and consumer group. For more information, see Reset Consumer Offsets in Confluent Cloud. Manage RBAC permissions on the resource. The ResourceOwner role cannot manage ACLs. Manage API keys for clusters (Kafka, Schema Registry, Flink, and ksqlDB) owned by the principal. No access to Stream Lineage views. Cannot view Data portal or resources in the Global search bar on the Cloud Console UI. For temporary limitations that you should be aware of, review Limitations. You can restrict the scope of the access permissions to a specified resource, all resources of a specific type, or all resources of a specific type whose names start with a given prefix, or all resources of a specific type. Related content¶ Role-Based Access Control (overview) Manage RBAC role bindings Manage Workload Identities RBAC for components Use ACLs with RBAC Confluent Cloud Security: RBAC and ACL module [Confluent Developer]

#### Code Examples

```sql
all-sso-users
```

```sql
IdentityPool
```

```sql
GroupMapping
```

```sql
MarcusProjectSA
```

```sql
MarcusProjectSA
```

```sql
MarcusProjectSA
```

```sql
all-sso-users
```

```sql
confluent_cli_consumer_
```

```sql
all-sso-users
```

---

### Grant Role-Based Access for Flink SQL Statements in Confluent Cloud for Apache Flink | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/operate-and-deploy/flink-rbac.html#flink-rbac-grant-sa-and-user-permission-for-sql-statements

Grant Role-Based Access in Confluent Cloud for Apache Flink¶ When deploying Flink SQL statements in production, you must configure appropriate access controls for different types of users and workloads. Confluent Cloud for Apache Flink® supports Role-based Access Control (RBAC) with these roles: FlinkAdmin: Full access to Flink resources and compute pool management FlinkDeveloper: Limited access for running statements but not managing infrastructure Assigner: Enables delegation of statement execution to service accounts Layered permission model: Flink permissions follow a layered approach. Start with base permissions required for all Flink operations, then add additional layers based on what users need to accomplish. Operational considerations: Use service accounts for production workloads and apply least-privilege principles by granting only the permissions needed for each use case. For complete role definitions, see Predefined RBAC Roles. Permission layers Common user scenarios Production best practices Access for UDF Logging Audit log events Reference Permission layers¶ Flink permissions follow a layered approach, in which each layer builds upon the previous one. This design enables you to grant only the permissions needed for each use case, following the principle of least privilege. These are the permission layers: Base/required layer: Fundamental permissions needed for all Flink operations Data access layer: Read and write access to specific tables and topics Table management layer: Create, alter, and delete tables Administrative layer: Manage compute pools and infrastructure Logging permissions layer: Access to UDF logs and audit events Start with the base layer and add additional layers as needed for your specific use cases. Base/required layer¶ All Flink user accounts need these permissions. Flink access¶ Choose the appropriate Flink role based on the user’s responsibilities: FlinkDeveloper: Can create and run statements, manage workspaces and artifacts, but can’t manage compute pools FlinkAdmin: All FlinkDeveloper capabilities plus compute pool management (create, delete, alter compute pool settings) Run the following commands to grant the necessary permissions. # For most users (statement execution and development) confluent iam rbac role-binding create \ --environment ${ENV_ID} \ --principal User:${USER_ID} \ --role FlinkDeveloper # For infrastructure administrators confluent iam rbac role-binding create \ --environment ${ENV_ID} \ --principal User:${USER_ID} \ --role FlinkAdmin Kafka Transactional-Id permissions¶ Flink uses Kafka transactions to ensure exactly-once processing semantics. All Flink statements require: DeveloperRead on Transactional-Id _confluent-flink_* (to read transaction state) DeveloperWrite on Transactional-Id _confluent-flink_* (to create and manage transactions) Run the following commands to grant the necessary permissions. # Read transaction state confluent iam rbac role-binding create \ --role DeveloperRead \ --principal User:${USER_ID} \ --environment ${ENV_ID} \ --cloud-cluster ${KAFKA_ID} \ --kafka-cluster ${KAFKA_ID} \ --resource Transactional-Id:_confluent-flink_ \ --prefix # Create and manage transactions confluent iam rbac role-binding create \ --role DeveloperWrite \ --principal User:${USER_ID} \ --environment ${ENV_ID} \ --cloud-cluster ${KAFKA_ID} \ --kafka-cluster ${KAFKA_ID} \ --resource Transactional-Id:_confluent-flink_ \ --prefix Data access layer¶ The data access layer provides permissions for reading from and writing to existing tables in your Flink statements. This layer builds on the base layer and adds specific access to Kafka topics and Schema Registry subjects that your statements need to interact with. Read from existing tables¶ When your Flink SQL statements read from tables, for example, by using a SELECT * FROM my_table statement, you need these roles: DeveloperRead on the Kafka topic DeveloperRead on the Schema Registry subject Run the following commands to grant the necessary permissions. # Kafka topic read permission confluent iam rbac role-binding create \ --role DeveloperRead \ --principal User:${USER_ID} \ --environment ${ENV_ID} \ --cloud-cluster ${KAFKA_ID} \ --kafka-cluster ${KAFKA_ID} \ --resource Topic:${TOPIC_NAME} # Schema Registry subject read permission confluent iam rbac role-binding create \ --role DeveloperRead \ --principal User:${USER_ID} \ --environment ${ENV_ID} \ --cloud-cluster ${SR_ID} \ --schema-registry-cluster ${SR_ID} \ --resource Subject:${SUBJECT_NAME} Write to existing tables¶ When your Flink SQL statements write to tables, for example, by using an INSERT INTO my_sink_table statement, you need the following roles: DeveloperWrite on the Kafka topic DeveloperRead on the Schema Registry subject, to validate data format Run the following commands to grant the necessary permissions. # Kafka topic write permission confluent iam rbac role-binding create \ --role DeveloperWrite \ --principal User:${USER_ID} \ --environment ${ENV_ID} \ --cloud-cluster ${KAFKA_ID} \ --kafka-cluster ${KAFKA_ID} \ --resource Topic:${TOPIC_NAME} # Schema Registry subject read permission, to validate data format confluent iam rbac role-binding create \ --role DeveloperRead \ --principal User:${USER_ID} \ --environment ${ENV_ID} \ --cloud-cluster ${SR_ID} \ --schema-registry-cluster ${SR_ID} \ --resource Subject:${SUBJECT_NAME} Table management layer¶ The table management layer provides permissions for creating and modifying tables in your Flink statements. This layer builds on the data access layer and adds specific access to Kafka topics and Schema Registry subjects that your statements need to interact with. Create new tables¶ When your Flink SQL statements create new tables, for example, by using a CREATE TABLE or CREATE TABLE AS SELECT statement, you need the following roles: DeveloperManage on Kafka topics, to create topics DeveloperWrite on Schema Registry subjects, to create schemas Run the following commands to grant the necessary permissions. # Kafka topic create/manage permission confluent iam rbac role-binding create \ --role DeveloperManage \ --principal User:${USER_ID} \ --environment ${ENV_ID} \ --cloud-cluster ${KAFKA_ID} \ --kafka-cluster ${KAFKA_ID} \ --resource Topic:${TABLE_PREFIX} \ --prefix # Schema Registry subject create/write permission confluent iam rbac role-binding create \ --role DeveloperWrite \ --principal User:${USER_ID} \ --environment ${ENV_ID} \ --cloud-cluster ${SR_ID} \ --schema-registry-cluster ${SR_ID} \ --resource Subject:${TABLE_PREFIX} \ --prefix Modify existing tables¶ When your Flink SQL statements modify table structures, for example, by using an ALTER TABLE statement for watermarks, computed columns, or column type changes, you need the following roles: DeveloperManage on the Kafka topic, for table structure changes DeveloperWrite on the Schema Registry subject, for schema evolution Run the following commands to grant the necessary permissions. # Kafka topic manage permission, for table structure changes confluent iam rbac role-binding create \ --role DeveloperManage \ --principal User:${USER_ID} \ --environment ${ENV_ID} \ --cloud-cluster ${KAFKA_ID} \ --kafka-cluster ${KAFKA_ID} \ --resource Topic:${TABLE_NAME} # Schema Registry subject write permission, for schema evolution confluent iam rbac role-binding create \ --role DeveloperWrite \ --principal User:${USER_ID} \ --environment ${ENV_ID} \ --cloud-cluster ${SR_ID} \ --schema-registry-cluster ${SR_ID} \ --resource Subject:${TABLE_NAME} Administrative layer¶ The administrative layer provides permissions for managing Flink compute pools and infrastructure. This layer builds on the table management layer and adds specific access to Flink resources that your statements need to interact with. The following roles can manage Flink compute pools and infrastructure. FlinkAdminThis role is for Flink-specific administrative access. It provides these capabilities: Manage compute pools (create, delete, and alter compute pool settings) All FlinkDeveloper capabilities (statements, workspaces, and artifacts) Most common choice for Flink-focused administrators EnvironmentAdminThis role provides environment-wide administrative access. It provides these capabilities: All Flink administrative capabilities plus broader environment management Typically assigned for other reasons (managing multiple services in an environment) OrganizationAdminThis role provides organization-wide administrative access. It provides these capabilities: All Flink administrative capabilities plus organization-wide management Typically assigned for other reasons, like managing the entire organization Use FlinkAdmin for users who primarily manage Flink infrastructure. Users with EnvironmentAdmin or OrganizationAdmin roles already have the necessary Flink administrative capabilities. For complete role definitions and capabilities, see Predefined RBAC Roles in Confluent Cloud. Logging permissions layer¶ The logging permissions layer provides permissions for accessing UDF logs and audit events related to your Flink statements. This layer builds on the administrative layer and adds specific access to Kafka topics that your statements need to interact with. UDF logging access¶ To access UDF logs, you need these roles: FlinkAdmin or FlinkDeveloper role: provides describe access to UDF logs DeveloperRead on the UDF log topics: to read the actual log messages CloudClusterAdmin role: can manage logging settings by enabling or disabling logging Run the following command to grant the necessary permissions. confluent iam rbac role-binding create \ --role DeveloperRead \ --principal User:${USER_ID} \ --environment ${ENV_ID} \ --cloud-cluster ${LOGGING_KAFKA_ID} \ --kafka-cluster ${LOGGING_KAFKA_ID} \ --resource Topic:${UDF_LOG_TOPIC} How Flink permissions work¶ Understanding the Flink permission model helps you make informed decisions about access control and troubleshoot permission issues effectively. Principal-Based Access Control¶ Flink uses a principal-based permission model, in which statements inherit all permissions from the principal that runs them. The principal can be a user or a service account. The following key concepts help you understand how Flink permissions work. Statements are not principals - A Flink SQL statement doesn’t have its own permissions. It uses the permissions of the principal that runs it. Flexible principal assignment - You can run statements under your user account, which is recommended for ad-hoc queries, or under a service account, which is recommended for production workloads. Permission inheritance - A statement can access any data that the principal has permissions to access in that region, even across environments. For example, if a service account has DeveloperRead on topics in multiple environments, any statement running under this service account can read from topics in all of these environments, when in the same region. Separation of control plane and data plane access¶ Flink separates access into two distinct planes: the control plane and the data plane. Understanding this separation is key to configuring permissions correctly. Control plane access (infrastructure)¶ This is managed by Flink-specific roles and governs what actions you can perform within the Flink service. Control plane access has these characteristics: Controls who can create statements, manage compute pools, and other Flink resources Managed by using FlinkAdmin and FlinkDeveloper roles Environment-scoped permissions, which apply to all environments in an organization, and organization-scoped permissions, which apply to all environments in an organization Data plane access (data)¶ This is managed by Kafka and Schema Registry roles and governs which data that your Flink statements can interact with. Data plane access has these characteristics: Controls which data your statements can read from and write to Managed by using DeveloperRead, DeveloperWrite, and DeveloperManage roles Resource-scoped permissions (topics, subjects) Data plane access is important because a user needs permissions on both planes to execute a Flink SQL statement successfully. For example, a user might have the FlinkDeveloper role (control plane access to create a statement), but if they lack DeveloperRead on a source topic (data plane access), the statement fails at runtime. In contrast, a principal with extensive data access but no Flink role can’t create statements in the first place. Compute pools as shared infrastructure¶ It’s important to understand that compute pools are resources, not principals. A compute pool provides the computational infrastructure for running statements. Compute pools don’t have their own permissions or identity. Multiple users can share the same compute pool if they have appropriate Flink roles. The principal running the statement determines data access, not the compute pool. For example, users Alice and Bob both have the FlinkDeveloper role and can use the same compute pool. Alice’s statements access data based on Alice’s permissions, while Bob’s statements use Bob’s permissions, even when running on the same compute pool. Cross-environment data access¶ Flink statements can access data across environment boundaries based on the principal’s permissions. For example, a statement in Environment A can read from topics in Environment B if the principal has: FlinkDeveloper role in Environment A, to create the statement DeveloperRead role on the topics in Environment B, to access the data Cross-environment data access is important in these use cases: Cross-environment analytics and reporting Data pipeline orchestration across multiple environments Centralized processing with distributed data sources Important Grant cross-environment permissions carefully, because a statement has broad access based on its principal’s permissions. Common user scenarios¶ This section describes common user scenarios and the required permission configurations for each. These scenarios follow the layered permission model, starting with base permissions and adding additional layers as needed. Choose the scenario that best matches your use case, then follow the corresponding permission setup instructions. Developers¶ Assign the following permissions to developer accounts: Base/Required Layer (Flink Developer role + Transactional-Id permissions) Data Access Layer (read/write access to existing tables) Table Management Layer (for creating and modifying tables) Run the commands shown in the previous sections to grant the necessary permissions. Production workloads (service accounts)¶ For automated deployments and long-running statements, use service accounts to ensure stable identity that isn’t affected by changes to user accounts. Setup options¶ Broad-access approach Create a service account and grant the EnvironmentAdmin role. Grant a user the Assigner role on the service account. Deploy statements using the service account. Least-privilege approach Create a service account and grant base/required layer permissions. Grant specific Data Access Layer and Table Management Layer permissions as needed. Grant a user account the Assigner role on the service account. Run the following commands to grant the necessary permissions. # Create service account confluent iam service-account create ${SA_NAME} \ --description "${SA_DESCRIPTION}" # Broad access: Grant EnvironmentAdmin role confluent iam rbac role-binding create \ --environment ${ENV_ID} \ --principal User:${SERVICE_ACCOUNT_ID} \ --role EnvironmentAdmin # Grant user Assigner role (for both approaches) confluent iam rbac role-binding create \ --principal User:${USER_ID} \ --resource service-account:${SERVICE_ACCOUNT_ID} \ --role Assigner For the least-privilege approach, run the commands in the previous layer sections, using the service account as the principal instead of a user account. Administrators (infrastructure management)¶ For managing compute pools and Flink infrastructure, grant an administrative layer role. These are the administrative roles that can manage Flink infrastructure: FlinkAdmin: Most common choice for Flink-focused administrators EnvironmentAdmin: If they already manage other services in the environment OrganizationAdmin: If they already manage the entire organization Production best practices¶ Grant permissions incrementally, starting with base permissions and adding additional layers as needed for your production use cases. Start with base/required layer - Grant fundamental Flink and Kafka permissions. Add data access - Grant read/write access to existing tables as needed. Add capabilities - Table management, administrative access as required. Validate each layer - Test functionality after adding each permission layer. Service account delegation pattern¶ For automated deployments, run the following command to grant the Assigner role on production service accounts. # CI/CD service account with Assigner role on production service accounts confluent iam rbac role-binding create \ --principal User:${CICD_SA_ID} \ --resource service-account:${PROD_SA_ID} \ --role Assigner Access for UDF Logging¶ OrganizationAdmin and EnvironmentAdmin roles have full permission to enable and disable custom code logging. The FlinkAdmin and FlinkDeveloper roles have permission to describe custom code logs. The CloudClusterAdmin role can disable logging and delete logs by deleting the Kafka cluster associated with the custom logging destination. Any security principal that has READ permission to the destination Kafka cluster has READ access to the actual log topics. Audit log events¶ Auditable event methods for the FLINK_WORKSPACE and STATEMENT resource types are triggered by operations on a Flink workspace and generate event messages that are sent to the audit log cluster, where they are stored as event records in a Kafka topic. For more information, see Auditable Event Methods. Reference¶ Permission summary by layer¶ Layer Kafka Schema Registry Flink Base/Required Transactional-Id – FlinkDeveloper OR FlinkAdmin Data Access DeveloperRead/Write DeveloperRead/Write – Table Management DeveloperManage DeveloperWrite – Administrative – – FlinkAdmin, EnvironmentAdmin, or OrganizationAdmin Logging DeveloperRead (UDF logs) – – Access to Flink resources¶ The following table shows which Flink resources the RBAC roles can access. “CRUD” stands for “Create, Read, Update, Delete”. Scope Statements Workspaces Compute pools Artifacts User-defined functions UDF logging AI inference models Kafka clusters Kafka Topics EnvironmentAdmin CRUD CRUD CRUD CRUD CRUD CRUD CRUD CRUD CRUD FlinkAdmin CRUD CRUD CRUD CRUD CRUD -R– CRUD – – FlinkDeveloper CRUD CRUD -R– CRUD CRUD [1] -R– CRUD [1] – – OrganizationAdmin CRUD CRUD CRUD CRUD CRUD CRUD CRUD CRUD CRUD [1](1, 2) Requires access to cluster. Related content¶ Auditable Event Methods DDL Statements Manage RBAC Role Bindings Role-based Access Control (RBAC) Service Accounts UDF logs Note This website includes content developed at the Apache Software Foundation under the terms of the Apache License v2.

#### Code Examples

```sql
# For most users (statement execution and development)
confluent iam rbac role-binding create \
  --environment ${ENV_ID} \
  --principal User:${USER_ID} \
  --role FlinkDeveloper

# For infrastructure administrators
confluent iam rbac role-binding create \
  --environment ${ENV_ID} \
  --principal User:${USER_ID} \
  --role FlinkAdmin
```

```sql
_confluent-flink_*
```

```sql
_confluent-flink_*
```

```sql
# Read transaction state
confluent iam rbac role-binding create \
  --role DeveloperRead \
  --principal User:${USER_ID} \
  --environment ${ENV_ID} \
  --cloud-cluster ${KAFKA_ID} \
  --kafka-cluster ${KAFKA_ID} \
  --resource Transactional-Id:_confluent-flink_ \
  --prefix

# Create and manage transactions
confluent iam rbac role-binding create \
  --role DeveloperWrite \
  --principal User:${USER_ID} \
  --environment ${ENV_ID} \
  --cloud-cluster ${KAFKA_ID} \
  --kafka-cluster ${KAFKA_ID} \
  --resource Transactional-Id:_confluent-flink_ \
  --prefix
```

```sql
SELECT * FROM my_table
```

```sql
# Kafka topic read permission
confluent iam rbac role-binding create \
  --role DeveloperRead \
  --principal User:${USER_ID} \
  --environment ${ENV_ID} \
  --cloud-cluster ${KAFKA_ID} \
  --kafka-cluster ${KAFKA_ID} \
  --resource Topic:${TOPIC_NAME}

# Schema Registry subject read permission
confluent iam rbac role-binding create \
  --role DeveloperRead \
  --principal User:${USER_ID} \
  --environment ${ENV_ID} \
  --cloud-cluster ${SR_ID} \
  --schema-registry-cluster ${SR_ID} \
  --resource Subject:${SUBJECT_NAME}
```

```sql
INSERT INTO my_sink_table
```

```sql
# Kafka topic write permission
confluent iam rbac role-binding create \
  --role DeveloperWrite \
  --principal User:${USER_ID} \
  --environment ${ENV_ID} \
  --cloud-cluster ${KAFKA_ID} \
  --kafka-cluster ${KAFKA_ID} \
  --resource Topic:${TOPIC_NAME}

# Schema Registry subject read permission, to validate data format
confluent iam rbac role-binding create \
  --role DeveloperRead \
  --principal User:${USER_ID} \
  --environment ${ENV_ID} \
  --cloud-cluster ${SR_ID} \
  --schema-registry-cluster ${SR_ID} \
  --resource Subject:${SUBJECT_NAME}
```

```sql
CREATE TABLE
```

```sql
CREATE TABLE AS SELECT
```

```sql
# Kafka topic create/manage permission
confluent iam rbac role-binding create \
  --role DeveloperManage \
  --principal User:${USER_ID} \
  --environment ${ENV_ID} \
  --cloud-cluster ${KAFKA_ID} \
  --kafka-cluster ${KAFKA_ID} \
  --resource Topic:${TABLE_PREFIX} \
  --prefix

# Schema Registry subject create/write permission
confluent iam rbac role-binding create \
  --role DeveloperWrite \
  --principal User:${USER_ID} \
  --environment ${ENV_ID} \
  --cloud-cluster ${SR_ID} \
  --schema-registry-cluster ${SR_ID} \
  --resource Subject:${TABLE_PREFIX} \
  --prefix
```

```sql
ALTER TABLE
```

```sql
# Kafka topic manage permission, for table structure changes
confluent iam rbac role-binding create \
  --role DeveloperManage \
  --principal User:${USER_ID} \
  --environment ${ENV_ID} \
  --cloud-cluster ${KAFKA_ID} \
  --kafka-cluster ${KAFKA_ID} \
  --resource Topic:${TABLE_NAME}

# Schema Registry subject write permission, for schema evolution
confluent iam rbac role-binding create \
  --role DeveloperWrite \
  --principal User:${USER_ID} \
  --environment ${ENV_ID} \
  --cloud-cluster ${SR_ID} \
  --schema-registry-cluster ${SR_ID} \
  --resource Subject:${TABLE_NAME}
```

```sql
confluent iam rbac role-binding create \
  --role DeveloperRead \
  --principal User:${USER_ID} \
  --environment ${ENV_ID} \
  --cloud-cluster ${LOGGING_KAFKA_ID} \
  --kafka-cluster ${LOGGING_KAFKA_ID} \
  --resource Topic:${UDF_LOG_TOPIC}
```

```sql
# Create service account
confluent iam service-account create ${SA_NAME} \
  --description "${SA_DESCRIPTION}"

# Broad access: Grant EnvironmentAdmin role
confluent iam rbac role-binding create \
  --environment ${ENV_ID} \
  --principal User:${SERVICE_ACCOUNT_ID} \
  --role EnvironmentAdmin

# Grant user Assigner role (for both approaches)
confluent iam rbac role-binding create \
  --principal User:${USER_ID} \
  --resource service-account:${SERVICE_ACCOUNT_ID} \
  --role Assigner
```

```sql
# CI/CD service account with Assigner role on production service accounts
confluent iam rbac role-binding create \
  --principal User:${CICD_SA_ID} \
  --resource service-account:${PROD_SA_ID} \
  --role Assigner
```

```sql
FLINK_WORKSPACE
```

---

### Private Networking with Confluent Cloud for Apache Flink | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/concepts/flink-private-networking.html#flink-sql-private-networking

Private Networking with Confluent Cloud for Apache Flink¶ Confluent Cloud for Apache Flink® supports private networking on AWS, Azure, and Google Cloud. This feature enables Flink to securely read and write data stored in Confluent Cloud clusters that are located in private networking, with no data flowing to the public internet. With private networking, you can use Flink and Apache Kafka® together for stream processing in Confluent Cloud, even in the most stringent regulatory environments. Confluent Cloud for Apache Flink supports private networking for AWS and Azure in all regions where Flink is supported. Google Cloud supports private networking in most regions where Flink is supported. For the regions that support Flink private networking, see Supported Cloud Regions. Connectivity options¶ There are a number of ways to access Flink with private networking. In all cases, they allow access to all types of private clusters (Enterprise, Dedicated, Freight), with all types of connectivity (VNET/VPC, Peering, Transit Gateway, PNI). PrivateLink Attachment: Works with any type of cluster and is available on AWS and Azure. Confluent Cloud network (CCN): Available on AWS only. If you already have an existing Confluent Cloud network, this is the easiest way to get started, but it works only on AWS when a Confluent Cloud network is already configured. If you need to create a new Confluent Cloud network, follow the steps in Create Confluent Cloud Network on AWS. PrivateLink Attachment¶ A PrivateLink Attachment is a resource that enables you to connect to Confluent serverless products, like Enterprise clusters and Flink. For Flink, the new PrivateLink Attachment is used only to establish a connection between your clients (like Cloud Console UI, Confluent CLI, Terraform, apps using the Confluent REST API) and Flink. Flink-to-Kafka is routed internally within Confluent Cloud. As a result, this PLATT is used only for submitting statements and fetching results from the client. For Dedicated clusters, regardless of the Kafka cluster connection type (Private Link, Peering, or Transit Gateway), Flink requires that you define a PLATT in the same region of the cluster, even if a private link exists for the Dedicated cluster. For Enterprise clusters, you can reuse the same PLATT used by your Enterprise clusters. By creating a PrivateLink Attachment to a Confluent Cloud environment in a region, you are enabling Flink statements created in that environment to securely access data in any of the Flink clusters in the same region, regardless of their environment. Access to the Flink clusters is governed by RBAC. Also, a PrivateLink Attachment enables your data-movement components in Confluent Cloud, including Flink statements and cluster links, to move data between all of the private networks in the organization, including the Confluent Cloud networks associated with any Dedicated Kafka clusters. For more information, see Enable private networking with PrivateLink Attachment. Confluent Cloud network (CCN)¶ If you have an existing Confluent Cloud network, this is the easiest way to get set up, but it works only on AWS when a Confluent Cloud network is configured already and at least one Kafka Dedicated cluster exists in the environment and region where you need to use Flink. For existing Kafka Dedicated users, this option requires no effort to configure, if everything is already configured for Kafka. If a reverse proxy is not set up, this requires setup for Flink or the use of a VM within the VPC to access Flink. To create a Confluent Cloud network, follow the steps in Create Confluent Cloud Network on AWS. For more information, see Enable private networking with Confluent Cloud Network. Protect resources with IP Filtering¶ With IP Filtering, you can enhance security for your Flink resources (statements and workspaces) based on trusted source IP addresses. IP Filtering is an authorization feature that allows you to create IP filters for your Confluent Cloud organization that permit inbound requests only from specified IP groups. All incoming API requests that originate from IP addresses not included in your IP filters are denied. For Flink resources, you can implement the following access controls: No public networks: Select the predefined No Public Networks group (ipg-none) to block all public network access, allowing access only from private network connections. This IP group cannot be combined with other IP groups in the same filter. Public: The default option if no IP filters are set. Flink statements and workspaces are accessible from all source networks when connecting over the public internet. While SQL queries are visible, private cluster data remains protected, and you can’t issue statements accessing private clusters. Public with restricted IP list: Create custom IP groups containing specific CIDR blocks to allow access only from trusted networks while maintaining the same protection for private cluster data. IP Filtering applies only to requests made over public networks and doesn’t limit requests made over private network connections. When creating IP filters for Flink resources, select the Flink operation group to control access to all operations related to Apache Flink data. For more information on setting IP filters, see IP Filtering and Manage IP Filters. The IP Filtering feature replaces the previous distinction between public and private Flink statements and workspaces. Administrators can modify access controls at any time by updating IP filters. For data protection in Kafka clusters, access is governed by network settings of the cluster: You can always read public data regardless of the connectivity, whether public or private. To read or write data in a private cluster, the cluster must use private connectivity. To prevent data exfiltration, you can’t write to public clusters when using private connectivity. Available endpoints for an environment and region¶ The following section shows the endpoints that are available for connecting to Flink. While the public endpoint is always present, others may require some effort to be created. Public endpoint PrivateLink Attachment Private connectivity through Confluent Cloud Network The following table shows how to get the endpoint value by using different Confluent interfaces. Interface Location Endpoint Cloud Console Flink Endpoints page Full FQDN shown for each network connection Confluent CLI confluent flink endpoint list Full FQDN shown for each network connection Network UI/API/CLI Network management details page in Environment overview GET /network/ confluent network describe Read the endpoint_suffix attribute, for example, <service-identifier>-abc1de.us-east-1.aws.glb.confluent.cloud Replace <service-identifier> with the relevant value, for example, flink for Flink or flinkpls for Language Service. Assign in interface (UI/CLI/Terraform) The following table shows the endpoint patterns for different DNS and cluster type combinations. Networking DNS Cluster Type Endpoints PrivateLink Private Enterprise (PrivateLink Attachment) flink.$region.$cloud.private.confluent.cloud flinkpls.$region.$cloud.private.confluent.cloud Dedicated flink.dom$id.$region.$cloud.private.confluent.cloud flinkpls.dom$id.$region.$cloud.private.confluent.cloud Public Dedicated flink-$nid.$region.$cloud.glb.confluent.cloud flinkpls-$nid.$region.$cloud.glb.confluent.cloud VPC Peering / Transit Gateway w/ /16 CIDR Public Dedicated flink-$nid.$region.$cloud.confluent.cloud flinkpls-$nid.$region.$cloud.confluent.cloud VPC Peering / Transit Gateway w/ /27 CIDRs Public Dedicated flink-$nid.$region.$cloud.glb.confluent.cloud flinkpls-$nid.$region.$cloud.glb.confluent.cloud Public endpoint¶ Source: Always present. Considerations: Can’t access Kafka private data. Kafka data access and scope: Can access public cluster data (read/write) in cloud region for this organization. Access to Flink statement and workspace: Configurable with IP Filtering. Endpoints: flink.<region>.<cloud>.confluent.cloud, for example: flink.us-east-2.aws.confluent.cloud. PrivateLink Attachment¶ Source: Must create a Private Link Attachment for the environment/region. Considerations: A single VPC can’t have private link connections to multiple Confluent Cloud environments. Available on AWS and Azure. Can access private cluster data (read/write) in Enterprise, Dedicated or Freight clusters for the cloud region for the organization of the endpoint. Can access public cluster data (read only). Access all Flink resources in the same environment and region of the endpoint Endpoints: flink.<region>.<cloud>.private.confluent.cloud, for example: flink.us-east-2.aws.private.confluent.cloud Private connectivity through Confluent Cloud Network¶ Source: Created with Kafka Dedicated clusters. Considerations: Easiest way to use Flink when the network is created already for Dedicated clusters. Available on AWS only. Can access private cluster data (read/write) in Enterprise, Dedicated or Freight clusters for the organization of the region. Can access public cluster data (read only). Access all Flink resources in the same environment and region of the endpoint To find the endpoints from the Cloud Console or Confluent CLI, see Available endpoints for an environment and region. Access private networking with the Confluent CLI¶ Run the confluent flink region --cloud <cloud-provider> --region <region> command to select a cloud provider and region. Run the confluent flink endpoint list command to list all endpoints, both public and private. Run the confluent flink endpoint use to select an endpoint. In addition to the main Flink endpoint listed here, you must have access to flinkpls.<network>.<region>.<cloud>.private.confluent.cloud (for private DNS resolution) or flinkpls-<network>.<region>.<cloud>.private.confluent.cloud (for public DNS resolution) to access the language service for autocompletion in the Flink SQL shell. In the case of public DNS resolution, routing is done transparently, but if you use private DNS resolution, you must make sure to route this endpoint from your client. For more information, see private DNS resolution. Access private networking with the Cloud Console¶ By default, public networking is used, which won’t work if IP Filtering is set, and/or the cluster is private. You can set defaults for each cloud region in an environment. For this, use the Flink Endpoints page. The default is per-user. When a default is set, it is used for all pages that access Flink, for example, the statement list, workspace list, and workspaces. If no default is set, the public endpoint is used. Related content¶ Video: Flink Queries on Dedicated PrivateLink Kafka Clusters in Confluent Cloud Use Confluent Cloud with Private Networking Flink Compute Pools Note This website includes content developed at the Apache Software Foundation under the terms of the Apache License v2.

#### Code Examples

```sql
confluent flink endpoint list
```

```sql
confluent network describe
```

```sql
endpoint_suffix
```

```sql
<service-identifier>-abc1de.us-east-1.aws.glb.confluent.cloud
```

```sql
<service-identifier>
```

```sql
flink.$region.$cloud.private.confluent.cloud
```

```sql
flinkpls.$region.$cloud.private.confluent.cloud
```

```sql
flink.dom$id.$region.$cloud.private.confluent.cloud
```

```sql
flinkpls.dom$id.$region.$cloud.private.confluent.cloud
```

```sql
flink-$nid.$region.$cloud.glb.confluent.cloud
```

```sql
flinkpls-$nid.$region.$cloud.glb.confluent.cloud
```

```sql
flink-$nid.$region.$cloud.confluent.cloud
```

```sql
flinkpls-$nid.$region.$cloud.confluent.cloud
```

```sql
flink-$nid.$region.$cloud.glb.confluent.cloud
```

```sql
flinkpls-$nid.$region.$cloud.glb.confluent.cloud
```

```sql
flink.<region>.<cloud>.confluent.cloud
```

```sql
flink.us-east-2.aws.confluent.cloud
```

```sql
flink.<region>.<cloud>.private.confluent.cloud
```

```sql
flink.us-east-2.aws.private.confluent.cloud
```

```sql
confluent flink region --cloud <cloud-provider> --region <region>
```

```sql
confluent flink endpoint list
```

```sql
confluent flink endpoint use
```

```sql
flinkpls.<network>.<region>.<cloud>.private.confluent.cloud
```

```sql
flinkpls-<network>.<region>.<cloud>.private.confluent.cloud
```

---

### Flink SQL Syntax in Confluent Cloud for Apache Flink | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/reference/sql-syntax.html#flink-sql-syntax

Flink SQL Syntax in Confluent Cloud for Apache Flink¶ SQL is a domain-specific language for managing and manipulating data. It’s used primarily to work with structured data, where the types and relationships across entities are well-defined. Originally adopted for relational databases, SQL is rapidly becoming the language of choice for stream processing. It’s declarative, expressive, and ubiquitous. The American National Standards Institute (ANSI) maintains a standard for the specification of SQL. Flink SQL is compliant with ANSI SQL 2011. Beyond the standard, there are many flavors and extensions to SQL so that it can express programs beyond what’s possible with the SQL 2011 grammar. Lexical structure¶ The grammar of Apache Flink® parses SQL using Apache Calcite, which supports standard ANSI SQL. Syntax¶ Flink SQL inputs are made up of a series of statements. Each statement is made up of a series of tokens and ends in a semicolon (;). The tokens that apply depend on the statement being invoked. A token is any keyword, identifier, backticked identifier, literal, or special character. By convention, tokens are separated by whitespace, unless there is no ambiguity in the grammar. This happens when tokens flank a special character. The following example statements are syntactically valid Flink SQL input: -- Create a users table. CREATE TABLE users ( user_id STRING, registertime BIGINT, gender STRING, regionid STRING ); -- Populate the table with mock users data. INSERT INTO users VALUES ('Thomas A. Anderson', 1677260724, 'male', 'Region_4'), ('Trinity', 1677260733, 'female', 'Region_4'), ('Morpheus', 1677260742, 'male', 'Region_8'); SELECT * FROM users; Keywords¶ Some tokens, such as SELECT, INSERT, and CREATE, are keywords. Keywords are reserved tokens that have a specific meaning in Flink’s syntax. They control their surrounding allowable tokens and execution semantics. Keywords are case insensitive, meaning SELECT and select are equivalent. You can’t create an identifier that is already a reserved word, unless you use backticked identifiers, for example, `table`. For a complete list of keywords, see Flink SQL Reserved Keywords. Identifiers¶ Identifiers are symbols that represent user-defined entities, like tables, columns, and other objects. For example, if you have a table named t1, t1 is an identifier for that table. By default, identifiers are case-sensitive, meaning t1 and T1 refer to different tables. Unless an identifier is backticked, it may be composed only of characters that are a letter, number, or underscore. There is no imposed limit on the number of characters. To make it possible to use any character in an identifier, you can enclose it in backtick characters (`) when you declare and use it. A backticked identifier is useful when you don’t control the data, so it might have special characters, or even keywords. If you want to use one of the keyword strings as an identifier, enclose them with backticks, for example: `value` `count` When you use backticked identifiers, Flink SQL captures the case exactly, and any future references to the identifier are case-sensitive. For example, if you declare the following table: CREATE TABLE `t1` ( id VARCHAR, `@MY-identifier-table-column!` INT); You must select from it by backticking the table name and column name and using the original casing: SELECT `@MY-identifier-table-column!` FROM `t1`; If you use an invalid identifier without enclosing it in backticks, you receive a SQL parse failed error. For example, the following SQL query tries to read records from a table named table-with-dashes, but the dash character (-) is not valid in an identifier. SELECT * FROM table-with-dashes; The error output resembles: SQL parse failed. Encountered "-" at line 1, column 20. You can fix the error by enclosing the identifier with backticks: SELECT * FROM `table-with-dashes`; Constants¶ There are three implicitly typed constants, or literals, in Flink SQL: strings, numbers, and booleans. String constants¶ A string constant is an arbitrary series of characters surrounded by single quotes ('), like 'Hello world'. To include a quote inside of a string literal, escape the quote by prefixing it with another quote, for example, 'You can call me ''Stuart'', or Stu.' Numeric constants¶ Numeric constants are accepted in the following forms: digits digits.[digits][e[+-]digits] [digits].digits[e[+-]digits] digitse[+-]digits where digits is one or more single-digit integers (0 through 9). At least one digit must be present before or after the decimal point, if there is one. At least one digit must follow the exponent symbol e, if there is one. No spaces, underscores, or any other characters are allowed in the constant. Numeric constants may also have a + or - prefix, but this is considered to be a function applied to the constant, not the constant itself. Here are some examples of valid numeric constants: 5 7.2 0.0087 1. .5 1e-3 1.332434e+2 +100 -250 Boolean constants¶ A boolean constant is represented as either the identifier true or false. Boolean constants are not case-sensitive, which means that true evaluates to the same value as TRUE. Operators¶ Operators are infix functions composed of special characters. Flink SQL doesn’t allow you to add user-space operators. For a complete list of operators, see Comparison Functions in Confluent Cloud for Apache Flink. Special characters¶ Some characters have a particular meaning that doesn’t correspond to an operator. The following list describes the special characters and their purposes. Parentheses (()) retain their usual meaning in programming languages for grouping expressions and controlling the order of evaluation. Brackets ([]) are used to work with arrays, both in their construction and subscript access. They also allow you to key into maps. Commas (,) delineate a discrete list of entities. The semi-colon (;) terminates a SQL statement. The asterisk (*), when used in particular syntax, is used as an “all” qualifier. This is seen most commonly in a SELECT command to retrieve all columns. The period (.) accesses a column in a table or a field in a struct data type. Comments¶ A comment is a string beginning with two dashes. It includes all of the content from the dashes to the end of the line: -- Here is a comment. You can also span a comment over multiple lines by using C-style syntax: /* Here is another comment. */ Lexical precedence¶ Operators are evaluated using the following order of precedence: *, /, % +, - =, >, <, >=, <=, <>, != NOT AND BETWEEN, LIKE, OR In an expression, when two operators have the same precedence level, they’re evaluated left-to-right, based on their position. You can enclose an expression in parentheses to force precedence or clarify precedence, for example, (5 + 2) * 3. Related content¶ Flink SQL Reserved Keywords Data Types Flink SQL Queries DDL Statements in Confluent Cloud for Apache Flink Note This website includes content developed at the Apache Software Foundation under the terms of the Apache License v2.

#### Code Examples

```sql
-- Create a users table.
CREATE TABLE users (
  user_id STRING,
  registertime BIGINT,
  gender STRING,
  regionid STRING
);

-- Populate the table with mock users data.
INSERT INTO users VALUES
  ('Thomas A. Anderson', 1677260724, 'male', 'Region_4'),
  ('Trinity', 1677260733, 'female', 'Region_4'),
  ('Morpheus', 1677260742, 'male', 'Region_8');

SELECT * FROM users;
```

```sql
CREATE TABLE `t1` (
  id VARCHAR,
  `@MY-identifier-table-column!` INT);
```

```sql
SELECT `@MY-identifier-table-column!` FROM `t1`;
```

```sql
SQL parse failed
```

```sql
table-with-dashes
```

```sql
SELECT * FROM table-with-dashes;
```

```sql
SQL parse failed. Encountered "-" at line 1, column 20.
```

```sql
SELECT * FROM `table-with-dashes`;
```

```sql
'Hello world'
```

```sql
'You can call me ''Stuart'', or Stu.'
```

```sql
1.332434e+2
```

```sql
-- Here is a comment.
```

```sql
/* Here is
   another comment.
*/
```

```sql
(5 + 2) * 3
```

---

### Java Table API Quick Start on Confluent Cloud for Apache Flink | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/get-started/quick-start-java-table-api.html#flink-java-table-api-quick-start

Java Table API Quick Start on Confluent Cloud for Apache Flink¶ Confluent Cloud for Apache Flink® supports programming applications with the Table API. Confluent provides a plugin for running applications that use the Table API on Confluent Cloud. For more information, see Table API. For code examples, see Java Examples for Table API on Confluent Cloud. For a Confluent Developer course, see Apache Flink Table API: Processing Data Streams in Java. Note The Flink Table API is available for preview. A Preview feature is a Confluent Cloud component that is being introduced to gain early feedback from developers. Preview features can be used for evaluation and non-production testing purposes or to provide feedback to Confluent. The warranty, SLA, and Support Services provisions of your agreement with Confluent do not apply to Preview features. Confluent may discontinue providing preview releases of the Preview features at any time in Confluent’s’ sole discretion. Comments, questions, and suggestions related to the Table API are encouraged and can be submitted through the established channels. Prerequisites¶ Access to Confluent Cloud A compute pool in Confluent Cloud A Apache Kafka® cluster, if you want to run examples that store data in Kafka Java version 11 or later Maven (see Installing Apache Maven) To run Table API and Flink SQL programs, you must generate an API key that’s specific to the Flink environment. Also, you need Confluent Cloud account details, like your organization and environment identifiers. Flink API Key: Follow the steps in Generate a Flink API key. For convenience, assign your Flink key and secret to the FLINK_API_KEY and FLINK_API_SECRET environment variables. Organization ID: The identifier your organization, for example, b0b421724-4586-4a07-b787-d0bb5aacbf87. For convenience, assign your organization identifier to the ORG_ID environment variable. Environment ID: The identifier of the environment where your Flink SQL statements run, for example, env-z3y2x1. For convenience, assign your environment identifier to the ENV_ID environment variable. Cloud provider name: The name of the cloud provider where your cluster runs, for example, aws. To see the available providers, run the confluent flink region list command. For convenience, assign your cloud provider to the CLOUD_PROVIDER environment variable. Cloud region: The name of the region where your cluster runs, for example, us-east-1. To see the available regions, run the confluent flink region list command. For convenience, assign your cloud region to the CLOUD_REGION environment variable. export CLOUD_PROVIDER="aws" export CLOUD_REGION="us-east-1" export FLINK_API_KEY="<your-flink-api-key>" export FLINK_API_SECRET="<your-flink-api-secret>" export ORG_ID="<your-organization-id>" export ENV_ID="<your-environment-id>" export COMPUTE_POOL_ID="<your-compute-pool-id>" Compile and run a Table API program¶ The following code example shows how to run a “Hello World” statement and how to query an example data stream. Copy the following project object model (POM) into a file named pom.xml. pom.xml <project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"> <modelVersion>4.0.0</modelVersion> <groupId>example</groupId> <artifactId>flink-table-api-java-hello-world</artifactId> <version>1.0</version> <packaging>jar</packaging> <name>Apache Flink® Table API Java Hello World Example on Confluent Cloud</name> <properties> <flink.version>1.20.0</flink.version> <confluent-plugin.version>1.20-42</confluent-plugin.version> <target.java.version>11</target.java.version> <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding> <maven.compiler.source>${target.java.version}</maven.compiler.source> <maven.compiler.target>${target.java.version}</maven.compiler.target> <log4j.version>2.17.1</log4j.version> </properties> <repositories> <repository> <id>confluent</id> <url>https://packages.confluent.io/maven/</url> </repository> <repository> <id>apache.snapshots</id> <name>Apache Development Snapshot Repository</name> <url>https://repository.apache.org/content/repositories/snapshots/</url> <releases> <enabled>false</enabled> </releases> <snapshots> <enabled>true</enabled> </snapshots> </repository> </repositories> <dependencies> <!-- Apache Flink dependencies --> <dependency> <groupId>org.apache.flink</groupId> <artifactId>flink-table-api-java</artifactId> <version>${flink.version}</version> </dependency> <!-- Confluent Flink Table API Java plugin --> <dependency> <groupId>io.confluent.flink</groupId> <artifactId>confluent-flink-table-api-java-plugin</artifactId> <version>${confluent-plugin.version}</version> </dependency> <!-- Add logging framework, to produce console output when running in the IDE. --> <!-- These dependencies are excluded from the application JAR by default. --> <dependency> <groupId>org.apache.logging.log4j</groupId> <artifactId>log4j-slf4j-impl</artifactId> <version>${log4j.version}</version> <scope>runtime</scope> </dependency> <dependency> <groupId>org.apache.logging.log4j</groupId> <artifactId>log4j-api</artifactId> <version>${log4j.version}</version> <scope>runtime</scope> </dependency> <dependency> <groupId>org.apache.logging.log4j</groupId> <artifactId>log4j-core</artifactId> <version>${log4j.version}</version> <scope>runtime</scope> </dependency> </dependencies> <build> <sourceDirectory>./example</sourceDirectory> <plugins> <!-- Java Compiler --> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <version>3.10.1</version> <configuration> <source>${target.java.version}</source> <target>${target.java.version}</target> </configuration> </plugin> <!-- We use the maven-shade plugin to create a fat jar that contains all necessary dependencies. --> <!-- Change the value of <mainClass>...</mainClass> if your program entry point changes. --> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-shade-plugin</artifactId> <version>3.4.1</version> <executions> <!-- Run shade goal on package phase --> <execution> <phase>package</phase> <goals> <goal>shade</goal> </goals> <configuration> <artifactSet> <excludes> <exclude>org.apache.flink:flink-shaded-force-shading</exclude> <exclude>com.google.code.findbugs:jsr305</exclude> </excludes> </artifactSet> <filters> <filter> <!-- Do not copy the signatures in the META-INF folder. Otherwise, this might cause SecurityExceptions when using the JAR. --> <artifact>*:*</artifact> <excludes> <exclude>META-INF/*.SF</exclude> <exclude>META-INF/*.DSA</exclude> <exclude>META-INF/*.RSA</exclude> </excludes> </filter> </filters> <transformers> <transformer implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/> <transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"> <mainClass>example.hello_table_api</mainClass> </transformer> </transformers> </configuration> </execution> </executions> </plugin> </plugins> <pluginManagement> <plugins> <!-- This improves the out-of-the-box experience in Eclipse by resolving some warnings. --> <plugin> <groupId>org.eclipse.m2e</groupId> <artifactId>lifecycle-mapping</artifactId> <version>1.0.0</version> <configuration> <lifecycleMappingMetadata> <pluginExecutions> <pluginExecution> <pluginExecutionFilter> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-shade-plugin</artifactId> <versionRange>[3.1.1,)</versionRange> <goals> <goal>shade</goal> </goals> </pluginExecutionFilter> <action> <ignore/> </action> </pluginExecution> <pluginExecution> <pluginExecutionFilter> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <versionRange>[3.1,)</versionRange> <goals> <goal>testCompile</goal> <goal>compile</goal> </goals> </pluginExecutionFilter> <action> <ignore/> </action> </pluginExecution> </pluginExecutions> </lifecycleMappingMetadata> </configuration> </plugin> </plugins> </pluginManagement> </build> </project> Create a directory named “example”. mkdir example Create a file named hello_table_api.java in the example directory. touch example/hello_table_api.java Copy the following code into hello_table_api.java. package example; import io.confluent.flink.plugin.ConfluentSettings; import io.confluent.flink.plugin.ConfluentTools; import org.apache.flink.table.api.EnvironmentSettings; import org.apache.flink.table.api.Table; import org.apache.flink.table.api.TableEnvironment; import org.apache.flink.types.Row; import java.util.List; /** * A table program example to get started with the Apache Flink® Table API. * * <p>It executes two foreground statements in Confluent Cloud. The results of both statements are * printed to the console. */ public class hello_table_api { // All logic is defined in a main() method. It can run both in an IDE or CI/CD system. public static void main(String[] args) { // Set up connection properties to Confluent Cloud. // Use the fromGlobalVariables() method if you assigned environment variables. // EnvironmentSettings settings = ConfluentSettings.fromGlobalVariables(); // Use the fromArgs(args) method if you want to run with command-line arguments. EnvironmentSettings settings = ConfluentSettings.fromArgs(args); // Initialize the session context to get started. TableEnvironment env = TableEnvironment.create(settings); System.out.println("Running with printing..."); // The Table API centers on 'Table' objects, which help in defining data pipelines // fluently. You can define pipelines fully programmatically. Table table = env.fromValues("Hello world!"); // Also, You can define pipelines with embedded Flink SQL. // Table table = env.sqlQuery("SELECT 'Hello world!'"); // Once the pipeline is defined, execute it on Confluent Cloud. // If no target table has been defined, results are streamed back and can be printed // locally. This can be useful for development and debugging. table.execute().print(); System.out.println("Running with collecting..."); // Results can be collected locally and accessed individually. // This can be useful for testing. Table moreHellos = env.fromValues("Hello Bob", "Hello Alice", "Hello Peter").as("greeting"); List<Row> rows = ConfluentTools.collectChangelog(moreHellos, 10); rows.forEach( r -> { String column = r.getFieldAs("greeting"); System.out.println("Greeting: " + column); }); } } Run the following command to build the jar file. mvn clean package Run the jar. If you assigned your cloud configuration to the environment variables specified in the Prerequisites section, and you used the fromGlobalVariables method in the hello_table_api code, you don’t need to provide the command-line options. java -jar target/flink-table-api-java-hello-world-1.0.jar \ --cloud aws \ --region us-east-1 \ --flink-api-key key \ --flink-api-secret secret \ --organization-id b0b21724-4586-4a07-b787-d0bb5aacbf87 \ --environment-id env-z3y2x1 \ --compute-pool-id lfcp-8m03rm Your output should resemble: Running with printing... +----+--------------------------------+ | op | f0 | +----+--------------------------------+ | +I | Hello world! | +----+--------------------------------+ 1 row in set Running with collecting... Greeting: Hello Bob Greeting: Hello Alice Greeting: Hello Peter Next steps¶ Python Table API Quick Start How-to Guides for Confluent Cloud for Apache Flink Related content¶ Course: Apache Flink® Table API: Processing Data Streams in Java GitHub repo: Java Examples for Table API on Confluent Cloud GitHub repo: Python Examples for Table API on Confluent Cloud Built-in Functions Note This website includes content developed at the Apache Software Foundation under the terms of the Apache License v2.

#### Code Examples

```sql
b0b421724-4586-4a07-b787-d0bb5aacbf87
```

```sql
confluent flink region list
```

```sql
confluent flink region list
```

```sql
export CLOUD_PROVIDER="aws"
export CLOUD_REGION="us-east-1"
export FLINK_API_KEY="<your-flink-api-key>"
export FLINK_API_SECRET="<your-flink-api-secret>"
export ORG_ID="<your-organization-id>"
export ENV_ID="<your-environment-id>"
export COMPUTE_POOL_ID="<your-compute-pool-id>"
```

```sql
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
        xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>example</groupId>
    <artifactId>flink-table-api-java-hello-world</artifactId>
    <version>1.0</version>
    <packaging>jar</packaging>

    <name>Apache Flink® Table API Java Hello World Example on Confluent Cloud</name>

    <properties>
        <flink.version>1.20.0</flink.version>
        <confluent-plugin.version>1.20-42</confluent-plugin.version>
        <target.java.version>11</target.java.version>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
        <maven.compiler.source>${target.java.version}</maven.compiler.source>
        <maven.compiler.target>${target.java.version}</maven.compiler.target>
        <log4j.version>2.17.1</log4j.version>
    </properties>

    <repositories>
        <repository>
            <id>confluent</id>
            <url>https://packages.confluent.io/maven/</url>
        </repository>
        <repository>
            <id>apache.snapshots</id>
            <name>Apache Development Snapshot Repository</name>
            <url>https://repository.apache.org/content/repositories/snapshots/</url>
            <releases>
                <enabled>false</enabled>
            </releases>
            <snapshots>
                <enabled>true</enabled>
            </snapshots>
        </repository>
    </repositories>

    <dependencies>
        <!-- Apache Flink dependencies -->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-table-api-java</artifactId>
            <version>${flink.version}</version>
        </dependency>

        <!-- Confluent Flink Table API Java plugin -->
        <dependency>
            <groupId>io.confluent.flink</groupId>
            <artifactId>confluent-flink-table-api-java-plugin</artifactId>
            <version>${confluent-plugin.version}</version>
        </dependency>

        <!-- Add logging framework, to produce console output when running in the IDE. -->
        <!-- These dependencies are excluded from the application JAR by default. -->
        <dependency>
            <groupId>org.apache.logging.log4j</groupId>
            <artifactId>log4j-slf4j-impl</artifactId>
            <version>${log4j.version}</version>
            <scope>runtime</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.logging.log4j</groupId>
            <artifactId>log4j-api</artifactId>
            <version>${log4j.version}</version>
            <scope>runtime</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.logging.log4j</groupId>
            <artifactId>log4j-core</artifactId>
            <version>${log4j.version}</version>
            <scope>runtime</scope>
        </dependency>
    </dependencies>

    <build>
    <sourceDirectory>./example</sourceDirectory>
        <plugins>

            <!-- Java Compiler -->
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>3.10.1</version>
                <configuration>
                    <source>${target.java.version}</source>
                    <target>${target.java.version}</target>
                </configuration>
            </plugin>

            <!-- We use the maven-shade plugin to create a fat jar that contains all necessary dependencies. -->
            <!-- Change the value of <mainClass>...</mainClass> if your program entry point changes. -->
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-shade-plugin</artifactId>
                <version>3.4.1</version>
                <executions>
                    <!-- Run shade goal on package phase -->
                    <execution>
                        <phase>package</phase>
                        <goals>
                            <goal>shade</goal>
                        </goals>
                        <configuration>
                            <artifactSet>
                                <excludes>
                                    <exclude>org.apache.flink:flink-shaded-force-shading</exclude>
                                    <exclude>com.google.code.findbugs:jsr305</exclude>
                                </excludes>
                            </artifactSet>
                            <filters>
                                <filter>
                                    <!-- Do not copy the signatures in the META-INF folder.
                                    Otherwise, this might cause SecurityExceptions when using the JAR. -->
                                    <artifact>*:*</artifact>
                                    <excludes>
                                        <exclude>META-INF/*.SF</exclude>
                                        <exclude>META-INF/*.DSA</exclude>
                                        <exclude>META-INF/*.RSA</exclude>
                                    </excludes>
                                </filter>
                            </filters>
                            <transformers>
                                <transformer
                                        implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/>
                                <transformer
                                        implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                                    <mainClass>example.hello_table_api</mainClass>
                                </transformer>
                            </transformers>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
        </plugins>

        <pluginManagement>
            <plugins>

                <!-- This improves the out-of-the-box experience in Eclipse by resolving some warnings. -->
                <plugin>
                    <groupId>org.eclipse.m2e</groupId>
                    <artifactId>lifecycle-mapping</artifactId>
                    <version>1.0.0</version>
                    <configuration>
                        <lifecycleMappingMetadata>
                            <pluginExecutions>
                                <pluginExecution>
                                    <pluginExecutionFilter>
                                        <groupId>org.apache.maven.plugins</groupId>
                                        <artifactId>maven-shade-plugin</artifactId>
                                        <versionRange>[3.1.1,)</versionRange>
                                        <goals>
                                            <goal>shade</goal>
                                        </goals>
                                    </pluginExecutionFilter>
                                    <action>
                                        <ignore/>
                                    </action>
                                </pluginExecution>
                                <pluginExecution>
                                    <pluginExecutionFilter>
                                        <groupId>org.apache.maven.plugins</groupId>
                                        <artifactId>maven-compiler-plugin</artifactId>
                                        <versionRange>[3.1,)</versionRange>
                                        <goals>
                                            <goal>testCompile</goal>
                                            <goal>compile</goal>
                                        </goals>
                                    </pluginExecutionFilter>
                                    <action>
                                        <ignore/>
                                    </action>
                                </pluginExecution>
                            </pluginExecutions>
                        </lifecycleMappingMetadata>
                    </configuration>
                </plugin>
            </plugins>
        </pluginManagement>
    </build>
</project>
```

```sql
mkdir example
```

```sql
hello_table_api.java
```

```sql
touch example/hello_table_api.java
```

```sql
hello_table_api.java
```

```sql
package example;
import io.confluent.flink.plugin.ConfluentSettings;
import io.confluent.flink.plugin.ConfluentTools;
import org.apache.flink.table.api.EnvironmentSettings;
import org.apache.flink.table.api.Table;
import org.apache.flink.table.api.TableEnvironment;
import org.apache.flink.types.Row;
import java.util.List;

/**
 * A table program example to get started with the Apache Flink® Table API.
 *
 * <p>It executes two foreground statements in Confluent Cloud. The results of both statements are
 * printed to the console.
 */
public class hello_table_api {

    // All logic is defined in a main() method. It can run both in an IDE or CI/CD system.
    public static void main(String[] args) {

        // Set up connection properties to Confluent Cloud.
        // Use the fromGlobalVariables() method if you assigned environment variables.
        // EnvironmentSettings settings = ConfluentSettings.fromGlobalVariables();

        // Use the fromArgs(args) method if you want to run with command-line arguments.
        EnvironmentSettings settings = ConfluentSettings.fromArgs(args);

        // Initialize the session context to get started.
        TableEnvironment env = TableEnvironment.create(settings);

        System.out.println("Running with printing...");

        // The Table API centers on 'Table' objects, which help in defining data pipelines
        // fluently. You can define pipelines fully programmatically.
        Table table = env.fromValues("Hello world!");

        // Also, You can define pipelines with embedded Flink SQL.
        // Table table = env.sqlQuery("SELECT 'Hello world!'");

        // Once the pipeline is defined, execute it on Confluent Cloud.
        // If no target table has been defined, results are streamed back and can be printed
        // locally. This can be useful for development and debugging.
        table.execute().print();

        System.out.println("Running with collecting...");

        // Results can be collected locally and accessed individually.
        // This can be useful for testing.
        Table moreHellos = env.fromValues("Hello Bob", "Hello Alice", "Hello Peter").as("greeting");
        List<Row> rows = ConfluentTools.collectChangelog(moreHellos, 10);
        rows.forEach(
                r -> {
                    String column = r.getFieldAs("greeting");
                    System.out.println("Greeting: " + column);
                });
    }
}
```

```sql
mvn clean package
```

```sql
fromGlobalVariables
```

```sql
hello_table_api
```

```sql
java -jar target/flink-table-api-java-hello-world-1.0.jar \
  --cloud aws \
  --region us-east-1 \
  --flink-api-key key \
  --flink-api-secret secret \
  --organization-id b0b21724-4586-4a07-b787-d0bb5aacbf87 \
  --environment-id env-z3y2x1 \
  --compute-pool-id lfcp-8m03rm
```

```sql
Running with printing...
+----+--------------------------------+
| op |                             f0 |
+----+--------------------------------+
| +I |                   Hello world! |
+----+--------------------------------+
1 row in set
Running with collecting...
Greeting: Hello Bob
Greeting: Hello Alice
Greeting: Hello Peter
```

---

### Python Table API Quick Start on Confluent Cloud for Apache Flink | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/get-started/quick-start-python-table-api.html#flink-python-table-api-quick-start

Python Table API Quick Start on Confluent Cloud for Apache Flink¶ Confluent Cloud for Apache Flink® supports programming applications with the Table API. Confluent provides a plugin for running applications that use the Table API on Confluent Cloud. For more information, see Table API. For code examples, see Python Examples for Table API on Confluent Cloud. Note The Flink Table API is available for preview. A Preview feature is a Confluent Cloud component that is being introduced to gain early feedback from developers. Preview features can be used for evaluation and non-production testing purposes or to provide feedback to Confluent. The warranty, SLA, and Support Services provisions of your agreement with Confluent do not apply to Preview features. Confluent may discontinue providing preview releases of the Preview features at any time in Confluent’s’ sole discretion. Comments, questions, and suggestions related to the Table API are encouraged and can be submitted through the established channels. Prerequisites¶ Access to Confluent Cloud A compute pool in Confluent Cloud A Apache Kafka® cluster, if you want to run examples that store data in Kafka Java version 11 or later To run Table API and Flink SQL programs, you must generate an API key that’s specific to the Flink environment. Also, you need Confluent Cloud account details, like your organization and environment identifiers. Flink API Key: Follow the steps in Generate a Flink API key. For convenience, assign your Flink key and secret to the FLINK_API_KEY and FLINK_API_SECRET environment variables. Organization ID: The identifier your organization, for example, b0b421724-4586-4a07-b787-d0bb5aacbf87. For convenience, assign your organization identifier to the ORG_ID environment variable. Environment ID: The identifier of the environment where your Flink SQL statements run, for example, env-z3y2x1. For convenience, assign your environment identifier to the ENV_ID environment variable. Cloud provider name: The name of the cloud provider where your cluster runs, for example, aws. To see the available providers, run the confluent flink region list command. For convenience, assign your cloud provider to the CLOUD_PROVIDER environment variable. Cloud region: The name of the region where your cluster runs, for example, us-east-1. To see the available regions, run the confluent flink region list command. For convenience, assign your cloud region to the CLOUD_REGION environment variable. export CLOUD_PROVIDER="aws" export CLOUD_REGION="us-east-1" export FLINK_API_KEY="<your-flink-api-key>" export FLINK_API_SECRET="<your-flink-api-secret>" export ORG_ID="<your-organization-id>" export ENV_ID="<your-environment-id>" export COMPUTE_POOL_ID="<your-compute-pool-id>" Compile and run a Table API program¶ Use poetry to create a virtual environment that contains all required dependencies and project files. Follow the instructions here to install pipx. Run the following command to install poetry. pipx install poetry Copy the following code into a file named hello_table_api.py. from pyflink.table.confluent import ConfluentSettings, ConfluentTools from pyflink.table import TableEnvironment, Row from pyflink.table.expressions import col, row def run(): # Set up the connection to Confluent Cloud settings = ConfluentSettings.from_file("/cloud.properties") env = TableEnvironment.create(settings) # Run your first Flink statement in Table API env.from_elements([row("Hello world!")]).execute().print() # Or use SQL env.sql_query("SELECT 'Hello world!'").execute().print() # Structure your code with Table objects - the main ingredient of Table API. table = env.from_path("examples.marketplace.clicks") \ .filter(col("user_agent").like("Mozilla%")) \ .select(col("click_id"), col("user_id")) table.print_schema() print(table.explain()) # Use the provided tools to test on a subset of the streaming data expected = ConfluentTools.collect_materialized_limit(table, 50) actual = [Row(42, 500)] if expected != actual: print("Results don't match!") if __name__ == "__main__": run() In the directory where you created hello_table_api.py, run the following command to build a virtual environment containing all required dependencies and project files. poetry install Run the following command to execute the Table API program. poetry run hello_table_api.py Next steps¶ Java Table API Quick Start How-to Guides for Confluent Cloud for Apache Flink Related content¶ GitHub repo: Java Examples for Table API on Confluent Cloud GitHub repo: Python Examples for Table API on Confluent Cloud Built-in Functions Note This website includes content developed at the Apache Software Foundation under the terms of the Apache License v2.

#### Code Examples

```sql
b0b421724-4586-4a07-b787-d0bb5aacbf87
```

```sql
confluent flink region list
```

```sql
confluent flink region list
```

```sql
export CLOUD_PROVIDER="aws"
export CLOUD_REGION="us-east-1"
export FLINK_API_KEY="<your-flink-api-key>"
export FLINK_API_SECRET="<your-flink-api-secret>"
export ORG_ID="<your-organization-id>"
export ENV_ID="<your-environment-id>"
export COMPUTE_POOL_ID="<your-compute-pool-id>"
```

```sql
pipx install poetry
```

```sql
hello_table_api.py
```

```sql
from pyflink.table.confluent import ConfluentSettings, ConfluentTools
from pyflink.table import TableEnvironment, Row
from pyflink.table.expressions import col, row

def run():
    # Set up the connection to Confluent Cloud
    settings = ConfluentSettings.from_file("/cloud.properties")
    env = TableEnvironment.create(settings)

    # Run your first Flink statement in Table API
    env.from_elements([row("Hello world!")]).execute().print()

    # Or use SQL
    env.sql_query("SELECT 'Hello world!'").execute().print()

    # Structure your code with Table objects - the main ingredient of Table API.
    table = env.from_path("examples.marketplace.clicks") \
        .filter(col("user_agent").like("Mozilla%")) \
        .select(col("click_id"), col("user_id"))

    table.print_schema()
    print(table.explain())

    # Use the provided tools to test on a subset of the streaming data
    expected = ConfluentTools.collect_materialized_limit(table, 50)
    actual = [Row(42, 500)]
    if expected != actual:
        print("Results don't match!")

if __name__ == "__main__":
    run()
```

```sql
hello_table_api.py
```

```sql
poetry install
```

```sql
poetry run hello_table_api.py
```

---

### Get Help with Confluent Cloud for Apache Flink | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/get-help.html#ccloud-flink-help

Get Help with Confluent Cloud for Apache Flink¶ You can request support in the Confluent Support Portal. You can access the portal directly, or you can navigate to it from the Confluent Cloud Console by selecting the Support menu identified by the help icon () in the upper-right and choosing Support portal. For more information, see Confluent Support for Confluent Cloud. Confluent Community Slack¶ There is a dedicated #flink channel in the Confluent Community Slack. Join with this link to ask questions, provide feedback, and engage with other users. Troubleshoot Flink in Confluent Cloud Console¶ If issues occur while running Flink in Cloud Console, consider generating a HAR file and uploading it to the Confluent Community Slack channel or sending it to the Support Portal. For more information, see Generate a HAR file for Troubleshooting on Confluent Cloud. Note This website includes content developed at the Apache Software Foundation under the terms of the Apache License v2.

---

### Get Started with Confluent Cloud for Apache Flink | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/get-started/overview.html#flink-sql-get-started

Get Started with Confluent Cloud for Apache Flink¶ Welcome to Confluent Cloud for Apache Flink®. This section guides you through the steps to get your queries running using the Confluent Cloud Console (browser-based) and the Flink SQL shell (CLI-based). Get Started for Free Sign up for a Confluent Cloud trial and get $400 of free credit. If you’re currently using Confluent Cloud in a region that doesn’t yet support Flink, so you can’t use your data in existing Apache Kafka® topics, you can still try out Flink SQL by using sample data generators or the Example catalog, which are used in the quick starts and How-to Guides for Confluent Cloud for Apache Flink. Choose one of the following quick starts to get started with Flink SQL on Confluent Cloud: Flink SQL Quick Start with Confluent Cloud Console Flink SQL Shell Quick Start Also, you can access Flink by using the REST API and the Confluent Terraform Provider. REST API-based data streams Sample Project for Confluent Terraform Provider If you get stuck, have a question, or want to provide feedback or feature requests, don’t hesitate to reach out. Check out Get Help with Confluent Cloud for Apache Flink for our support channels. Next steps¶ Flink SQL Quick Start with Confluent Cloud Console Flink SQL Shell Quick Start Related content¶ Stream Processing Concepts Flink SQL Queries Note This website includes content developed at the Apache Software Foundation under the terms of the Apache License v2.

---

### Stream Processing Concepts in Confluent Cloud for Apache Flink | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/concepts/overview.html#flink-sql-stream-processing-concepts

Stream Processing Concepts in Confluent Cloud for Apache Flink¶ Apache Flink® SQL, a high-level API powered by Confluent Cloud for Apache Flink, offers a simple and easy way to leverage the power of stream processing. With support for a wide variety of built-in functions, queries, and statements, Flink SQL provides real-time insights into streaming data. Time is a critical element in stream processing, and Flink SQL makes it easy to process data as it arrives, avoiding delays. By using SQL syntax, you can declare expressions that filter, aggregate, route, and mutate streams of data, simplifying your data processing workflows. Stream processing¶ Streams are the de-facto way to create data. Whether the data comprises events from web servers, trades from a stock exchange, or sensor readings from a machine on a factory floor, data is created as part of a stream. When you analyze data, you can either organize your processing around bounded or unbounded streams, and which of these paradigms you choose has significant consequences. Batch processing is the paradigm at work when you process a bounded data stream. In this mode of operation, you can choose to ingest the entire dataset before producing any results, which means that it’s possible, for example, to sort the data, compute global statistics, or produce a final report that summarizes all of the input. Snapshot queries are a type of batch processing query that enables you to process a subset of data from a Kafka topic. Stream processing, on the other hand, involves unbounded data streams. Conceptually, at least, the input may never end, and so you must process the data continuously as it arrives. Bounded and unbounded tables¶ In the context of a Flink table, bounded mode refers to processing data that is finite, which means that the dataset has a clear beginning and end and does not grow continuously or update over time. This is in contrast to unbounded mode, where data arrives as a continuous stream, potentially with no end. The scan.bounded.mode property controls how Flink consumes data from a Kafka topic. A table can be bounded by committed offsets in Kafka brokers of a specific consumer group, by latest offsets, or by a user-supplied timestamp. Key characteristics of bounded mode¶ Finite data: The table represents a static dataset, similar to a traditional table in a relational database or a file in a data lake. Once all records are read, there is no more data to process. Batch processing: Operations on bounded tables are executed in batch mode. This means Flink processes all the available data, computes the results, and then the job finishes. This is suitable for use cases like ETL, reporting, and historical analysis. Optimized execution: Since the system knows the data is finite, it can apply optimizations that are not possible with unbounded (streaming) data. For example, it can sort by any column, perform global aggregations, and use blocking operators. No need for state retention: Unlike streaming mode, where Flink must keep state around to handle late or out-of-order events, batch mode can drop state as soon as it is no longer needed, reducing resource usage. The following table compares the characteristics of bounded and unbounded tables. Aspect Bounded Mode (Batch) Unbounded Mode (Streaming) Data Size Finite (static) Infinite (dynamic, continuous) Processing Style Batch processing Real-time/continuous processing Query Semantics All data available at once Data arrives over time State Management Minimal, can drop state when done Must retain state for late/out-of-order data Use Cases ETL, reporting, historical analytics Real-time analytics, monitoring, alerting Parallel dataflows¶ Programs in Flink are inherently parallel and distributed. During execution, a stream has one or more stream partitions, and each operator has one or more operator subtasks. The operator subtasks are independent of one another, and execute in different threads and possibly on different machines or containers. The number of operator subtasks is the parallelism of that particular operator. Different operators of the same program may have different levels of parallelism. A parallel dataflow in Flink with condensed view (above) and parallelized view (below).¶ Streams can transport data between two operators in a one-to-one (or forwarding) pattern, or in a redistributing pattern: One-to-one streams (for example between the Source and the map() operators in the figure above) preserve the partitioning and ordering of the elements. That means that subtask[1] of the map() operator will see the same elements in the same order as they were produced by subtask[1] of the Source operator. Redistributing streams (as between map() and keyBy/window above, as well as between keyBy/window and Sink) change the partitioning of streams. Each operator subtask sends data to different target subtasks, depending on the selected transformation. Examples are keyBy() (which re-partitions by hashing the key), broadcast(), or rebalance() (which re-partitions randomly). In a redistributing exchange the ordering among the elements is only preserved within each pair of sending and receiving subtasks (for example, subtask[1] of map() and subtask[2] of keyBy/window). So, for example, the redistribution between the keyBy/window and the Sink operators shown above introduces non-determinism regarding the order in which the aggregated results for different keys arrive at the Sink. Timely stream processing¶ For most streaming applications it is very valuable to be able re-process historic data with the same code that is used to process live data - and to produce deterministic, consistent results, regardless. It can also be crucial to pay attention to the order in which events occurred, rather than the order in which they are delivered for processing, and to be able to reason about when a set of events is (or should be) complete. For example, consider the set of events involved in an e-commerce transaction, or financial trade. These requirements for timely stream processing can be met by using event time timestamps that are recorded in the data stream, rather than using the clocks of the machines processing the data. Stateful stream processing¶ Flink operations can be stateful. This means that how one event is handled can depend on the accumulated effect of all the events that came before it. State may be used for something simple, such as counting events per minute to display on a dashboard, or for something more complex, such as computing features for a fraud detection model. A Flink application is run in parallel on a distributed cluster. The various parallel instances of a given operator will execute independently, in separate threads, and in general will be running on different machines. The set of parallel instances of a stateful operator is effectively a sharded key-value store. Each parallel instance is responsible for handling events for a specific group of keys, and the state for those keys is kept locally. The following diagram shows a job running with a parallelism of two across the first three operators in the job graph, terminating in a sink that has a parallelism of one. The third operator is stateful, and a fully-connected network shuffle is occurring between the second and third operators. This is being done to partition the stream by some key, so that all of the events that need to be processed together will be. A Flink job running with a parallelism of two.¶ State is always accessed locally, which helps Flink applications achieve high throughput and low-latency. State management¶ Fault tolerance via state snapshots¶ Flink is able to provide fault-tolerant, exactly-once semantics through a combination of state snapshots and stream replay. These snapshots capture the entire state of the distributed pipeline, recording offsets into the input queues as well as the state throughout the job graph that has resulted from having ingested the data up to that point. When a failure occurs, the sources are rewound, the state is restored, and processing is resumed. As depicted above, these state snapshots are captured asynchronously, without impeding the ongoing processing. Table programs that run in streaming mode leverage all capabilities of Flink as a stateful stream processor. In particular, a table program can be configured with a state backend and various checkpointing options for handling different requirements regarding state size and fault tolerance. It is possible to take a savepoint of a running Table API and SQL pipeline and to restore the application’s state at a later point in time. State usage¶ Due to the declarative nature of Table API & SQL programs, it is not always obvious where and how much state is used within a pipeline. The planner decides whether state is necessary to compute a correct result. A pipeline is optimized to claim as little state as possible given the current set of optimizer rules. Conceptually, source tables are never kept entirely in state. An implementer deals with logical tables, named Tables and Topics. Their state requirements depend on the operations that are in use. Queries such as SELECT ... FROM ... WHERE which consist only of field projections or filters are usually stateless pipelines. But operations like joins, aggregations, or deduplications require keeping intermediate results in a fault-tolerant storage for which Flink state abstractions are used. Refer to the individual operator documentation for more details about how much state is required and how to limit a potentially ever-growing state size. For example, a regular SQL join of two tables requires the operator to keep both input tables in state entirely. For correct SQL semantics, the runtime needs to assume that a match could occur at any point in time from both sides of the join. Flink provides optimized window and interval joins that aim to keep the state size small by exploiting the concept of watermark strategies. Another example is the following query that computes the number of clicks per session. SELECT sessionId, COUNT(*) FROM clicks GROUP BY sessionId; The sessionId attribute is used as a grouping key and the continuous query maintains a count for each sessionId it observes. The sessionId attribute is evolving over time and sessionId values are only active until the session ends, i.e., for a limited period of time. However, the continuous query cannot know about this property of sessionId and expects that every sessionId value can occur at any point of time. It maintains a count for each observed sessionId value. Consequently, the total state size of the query is continuously growing as more and more sessionId values are observed. Dataflow Model¶ Flink implements many techniques from the Dataflow Model. The following articles provide a good introduction to event time and watermark strategies. Blog post: Streaming 101 by Tyler Akidau Dataflow Model Related content¶ Autopilot Comparison with Apache Flink Compute Pools Dynamic Tables Statements Time and Watermarks Time attributes Joins in Continuous Queries Determinism in Continuous Queries Note This website includes content developed at the Apache Software Foundation under the terms of the Apache License v2.

#### Code Examples

```sql
SELECT ... FROM ... WHERE
```

```sql
SELECT sessionId, COUNT(*) FROM clicks GROUP BY sessionId;
```

---

### How-to Guides for Developing Flink Applications on Confluent Cloud for Apache Flink | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/how-to-guides/overview.html#flink-sql-how-to-guides

How-to Guides for Confluent Cloud for Apache Flink¶ Discover how Confluent Cloud for Apache Flink® can help you accomplish common processing tasks such as joins and aggregations. This section provides step-by-step guidance on how to use Flink to process your data efficiently and effectively. Aggregate a Stream in a Tumbling Window Combine Streams and Track Most Recent Records Compare Current and Previous Values in a Data Stream Convert the Serialization Format of a Topic Create a User Defined Function Handle Multiple Event Types Process Schemaless Events Resolve Common SQL Query Problems Run a Snapshot Query Scan and Summarize Tables View Time Series Data Flink actions¶ Confluent Cloud for Apache Flink provides Flink Actions that enable you to perform specific data-processing tasks on topics with minimal configuration. These actions are designed to simplify common workloads by providing a user-friendly interface to configure and execute them. Create an Embedding: Convert data in a topic’s column into a vector embedding for AI model inference. Deduplicate Rows in a Table: Remove duplicate records from a topic based on specified fields, ensuring that only unique records are retained in the output topic. Mask Fields in a Table: Mask sensitive data in specified fields of a topic by replacing the original data with a static value. Transform a Topic: Change a topic’s properties by applying custom Flink SQL transformations. Related content¶ Video: How to Set Idle Timeouts Video: How to Analyze Data from a REST API with Flink SQL Video: How To Use Streaming Joins with Apache Flink Video: How to Visualize Real-Time Data from Apache Kafka using Apache Flink SQL and Streamlit Use Flink SQL with Kafka, Streamlit, and the Alpaca API Note This website includes content developed at the Apache Software Foundation under the terms of the Apache License v2.

---

### Operate and Deploy Flink SQL Statements with Confluent Cloud for Apache Flink | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/operate-and-deploy/overview.html#flink-sql-operate-and-deploy

Operate and Deploy Flink Statements with Confluent Cloud for Apache Flink¶ Confluent provides tools for operating Confluent Cloud for Apache Flink® in the Cloud Console, the Confluent CLI, the Confluent Terraform Provider, and the REST API: Deploy a Statement Billing Monitor Statements with Cloud Console CLI commands Terraform resources REST API RBAC Flink API Keys Note This website includes content developed at the Apache Software Foundation under the terms of the Apache License v2.

---

### Flink SQL and Table API Reference in Confluent Cloud for Apache Flink | Confluent Documentation
Source: https://docs.confluent.io/cloud/current/flink/reference/overview.html#flink-sql-reference

Flink SQL and Table API Reference in Confluent Cloud for Apache Flink¶ This section describes the SQL language support in Confluent Cloud for Apache Flink®, including Data Definition Language (DDL) statements, Data Manipulation Language (DML) statements, built-in functions, and the Table API. Apache Flink® SQL is based on Apache Calcite, which implements the SQL standard. Data Types¶ Flink SQL has a rich set of native data types that you can use in SQL statements and queries. Data Types Serialize and deserialize data¶ Data Type Mappings Reserved keywords¶ Some string combinations are reserved as keywords for future use. Flink SQL Reserved Keywords Related content¶ Stream Processing Concepts Time and Watermarks Note This website includes content developed at the Apache Software Foundation under the terms of the Apache License v2.

---

### Introducing Confluent Cloud for Apache Flink
Source: https://www.confluent.io/blog/introducing-flink-on-confluent-cloud/

Join us at Current New Orleans! Save $500 with early bird pricing until August 15 | Register NowLogin Contact UsIn the first three parts of our Inside Flink blog series, we discussed the benefits of stream processing, explored why developers are choosing Apache Flink® for a variety of stream processing use cases, and took a deep dive into Flink's SQL API. In this post, we'll focus on how we’ve re-architected Flink as a cloud-native service on Confluent Cloud. However, before we get into the specifics, there is exciting news to share.As of today, Confluent’s fully managed Flink service is available for preview in select regions on AWS. We will be continuing to build out the offering and make it available to more regions and cloud providers during this preview phase. Check out the Flink quick start to see how you can try the industry's only cloud-native, serverless Flink service today. It is an exciting time to be a part of the Kafka and Flink communities, and we hope everyone takes advantage of this opportunity to try out the service.Part 1: Stream Processing Simplified: An Inside Look at Flink for Kafka UsersPart 2: Flink in Practice: Stream Processing Use Cases for Kafka UsersPart 3: Your Guide to Flink SQL: An In-depth Exploration Get Started FreeNow let’s turn our attention to Confluent Cloud for Apache Flink. What is it? How is it different? Why should you care? What is Confluent Cloud for Apache Flink?Simply put, Confluent Cloud for Apache Flink is Flink re-imagined as a truly cloud-native service.Confluent's fully managed Flink service allows you to:Effortlessly filter, join, and enrich your data streams with Flink, the de facto standard for stream processingEnable high-performance and efficient stream processing at any scale, without the complexities of infrastructure managementExperience Apache Kafka® and Flink as a unified platform, with fully integrated monitoring, security, and governance Flink serves as the streaming compute layer for KafkaWhen bringing Flink to Confluent Cloud, our goal was to provide a uniquely serverless experience beyond just "cloud-hosted" Flink. Kafka on Confluent Cloud goes beyond Apache Kafka through the Kora engine, which showcases Confluent's engineering expertise in building cloud-native data systems. Our goal is to deliver the same simplicity, security, and scalability for Flink as our customers expect for Kafka.Let’s double-click into each of the key benefits mentioned above.Filter, join, and enrich your data streamsAs we mentioned in the first blog post of this series, stream processing plays a critical role within the data streaming stack. Flink serves as the streaming compute layer to your Kafka storage layer. It empowers developers to query and inspect data streaming into Kafka, along with functionality to enrich, curate, and transform those streams for improved usability, portability, and compliance. One of the great benefits of Flink is its ANSI standard implementation of Flink SQL—if you know SQL, then you know Flink SQL. Our Flink service takes Flink’s SQL API further by integrating the operational catalog with the rest of the Kafka ecosystem on Confluent Cloud. Customers with Kafka topics and schemas in Schema Registry will already have tables to browse and query in Flink SQL without having to wrangle with tedious CREATE TABLE statements or data type mappings that so often trip people up. By eliminating the need to duplicate operational metadata and retaining one holistic view of your data, your first query on Confluent Cloud can simply be a SELECT statement, lowering the barrier to exploration and making it easier to understand and build upon existing data streams.With that said, submitting Flink SQL queries with open source Apache Flink is not straightforward or accessible to the broad community of users who know SQL due to operational complexity. Furthermore, CLI access isn’t for everyone. The goal at Confluent is to make Flink accessible for everyone! Our Flink service also includes a new SQL editor and workspace experience that’s fully integrated with Confluent Cloud and our upcoming Data Portal for data discovery. You can seamlessly move from browsing topics to writing queries in seconds. It’s really that simple. Rich SQL editor experience in the Confluent Cloud UIWe’ll cover the SQL editor and workspace experience in detail in a future blog post. Stay tuned!Enable stream processing at any scaleTo deliver a cloud-native experience at launch, we’ve focused on a few core principles:Flink must be serverless Flink must be simple to useFlink must be independently scalable from KafkaThis approach enabled us to offer a cloud-native service that is highly scalable, easy to use, and optimized for efficient resource utilization.ServerlessApache Flink has a cluster-based architecture that provides building blocks for elastic scalability, offers a consistent suite of polyglot APIs, and is supported by a vibrant developer community. However, as with open source Kafka, it's not all smooth sailing. Apache Flink can be very challenging to operate and manage on your own.Developers must first evaluate the upfront cost of deploying and managing the framework before creating their first application. Next, clusters need to be maintained and the applications that run on top need to stay up-to-date with the framework, making upgrades to Flink painful. This had to change with our Flink offering—enter serverless Flink.The term "serverless" itself can have many connotations. To us, it has three primary dimensions:Elastic autoscaling with scale-to-zeroEvergreen runtime and APIsUsage-based billingElastic autoscaling with scale-to-zeroOn Confluent Cloud, Flink workloads scale automatically without the need for user intervention. Our autoscaler takes care of everything, from managing the scale-out to determining parallelism, load balancing, and more. There is no need to pre-size your workload or take into account operational peaks with capacity planning. Maximize resource utilization and avoid over-provisioning infrastructure Evergreen runtime & APIsThe Flink runtime must always be up-to-date, providing you with the latest functionality. There must also be strong backward compatibility guarantees so that existing applications continue to function as the runtime is upgraded. As a result, the runtime is not versioned, and its upgrades are fully automated—i.e., what the user experiences is a fully managed service. The APIs we expose should also be declarative. You state their intended outcome, and Flink determines how that result is achieved, enabling you to focus on building business logic, not managing infrastructure.Usage-based billingFinally, you should pay only for what you use, not what you provision. Flink compute is ephemeral in Confluent Cloud. Once you stop using the compute resources, they are deallocated, and you no longer pay for them. Coupled with the elasticity provided by scale-to-zero, you can benefit from unbounded scalability while maintaining cost efficiency.Tying it all together: Flink Compute PoolsTo tie our serverless principles together, our Flink service exposes a new concept known as Flink compute pools. Compute pools provide users with seamless access to the elastic compute resources of Flink. Developers simply deploy apps without having to pre-determine the resources needed to operate them, and pools automatically expand and contract based on the resources required, improving developer productivity and cost efficiency.A compute pool can support multiple apps and statements running in parallel, taking advantage of the peaks and troughs each app experiences. Resource usage is aggregated to the compute pool level, eliminating app sizing and replacing it with a simple, user-defined budgetary cap for each pool being used. Operators can easily manage access to the compute resources of Flink, segment workloads by pool, and separate billing line items for each pool.Flink compute pools provide elastic compute resources To create a pool, simply pick a cloud provider and region, give it a name, and set its budget. The options provided in the interface automatically detect the regions where you have stored your data streams, helping to ensure the pool is co-located with your data (i.e., Kafka cluster). Everything else is taken care of by us.SimpleManaging apps on Apache Flink can also be challenging. Each application has to be independently sized and subsequently managed on an ongoing basis. Developers need to remain actively involved as the workload fluctuates, determining application scale and the required parallelism.You don’t size apps when using Flink on Confluent Cloud. You create Flink compute pools and Confluent takes care of the rest—managing resource assignments, parallelism, and the scale of the pool. Our implementation also takes care of advanced deployment challenges, such as high availability, resource management, security, and auditing. Our goal is for developers to focus on app development, not complex infrastructure-related tasks.Simplicity isn’t just reserved for scaling and managing applications—it permeates our whole approach to unifying Flink with Kafka. For example, open source Apache Flink has two different connectors when creating tables against Apache Kafka—one for reads and writes, and another for Upserts. This means having two different table definitions that need to be kept in sync as schemas evolve. There is only one unified Kafka connector for Flink SQL in Confluent Cloud, meaning you only need one table to perform any operation.ScalableThe scalability of a cloud-native service is closely tied to the separation of compute and storage. By separating these two components, you can scale each one independently, allowing for more efficient use of resources and better overall scalability. Although Flink is closely integrated with Kafka and Schema Registry metadata, it applies separation semantics to all data sources, including Kafka. For example, many stream processing applications need to join streams together, but not all streams are stored in the same Kafka cluster. Many companies organize their storage by line of business, geography, or domain. This works well until a business unit has cross-cutting questions and needs to join data from different clusters and potentially write back to another Kafka cluster. We wanted Flink to take an unbounded approach when reading and writing data, reaching across environments and clusters and enriching data streams wherever they exist. Flink's separation of compute from Kafka storage means that joining across domains is a completely seamless experience. What's more, this allows you to better align how you organize your data streams to your business needs, empowering you to have smaller, domain-specific clusters rather than forcing all data into one monolithic cluster.This separation also allows us to offer our service with more affordable pricing. Flink and Kafka services are co-located on Confluent Cloud, meaning Flink only reads and writes to Kafka clusters in the same cloud region (i.e., saving you on those hidden networking costs). Furthermore, Flink in Confluent Cloud will be designed so that reads from Flink are aligned to the same availability zones as Kafka whenever possible. This capability is known as "Fetch From Follower" and effectively eliminates expensive network traffic charges.Experience Kafka & Flink as a unified platform Integrating Flink has enabled Confluent to double down on our capabilities for stream processing, providing a generalized layer of streaming compute over streaming data movement and storage powered by the Kora engine. However, Confluent is much more than Kafka and Flink. Customers benefit from the fact that Confluent is a complete data streaming platform. Just like with Kafka, Flink is fully integrated with our tooling for security, governance, and observability.SecurityOpen source Apache Flink does not have a security model built into its framework. This is a critical dimension that organizations have to address when deploying Flink. By contrast, Confluent Cloud offers a robust and secure suite of capabilities to control, manage, and govern access to data.To enable secure stream processing, Flink inherits the same Identity and Access Management providers available on Confluent Cloud. In addition, our role-based access control (RBAC) has been extended to include Flink, creating new roles for scalable management of permissions. Developers can access a compute pool using Flink RBAC roles defined at the environment or compute pool level.RBAC provides platform-wide security with granular access to critical resources Your data is already secured at the data level in Confluent Cloud, and Flink is fully integrated with our security model to enforce those controls. Flink complies with our Trust and Security policies, so auditing is always on. Finally, Flink uses managed service accounts for the execution of continuous statements, improving manageability.GovernanceFlink can read from and write to any Confluent Cloud Kafka cluster in the same region. However, we do not allow you to query across regions by design. Not only do we want to help you avoid expensive data transfer charges, but we also want to protect data locality and sovereignty by keeping reads and writes in-region.When deploying open source Apache Flink, you must first integrate it with a metadata management repository. This enables Flink to persist technical metadata, making it durable across sessions. So far, so good. However, what if you already have topics in Kafka and schemas in Schema Registry? Sadly, you must create any and all table definitions yourself and maintain them over time. This duplication of metadata is hard to keep in sync and makes schema evolution significantly more challenging.Confluent Cloud provides a unified approach to metadata management. There is one object definition, and Flink integrates directly with that definition. In doing so, Flink avoids any unnecessary duplication of metadata and makes all topics immediately queryable via Flink SQL. Furthermore, any existing schemas in Schema Registry are used to surface fully defined entities in Confluent Cloud. If you’re already on Confluent Cloud, you will automatically see tables ready to query using Flink, simplifying data discovery and exploration.There is no need to maintain two disparate sets of technical metadata—instead, the experience looks like this:CREATE TABLE catalog.database.T1 ( C1 INT, C2 INT, PRIMARY KEY (C1) NOT ENFORCED ) ; When table T1 is created, three objects are created under the hood. The first is a topic called T1. The second and third objects are schema subjects T1-key and T1-value that capture the definition of the table, mapped to T1. Both the topic and the schemas are consumable by Flink and Kafka applications, enabling interoperability. You can see the schema subjects created for T1 below. Similarly, had I started with topic T2 and schema subjects T2-key and T2-value then Flink in Confluent Cloud would have automatically created table T2 for me making topic T2 immediately queryable.T1-key{ "fields": [ { "name": "C1", "type": "int" } ], "name": "record", "namespace": "org.apache.flink.avro.generated", "type": "record" } T1-value{ "fields": [ { "default": null, "name": "C2", "type": [ "null", "int" ] } ], "name": "record", "namespace": "org.apache.flink.avro.generated", "type": "record" }ObservabilityApache Flink provides a plethora of metrics to choose from, but the onus is on the DevOps team to select the right ones.Confluent Cloud provides you with a curated set of metrics to simplify the process, exposing them via Confluent's existing metrics API. An opinionated set of Flink metrics will soon be similarly exposed, providing a consistent approach to metrics and monitoring across all services in Confluent Cloud. For customers with established observability platforms in place, Confluent Cloud provides first-class integrations with New Relic, Datadog, Grafana Cloud, and Dynatrace.You can also monitor workloads directly within the Confluent Cloud UI. Clicking into a compute pool gives you insight into the health and performance of your applications, in addition to the resource consumption of your compute pool.Monitor compute pool utilization and metricsWhat’s next and getting startedAnd that’s a wrap (for now)! We hope you've enjoyed our “Inside Flink” blog series, where we’ve covered a lot of the questions we’re hearing around Flink. We are thrilled to have concluded with the introduction of the industry's only cloud-native, serverless Flink service.We have an exciting journey ahead, and this is only the beginning! We look forward to expanding our Flink service to additional clouds and regions. We’ll also be adding exciting new features, such as more automation for developer workflows, integration with OpenAI, support for programmatic APIs including Java and Python support, and a host of other exciting capabilities. Are you ready to get started? If you haven't already, sign up for a free trial of Confluent Cloud and create your first Flink SQL application within a matter of minutes using the Flink quick start. Use promo code CL60BLOG to get an additional $60 of free Confluent Cloud usage.*Interested in learning more? Be sure to register for the upcoming Flink webinar to get hands-on with a technical demo that showcases the full capabilities of Flink SQL on Confluent Cloud.Register for Flink Webinar Flink on Confluent Cloud WebinarGet hands-on with a demo that showcases the full capabilities of Flink SQL on Confluent Cloud.Register NowCourse: Building Apache Flink Applications in JavaLearn the basics of building Apache Flink applications in Java in our course on Confluent Developer.Start the CourseWritten ByJames Rowland-Jones (JRJ)Director of Product ManagementJames Rowland-Jones (JRJ) is a director of product management at Confluent, where he leads the Stream Processing and Analytics team. JRJ has over 20 years of experience in the technology industry, specializing in distributed cloud computing and analytics. Prior to joining Confluent, JRJ held senior leadership positions at Microsoft, leading stealth investments for Azure Synapse Analytics (now Azure Fabric), Azure SQL and SQL Server. James is a recognized thought leader and community contributor, having served for many years on the organizing bodies of www.SQLBits.com and the PASS organization. JRJ has spoken at numerous conferences and events around the world and is the author of several books and publications.Flink on Confluent Cloud WebinarGet hands-on with a demo that showcases the full capabilities of Flink SQL on Confluent Cloud.Register NowCourse: Building Apache Flink Applications in JavaLearn the basics of building Apache Flink applications in Java in our course on Confluent Developer.Start the CourseDid you like this blog post? Share it nowSubscribe to the Confluent blogSubscribeDataflow Programming with Apache Flink and Apache KafkaSep 14, 2023Learn how to build a Java pipeline that consumes clickstream data from Apache Kafka®. Consuming clickstreams is something that many businesses have a use for and it can also be generalized to consuming other types of streaming data.Wade WaldronYour Guide to Flink SQL: An In-Depth ExplorationSep 12, 2023Dive into Flink SQL, a powerful data processing engine that allows you to process and analyze large volumes of data in real time. We’ll cover how Flink SQL relates to the other Flink APIs and showcase some of its built-in functions and operations with syntax examples.Martijn VisserFeedback

#### Code Examples

```sql
CREATE TABLE catalog.database.T1 
(
C1 INT,
C2 INT, 
PRIMARY KEY (C1) NOT ENFORCED
)
;
```

```sql
{
  "fields": [
    {
      "name": "C1",
      "type": "int"
    }
  ],
  "name": "record",
  "namespace": "org.apache.flink.avro.generated",
  "type": "record"
}
```

```sql
{
  "fields": [
    {
      "default": null,
      "name": "C2",
      "type": [
        "null",
        "int"
      ]
    }
  ],
  "name": "record",
  "namespace": "org.apache.flink.avro.generated",
  "type": "record"
}
```

---

### Getting Started with Flink SQL: In-Depth Guide
Source: https://www.confluent.io/blog/getting-started-with-apache-flink-sql/

Join us at Current New Orleans! Save $500 with early bird pricing until August 15 | Register NowLogin Contact UsIn the first two parts of our Inside Flink blog series, we explored the benefits of stream processing with Flink and common Flink use cases for which teams are choosing to leverage the popular framework to unlock the full potential of streaming. Specifically, we broke down the key reasons why developers are choosing Apache Flink® as their stream processing framework, as well as the ways in which they are putting it into practice. These range from streaming data pipelines to train ML models, to real-time inventory management in retail and predictive maintenance in manufacturing.Next, we’ll dive into Flink SQL, which is a powerful data processing engine that allows developers to process and analyze large volumes of data in real time. We’ll cover how Flink SQL relates to the other Flink APIs and showcase some of its built-in functions and operations with syntax examples.Part 1: Stream Processing Simplified: An Inside Look at Flink for Kafka UsersPart 2: Flink in Practice: Stream Processing Use Cases for Kafka UsersPart 4: Introducing Confluent Cloud for Apache Flink For those who want to explore Flink SQL further, we recommend checking out the Flink 101 developer course on Confluent Developer. The course provides an in-depth introduction to Apache Flink, including a detailed module on Flink SQL with practical exercises.Table of contentsWhat is Flink SQL?How Flink SQL relates to other Flink APIsDynamic tables and continuous queriesThe first queries to get started with Flink SQLFlink SQL featuresJoinsAggregationsWindowsPattern RecognitionStreaming vs. batch in Flink SQLReady to put Flink SQL into practice?What is Flink SQL? Flink SQL is an ANSI standard compliant SQL engine that can process both real-time and historical data. It provides users with a declarative way to express data transformations and analytics on streams of data.With Flink SQL, users can easily transform and analyze data streams without having to write complex code. It supports a wide range of SQL operations, including filtering, aggregating, joining, and windowing. Flink SQL can be extended via user-defined functions (UDFs) that can be written in Java or Python. Additionally, it comes with an extensive ecosystem that includes a JDBC Driver, SQL Gateway, catalogs, and an interactive SQL shell.Overall, Flink SQL is an easy, yet powerful solution for processing data streams using SQL syntax. It provides users with a simple and efficient way to analyze and transform data. The widespread use and familiarity of SQL allows more developers to access and work with streaming data using SQL-based tools, even if they don't have prior experience with streaming data technologies.How Flink SQL relates to other Flink APIs Flink SQL is one of several APIs offered by Apache Flink for stream processing. As we mentioned in Part One, the different APIs in Flink cater to developers with varying levels of expertise and are suitable for simple to complex use cases. Flink features layered APIs at different levels of abstraction which offers flexibility to handle both common and specialized use casesSince all the APIs in Flink are interoperable, developers can use one or many APIs and switch between them as per their requirements. Flink SQL is an extremely powerful tool that can define both simple and complex queries, making it well-suited for most stream processing use cases, particularly building real-time data products and pipelines. However, in some cases where users require access to lower-level APIs for more customization, programmatic APIs, such as the DataStream API, may be recommended.Dynamic tables and continuous queries In Flink SQL, a table is a structured representation of data that can be queried using SQL syntax. Tables can be created from various sources such as streams, files, or other tables. To create a table, you register metadata to connect to the system you want to use, such as Kafka. Dynamic tables process streaming data and continuously update their results to reflect changes on input tables. The underlying data of a dynamic table is stored in the storage layer, which in this example is Kafka.Transforming a stream to a dynamic tableQuerying a dynamic table and sending the result of the continuous query to another dynamic tableFlink uses stream-table duality, allowing developers to use the same operations and functions to process both streams and tables. A stream is a record of changes in a dynamic table over time, known as a changelog. The changelog stream contains all changes, including “before” and “after” values, and can reconstruct the current state of the table at any point in time.Understanding the types of changelogs is important because it helps Flink SQL users to design their jobs in a way that ensures fault tolerance and consistency of their data processing.Flink provides four different types of changelog entries:+IInsertionThis records only the insertions that occur.-UUpdate BeforeThis retracts a previously emitted result. +UUpdate AfterThis updates a previously emitted result. This requires a primary key if -U is omitted for idempotent updates.-DDeleteThis deletes the last result.Flink's changelogs have various uses depending on the application. For example, Insertion changelogs track new orders, Update Before changelogs track stock price changes, Update After changelogs track updated order amounts, and Delete changelogs track canceled orders. Flink generates the appropriate changelog based on the query but may require manual specification in some cases.Depending on the combination of source, sink, and business logic applied, you can end up with the following types of streams: Contains only +I Appending/Insert only Contains not only +I Contains -U Never -U but +U Updating Retracting Upserting The first queries to get started with Flink SQLThe first step of data processing requires you to connect to your source system by defining a table. The Flink Catalog provides a unified metadata management system for Flink's Table API and SQL. It is a logical namespace that contains metadata information about data sources, such as Apache Kafka, file systems, and databases. The Flink Catalog enables users to define and manage data sources, tables, and views that can be queried using SQL.For example, here's how you can register the metadata in Flink to connect to your Kafka topic transactions:CREATE TABLE transactions ( `name` STRING, `amount` INT ) WITH ( 'connector' = 'kafka', 'topic' = transactions, 'properties.bootstrap.servers' = 'YourKafkaBroker', 'properties.group.id' = 'YourConsumerID', 'scan.startup.mode' = 'earliest-offset', 'format' = 'avro', 'properties.security.protocol' = 'SASL_SSL', 'properties.sasl.mechanism' = 'PLAIN', 'properties.sasl.jaas.config' = 'org.apache.flink.kafka.shaded.org.apache.kafka.common.security.plain.PlainLoginModule required username="YourKey" password="YourSecret";' ); If you want to store the result of your data product, such as the revenue per user, you would first register appropriate metadata, like this:CREATE TABLE revenue ( `name` STRING, `total` INT ) WITH ( 'connector' = 'kafka', 'topic' = 'number_of_requests', 'properties.bootstrap.servers' = 'YourKafkaBroker', 'properties.group.id' = 'YourConsumerID', 'scan.startup.mode' = 'earliest-offset', 'format' = 'avro', 'properties.security.protocol' = 'SASL_SSL', 'properties.sasl.mechanism' = 'PLAIN', 'properties.sasl.jaas.config' = 'org.apache.flink.kafka.shaded.org.apache.kafka.common.security.plain.PlainLoginModule required username="YourKey" password="YourSecret";' ); You can find all security configuration options in the Flink documentation.Secondly, write the desired business logic in your SQL statement, like this:INSERT INTO revenue SELECT name, SUM(amount) as total FROM transactions GROUP BY name; When using Flink on Confluent Cloud, you can start writing your business logic directly, since all of the Confluent Cloud metadata is automatically available and ready to use.Flink SQL features Flink SQL is compliant with ANSI SQL standards, making it easy for those familiar with relational databases to learn. It offers a wide range of SQL features and functions, including support for joins, aggregations, windows, and more. With Flink's unified API for batch and stream processing, you can apply these features to both bounded and unbounded data, allowing users to write complex queries that handle a variety of data processing tasks.Flink SQL applies advanced optimization techniques, such as query optimization and cost-based optimization, to ensure queries are executed efficiently and with minimal resource usage. This allows users to process large volumes of data quickly and efficiently, even in complex data processing scenarios.JoinsFlink SQL offers a variety of joins, including inner joins and outer joins (left, right, and full), cross joins, and other special joins. Each of these joins follows the conventional semantics for its respective operation.Inner and Outer joinsIn Flink SQL, inner and outer joins are used to combine rows from two or more tables based on a common column between them.Inner join: Returns only the matching rows from both tablesLeft outer join: Returns all the rows from the left table and matching rows from the right table, or null values if there is no matchRight outer join: Returns all the rows from the right table and matching rows from the left table, or null values if there is no matchFull outer join: Returns all the rows from both tables, or null values if there is no matchHere's an example query that illustrates how to use inner and outer joins:SELECT SUM(o.amount), p.BrandID, s.StoreName, d.QuarterYear FROM Orders AS o INNER JOIN DateDim AS d ON o.DateID = d.DateID -- LEFT OUTER Join to stores in case stores are missing LEFT OUTER JOIN Store AS s ON o.StoreID = s.StoreID -- LEFT OUTER JOIN to product in case products are missing LEFT OUTER JOIN Product AS p ON o.ProductID = p.ProductID GROUP BY p.BrandID, s.StoreName, d.QuarterYear; This query calculates the total amount of orders placed by brand and store in a specific quarter. It first joins the Orders table with the DateDim table on the shared column "DateID". It then performs left outer joins with the Store and Product tables on the common columns "StoreID" and "ProductID", respectively. Note that right outer joins could have been used instead. The result is then grouped by brand, store, and quarter using the GROUP BY clause.Cross join In Flink SQL, a cross join is a type of join that returns the Cartesian product of the two tables being joined. The Cartesian product is a combination of every row from the first table with every row from the second table. This feature can be particularly useful when you need to expand an array column into multiple rows.SELECT order_id, items FROM orders CROSS JOIN UNNEST(items) AS i (items) Interval joinAn interval join is another type of join that allows users to join two streams or tables based on a time interval or range. In an interval join, the join condition is based on a time attribute, and the join result includes all rows that fall within the specified time interval or range.For instance, this query will join all orders with their corresponding shipments if the order was shipped four hours after it was received.SELECT * FROM orders o, shipments s WHERE o.id = s.order_id AND o.order_time BETWEEN s.ship_time - INTERVAL '4' HOUR AND s.ship_time Temporal joinA temporal join is a type of join that enables users to join two streams or tables based on a temporal relationship between the records. In a temporal join, the join condition is based on a time attribute, and the join result includes all rows that satisfy the temporal relationship. A common use case for temporal joins is analyzing financial data, which often includes information that changes over time, such as stock prices, interest rates, and exchange rates.SELECT order_id, price, orders.currency, conversion_rate, order_time FROM orders LEFT JOIN currency_rates FOR SYSTEM_TIME AS OF orders.order_time ON orders.currency = currency_rates.currency; Lateral join A lateral join in Flink SQL is a type of join that allows you to apply a table-valued function to each row of a table and generate additional rows based on the function's output. Lateral joins are useful for scenarios where you need to split a column into multiple rows or generate additional rows based on complex calculations or queries. Suppose you have a table called order_items that contains information about the items in each order, and you want to find the top 2 items with the highest price per order. You can use a lateral join to join the orders table with the order_items table and get the top 2 items with the highest price for each order.SELECT o.order_id, oi.item_id, oi.item_name, oi.price FROM orders o, LATERAL ( SELECT oi.item_id, oi.item_name, oi.price FROM order_items oi WHERE oi.order_id = o.order_id ORDER BY oi.price DESC LIMIT 2 ) AS oi; Aggregations In addition to joining data, there is often a need to perform various aggregations. For example, the first Flink SQL query in this blog post already used a GROUP BY aggregation.OVER aggregation The OVER aggregation (covered in Part Two) is a critical tool for analyzing streaming data over time windows. Unlike typical window aggregates in a SQL database, OVER uses a sliding or tumbling window of rows over a specified time period. This makes it ideal for processing and analyzing constantly changing data streams.Additionally, the OVER clause can define time-based windows with the help of a watermark to track the progress of time in a data stream. By using the OVER aggregation, users can perform complex calculations and aggregations on streaming data in real time, gaining valuable insights into emerging trends and patterns.For instance, suppose we have a table of sales data that includes the product ID, sales date, and sales amount. We can use an OVER aggregation to calculate the rolling sum of sales for each product over a sliding window of the past 7 days.SELECT product_id, sales_date, sales_amount, SUM(sales_amount) OVER ( PARTITION BY product_id ORDER BY sales_date RANGE BETWEEN INTERVAL '7' DAY PRECEDING AND CURRENT ROW ) AS rolling_sum FROM sales_data; TopN aggregationAnother type of aggregation is the TopN aggregation, which enables users to find the top N values for a given column within a sliding or tumbling window of rows. The TopN function is applied to the window of rows and returns the top N rows based on the specified column and ordering.For instance, suppose we have a table of website traffic data that includes the page URL, visitor IP address, and number of visits. We can use a TopN aggregation to find the top 10 most visited pages over a sliding window of the past 24 hours.Here's an example of what the SQL query might look like:SELECT page_url, total_visits FROM ( SELECT page_url, SUM(visits) AS total_visits, ROW_NUMBER() OVER (ORDER BY SUM(visits) DESC) AS rownum FROM traffic_data WHERE event_time >= TIMESTAMPADD(HOUR, -24, CURRENT_TIMESTAMP) GROUP BY page_url ) t WHERE rownum <= 10; Windows In addition to aggregations, Flink also provides various ways to window data. A window is a logical grouping of rows based on a time or key attribute that is used to define a subset of the data over which an aggregation will be performed. Flink's table valued functions offer tumbling, hopping, or cumulative windows:Tumbling window Tumbling windows divide data into non-overlapping, fixed-size windows. This is useful when you want to analyze data in discrete time intervals. For instance, this SQL statement uses a tumbling window to group the data in the “orders” table based on a fixed-size time interval of 10 minutes.SELECT * FROM TABLE( TUMBLE(TABLE orders, DESCRIPTOR(order_time), INTERVAL '10' MINUTES)); Hopping window A hopping window is a type of window that groups rows based on a sliding time interval. Unlike a tumbling window, which groups rows into non-overlapping fixed-size windows, a hopping window groups rows into overlapping windows of a fixed size and with a specified slide interval.This can be useful when analyzing time-series data, where you want to see how a certain metric changes over time in a more granular way than a tumbling window would allow. For example, this query uses a hopping window to group the rows in the orders table into overlapping windows of 5 minutes with a slide interval of 10 minutes:SELECT * FROM TABLE( HOP(TABLE orders, DESCRIPTOR(order_time), INTERVAL '5' MINUTES, INTERVAL '10' MINUTES)); Cumulative window A cumulative window is a type of window that groups rows based on a cumulative condition. Unlike tumbling or hopping windows, which group rows based on fixed-size time intervals, a cumulative window groups rows based on a condition that accumulates over time.An example of when you would use a cumulative window is when you want to calculate a running total or a rolling average of a certain metric over time. For instance, this query calculates the cumulative sales for each order in the orders table over a window size of 2 minutes and a slide interval of 10 minutes:SELECT * FROM TABLE( CUMULATE(TABLE orders, DESCRIPTOR(order_time), INTERVAL '2' MINUTES, INTERVAL '10' MINUTES)); Pattern recognition During my keynote demo at KSL 2023, I talked about pattern recognition, which is a crucial aspect of data analysis. One of the powerful features that Flink SQL offers for pattern recognition is MATCH_RECOGNIZE. This feature enables pattern matching on a stream of data, allowing you to specify a pattern of events that you want to detect within the stream. You can then perform various calculations or actions based on that patternHere's an example of what the SQL query might look like:SELECT * FROM StockPrice MATCH_RECOGNIZE ( PARTITION BY symbol ORDER BY event_time MEASURES FIRST(price) AS start_price, LAST(price) AS end_price, AVG(price) AS avg_price PATTERN (start up+ down+ end) DEFINE up AS price > PREV(price), down AS price < PREV(price) ) In this example, the MATCH_RECOGNIZE statement is used to detect patterns in a stream of stock price data. The PARTITION BY clause is used to group the data by stock symbol, and the ORDER BY clause is used to order the data by event time.The MEASURES clause is used to define the measures that we want to calculate based on the pattern. In this case, we're calculating the first and last prices in each pattern, as well as the average price.The PATTERN clause is used to define the pattern that we want to detect. In this case, we're looking for patterns that start with one or more upward price movements, followed by one or more downward price movements, and end with a downward movement.The DEFINE clause is used to define the conditions that must be met for each part of the pattern. In this case, an upward movement is defined as a price that is higher than the previous price, and a downward movement is defined as a price that is lower than the previous price.Streaming vs. batch in Flink SQLFlink SQL supports both streaming and batch processing modes, allowing developers to use the same SQL queries for both batch and streaming data processing without the need for rewriting code.Flink can unify stream and batch processing under the same umbrellaIn streaming mode, Flink SQL processes continuous streams of data in real time, and only supports sorting by time. This means you can use ORDER BY to sort data based on a time attribute of each record. In batch mode, Flink SQL processes static datasets that do not change over time, and supports sorting by any column using ORDER BY.Flink SQL's streaming mode has optimizations for temporal joins that take advantage of the time-based nature of the data, making them more efficient than regular joins. However, batch mode does not have these optimizations, so regular joins are used instead. This can be less efficient for streaming data.When choosing between streaming and batch processing modes in Flink SQL, consider the nature of your data and the type of processing you need to perform. Streaming mode is ideal for real-time processing of continuous data, while batch mode is best suited for processing static datasets. For example, use streaming mode for use cases such as real-time fraud detection in financial transactions, and use batch mode for generating financial reports on a daily, weekly, or monthly basis.Each mode has its own set of features and limitations, so it's important to choose the mode that best fits your use case.Ready to put Flink SQL into practice? Flink SQL is a powerful tool for processing data streams with SQL syntax, suitable for real-time data products or generating reports from static datasets. It offers features and capabilities for a wide range of use cases. To explore Flink SQL further, try our Flink 101 developer course.We're excited to share the amazing lineup of talks on Flink and Kafka at the Current 2023 data streaming conference in San Jose on September 26-27th. Learn from top Flink experts and gain valuable insights into the latest trends and best practices in data streaming. Register now to secure your spot at the conference!Our next blog post will explore making Flink cloud-native, including the benefits and factors to consider when deploying Flink in a cloud-native environment. Whether you're new to cloud-native design principles or a seasoned expert, this post will provide valuable insights and practical advice for running Flink in the cloud. Keep an eye out for the next post in our series! Course: Building Apache Flink Applications in JavaLearn the basics of building Apache Flink applications in Java in our course on Confluent Developer.Start the CourseThe Confluent Developer NewsletterGet Apache Kafka and Flink news delivered to your inbox biweekly or read the latest editions on Confluent Developer!Read the LatestWritten ByMartijn VisserGroup Product ManagerMartijn Visser is Group Product Manager at Confluent and PMC member and committer for the Apache Flink project. He works with the open source community on user-facing features such as the Table/SQL and DataStream API, connectors, and formats. Prior to joining Confluent, he worked as product manager at Immerok, which was acquired by Confluent. He has also worked as product manager at Ververica, where he was responsible for product development on Apache Flink and Ververica Platform, and as product lead at ING, where he was responsible for ING's Streaming Data and Engagement Platform.Course: Building Apache Flink Applications in JavaLearn the basics of building Apache Flink applications in Java in our course on Confluent Developer.Start the CourseThe Confluent Developer NewsletterGet Apache Kafka and Flink news delivered to your inbox biweekly or read the latest editions on Confluent Developer!Read the LatestDid you like this blog post? Share it nowSubscribe to the Confluent blogSubscribeStream Processing Simplified: An Inside Look at Flink for Kafka UsersAug 15, 2023Learn why stream processing is such a critical component of the data streaming stack, why developers are choosing Apache Flink as their stream processing framework of choice, and how to use Flink with Kafka.Konstantin KnaufIntroducing Confluent Cloud for Apache FlinkSep 26, 2023As of today, Confluent Cloud for Apache Flink® is available for preview in select regions on AWS. In this post, learn how we’ve re-architected Flink as a cloud-native service on Confluent Cloud.James Rowland-Jones (JRJ)Feedback

#### Code Examples

```sql
transactions
```

```sql
CREATE TABLE transactions (
  `name` STRING,
  `amount` INT
) WITH (
  'connector' = 'kafka',
  'topic' = transactions,
  'properties.bootstrap.servers' = 'YourKafkaBroker',
  'properties.group.id' = 'YourConsumerID',
  'scan.startup.mode' = 'earliest-offset',
  'format' = 'avro',
  'properties.security.protocol' = 'SASL_SSL',
  'properties.sasl.mechanism' = 'PLAIN',
  'properties.sasl.jaas.config' = 'org.apache.flink.kafka.shaded.org.apache.kafka.common.security.plain.PlainLoginModule required username="YourKey" password="YourSecret";'
);
```

```sql
CREATE TABLE revenue (
  `name` STRING,
  `total` INT
) WITH (
  'connector' = 'kafka',
  'topic' = 'number_of_requests',
  'properties.bootstrap.servers' = 'YourKafkaBroker',
  'properties.group.id' = 'YourConsumerID',
  'scan.startup.mode' = 'earliest-offset',
  'format' = 'avro',
  'properties.security.protocol' = 'SASL_SSL',
  'properties.sasl.mechanism' = 'PLAIN',
  'properties.sasl.jaas.config' = 'org.apache.flink.kafka.shaded.org.apache.kafka.common.security.plain.PlainLoginModule required username="YourKey" password="YourSecret";'
);
```

```sql
INSERT INTO revenue
SELECT name, SUM(amount) as total
FROM transactions
GROUP BY name;
```

```sql
SELECT SUM(o.amount), p.BrandID, s.StoreName, d.QuarterYear
FROM Orders AS o
INNER JOIN DateDim AS d
ON o.DateID = d.DateID
-- LEFT OUTER Join to stores in case stores are missing
LEFT OUTER JOIN Store AS s
ON o.StoreID = s.StoreID
-- LEFT OUTER JOIN to product in case products are missing 
LEFT OUTER JOIN Product AS p
ON o.ProductID = p.ProductID
GROUP BY p.BrandID, s.StoreName, d.QuarterYear;
```

```sql
SELECT order_id, items
FROM orders CROSS JOIN UNNEST(items) AS i (items)
```

```sql
SELECT *
FROM orders o, shipments s
WHERE o.id = s.order_id
AND o.order_time BETWEEN s.ship_time - INTERVAL '4' HOUR AND s.ship_time
```

```sql
SELECT 
     order_id,
     price,
     orders.currency,
     conversion_rate,
     order_time
FROM orders
LEFT JOIN currency_rates FOR SYSTEM_TIME AS OF orders.order_time
ON orders.currency = currency_rates.currency;
```

```sql
order_items
```

```sql
order_items
```

```sql
SELECT o.order_id, oi.item_id, oi.item_name, oi.price
FROM orders o, LATERAL (
    SELECT oi.item_id, oi.item_name, oi.price
    FROM order_items oi
    WHERE oi.order_id = o.order_id
    ORDER BY oi.price DESC
    LIMIT 2
) AS oi;
```

```sql
SELECT product_id, sales_date, sales_amount, SUM(sales_amount) OVER (
  PARTITION BY product_id
  ORDER BY sales_date
  RANGE BETWEEN INTERVAL '7' DAY PRECEDING AND CURRENT ROW
) AS rolling_sum
FROM sales_data;
```

```sql
SELECT page_url, total_visits
FROM (
    SELECT page_url, SUM(visits) AS total_visits,
        ROW_NUMBER() OVER (ORDER BY SUM(visits) DESC) AS rownum
    FROM traffic_data
    WHERE event_time >= TIMESTAMPADD(HOUR, -24, CURRENT_TIMESTAMP)
    GROUP BY page_url
) t
WHERE rownum <= 10;
```

```sql
SELECT * FROM TABLE( TUMBLE(TABLE orders, DESCRIPTOR(order_time), INTERVAL '10' MINUTES));
```

```sql
SELECT * FROM TABLE(
    HOP(TABLE orders, DESCRIPTOR(order_time), INTERVAL '5' MINUTES, INTERVAL '10' MINUTES));
```

```sql
SELECT * FROM TABLE(
    CUMULATE(TABLE orders, DESCRIPTOR(order_time), INTERVAL '2' MINUTES, INTERVAL '10' MINUTES));
```

```sql
MATCH_RECOGNIZE
```

```sql
SELECT *
FROM StockPrice
MATCH_RECOGNIZE (
  PARTITION BY symbol
  ORDER BY event_time
  MEASURES
    FIRST(price) AS start_price,
    LAST(price) AS end_price,
    AVG(price) AS avg_price
  PATTERN (start up+ down+ end)
  DEFINE
    up AS price > PREV(price),
    down AS price < PREV(price)
)
```

```sql
MATCH_RECOGNIZE
```

```sql
PARTITION BY
```

---

### Part 1: How to Use Flink SQL, Streamlit, and Kafka
Source: https://www.confluent.io/blog/how-to-use-flinksql-streamlit-kafka-part-1/

Join us at Current New Orleans! Save $500 with early bird pricing until August 15 | Register NowLogin Contact UsMarket data analytics has always been a classic use case for Apache Kafka®. However, new technologies have been developed since Kafka was born.Apache Flink® has grown in popularity for stateful processing with low latency output. Streamlit, a popular open source component library and deployment platform, has emerged, providing a familiar Python framework for crafting powerful and interactive data visualizations. Acquired by Snowflake in 2022, Streamlit remains agnostic with respect to data sources.We can take advantage of the growth in the data landscape and use all three of these technologies to create a performant market data application. This article walks through how to use Streamlit, Kafka, and Flink to create a live data-driven user interface.OverviewIn part 1 of this series, we’ll make an app, hosted on Streamlit, that allows a user to select a stock, in this case SPY, or the SPDR S&P 500 ETF Trust. Upon selection, a live chart of the stock’s bid prices, calculated every five seconds, will appear.What are the pieces that go into making this work? The source of the data is the Alpaca Market Data API. We’ll hook up a Kafka producer to the websocket stream and send data to a Kafka topic in Confluent Cloud. Then we’ll use Flink SQL within Confluent Cloud’s Flink SQL workspace to tumble an average bid price every five seconds. Finally, we’ll use a Kafka consumer to receive that data and populate it to a Streamlit component in real time. This frontend component will be deployed on Streamlit as well.Data sourceWe’ll use the market data websocket endpoint. There are ways to use REST APIs with Kafka—if you’re interested in that, give this demo a whirl. But we’d like our data transfer to be as instantaneous as possible, with the sub-second latency we’re used to with Kafka, so we don’t have time for REST API request and response cycles.To see the data coming in from the websocket yourself, use websocat:websocat wss://stream.data.alpaca.markets/v2/test \ -H="APCA-API-KEY-ID:HERE" -H="APCA-API-SECRET-KEY: HERE"To subscribe to that endpoint, we call a subscribe function from the Alpaca API. This function includes a callback specifying a partial function, fn,because we need to pass the stockname to the handler: fn = partial(quote_data_handler, stockname) wss_client.subscribe_quotes(fn, stockname)This in turn specifies the quote_data_handler function. This is where the data from the websocket will flow.async def quote_data_handler(stockname, data): print("quote handler called") print(data)Getting the data into KafkaOk, we’ve got the stock market records coming in from the websocket endpoint. Now, we have to produce them to a Kafka topic (which we’ve already set up in Confluent Cloud). We’ll instantiate the producer and set up a JSON serializer (using the Apache Kafka Python client), then feed it the topic name, which we’ll set up to be the same as the stockname.producer = Producer(client_config) srconfig = { "url": st.secrets["SR_URL"], "basic.auth.user.info": st.secrets["BASIC_AUTH_USER_INFO"], } schema_registry_client = SchemaRegistryClient(srconfig) json_serializer = JSONSerializer( schema_str, schema_registry_client, serialize_custom_data ) async def quote_data_handler(stockname, data): producer.produce( topic=stockname, key=stockname, value=json_serializer( data, SerializationContext(stockname, MessageField.VALUE) ), on_delivery=delivery_report, ) producer.flush()Now, when we check the Kafka topic in Confluent Cloud, we can see the messages coming in. They’re composed of three parts: bid_timestamp, price, and symbol.{ "bid_timestamp": "2024-04-01 15:56:48.427360+00:00", "price": 521, "symbol": "SPY" }Getting the data into FlinkNow that we have our data flowing into a Kafka topic, we need to process it. We want tumbling windows, five seconds apart. To achieve this, we’ll crack our knuckles and open up a Flink SQL workspace on Confluent Cloud.Wait, what’s a Flink SQL workspace? Further, what’s Flink SQL? Well, let’s take a step back and look at what Flink is. It’s a stream processing framework specifically designed for handling complex, stateful streaming workloads. On a high level, Flink uses checkpointing to create snapshots of state and stores those instead of the whole state history, which makes it highly efficient.There are three APIs of note here, each at a different level of abstraction, for interacting with Flink. As with most API groups, the higher level APIs offer a faster onboarding experience at the expense of more control. On the other hand, the lower level APIs require a higher level of learning to use properly but offer more granular access to the underlying technology. At the lowest level of abstraction is the Datastream API, which offers developers an expressive way to use the elements of data streaming like windows and joins. One level up is the Table API, which centers around Flink Tables and involves writing less code than the Datastream API. Flink SQL is at the highest level of abstraction. It allows you to use SQL as a declarative approach for implementing unified batch and stream workload processing.For this project, we’ll use Flink SQL with Confluent Cloud. We’ll use Flink by provisioning a compute pool representing the resources used to run our SQL statements. We can create these statements in the workspace provided in Confluent Cloud’s user interface. Now here’s a key thing to understand about Flink tables: they are not where data is stored. The data we’re processing is stored in a Kafka topic. That means we need schemas for the data we produce to Kafka topics to be processed with Flink.Here’s what a JSON schema could look like for our topic with records including a price, a bid_timestamp, and a symbol.schema_str = """{ "$id": "http://confluent.io/myURI.schema.json", "$schema": "http://json-schema.org/draft-07/schema#", "additionalProperties": false, "description": "JSON schema for stock price topic", "properties": { "bid_timestamp": { "description": "The string type is used for strings of text describing the timestamp of the bid.", "type": "string" }, "price": { "description": "JSON number type denoting the price of the stock.", "type": "number" }, "symbol": { "description": "The string type is used for strings of text describing the stock symbol sold.", "type": "string" } }, "title": "StockRecord", "type": "object" }"""And here, as highlighted above when we were talking about the producer, you can see how it’s added to the producer, registering the schema, passing it to the JSON serializer, and finally using it to serialize the produced message. schema_registry_client = SchemaRegistryClient(srconfig) json_serializer = JSONSerializer( schema_str, schema_registry_client, serialize_custom_data ) producer.produce( topic=stockname, key=stockname, value=json_serializer( data, SerializationContext(stockname, MessageField.VALUE) ), on_delivery=delivery_report, )Once that was done, we could create a table, and then process the data in a Kafka topic using windowing. Here’s the syntax. Let’s go through it line by line.[1] INSERT INTO tumble_interval_SPY [2] SELECT symbol, DATE_FORMAT(window_start,'yyyy-MM-dd hh:mm:ss.SSS'), DATE_FORMAT(window_end,'yyyy-MM-dd hh:mm:ss.SSS'), AVG(price) [3] FROM TABLE( TUMBLE(TABLE SPY, DESCRIPTOR($rowtime), INTERVAL '5' SECONDS)) [4] GROUP BY symbol, window_start, window_end;[1] Here, we’re inserting the result into the destination table.[2] Here, we select four values from the source table. symbol identifies the stock name. window_start is the start of the window and formats it (note that this will be in event time as gleaned from the app), as window_end is the end of that window. We’re formatting the date here because it will make it easier to display in the front end without having to massage the message as much.[3] This specifies the row table, the interval, and the watermarking strategy via DESCRIPTOR. $rowtime is the value of the Kafka record timestamp, provided by the technology behind Confluent Cloud.[4] We group the results by the symbol, window_start, and window_end columns.Getting the data back into a Kafka topicFlink tables are a description of how to view the data stored in Kafka. So really, we don’t have to ‘get the data back into’ a Kafka topic, as the topic is created to store the data once we make the table. The data processed by FlinkSQL is not stored in a FlinkSQL table.And the data, stored in a tumble_interval_SPY topic, is comprised of records that look like this after Flink processing:{ "window_start": "2024-04-01 03:57:05.000", "window_end": "2024-04-01 03:57:10.000", "price": 521 }That’s the information we need for our live chart! The price will be represented by the y-axis, and the difference between the window end and start provides the value for the x-axis.That means that we can consume data from our final destination, the Streamlit app, right away … or can we?The producer and consumer run on two different threads, and without the async.io library in use, we weren’t able to run them at the same time from the same Streamlit application.Now Streamlit itself is multithreaded, and in fact, this behavior caused us to run into a difficulty with the Alpaca rate limits, but we’ll talk about that, as well as the solution to running the Kafka producer and consumer in the Streamlit app, in our next installment on this topic. We’ll also tell you what we learned about handling multithreading from our colleague, Gilles Philippart.Beyond that, in part 2 we’ll complete our journey through the project by examining how we surface the data to Streamlit using a bit of data visualization.Where to go from herePart 2 of this seriesGitHub README: run the code from this demo yourself!Flink 101 course: learn the fundamentals of Apache FlinkKafka 101 course: get the basics of Apache KafkaDemos page: more demos like this one on varying topics within the Flink and Kafka universeConfluent Cloud signupApache Flink® 101: Complete guides, courses, and video tutorials →Learn more →Written ByLucia CerchieSenior Software EngineerLucia Cerchie is a Senior Software Engineer at Confluent. She believes in a human-centered developer experience and in the joy of learning. She blogs at her personal site, https://luciacerchie.dev/blog/, and here on Confluent, https://www.confluent.io/blog/author/lucia-cerchie/Apache Flink® 101: Complete guides, courses, and video tutorials →Learn more →Did you like this blog post? Share it nowSubscribe to the Confluent blogSubscribeHow to Use Flink SQL, Streamlit, and Kafka: Part 2Jun 10, 2024Part two in the series on using FlinkSQL, Kafka, and Streamlit dives into async.io, FlinkSQL syntax, and Streamlit barchart component structure.Lucia CerchieStream Processing Simplified: An Inside Look at Flink for Kafka UsersAug 15, 2023Learn why stream processing is such a critical component of the data streaming stack, why developers are choosing Apache Flink as their stream processing framework of choice, and how to use Flink with Kafka.Konstantin KnaufFeedback

#### Code Examples

```sql
websocat wss://stream.data.alpaca.markets/v2/test \
 -H="APCA-API-KEY-ID:HERE" -H="APCA-API-SECRET-KEY: HERE"
```

```sql
fn = partial(quote_data_handler, stockname)
wss_client.subscribe_quotes(fn, stockname)
```

```sql
quote_data_handler
```

```sql
async def quote_data_handler(stockname, data):
   print("quote handler called")
   print(data)
```

```sql
producer = Producer(client_config)
srconfig = {
       "url": st.secrets["SR_URL"],
       "basic.auth.user.info": st.secrets["BASIC_AUTH_USER_INFO"],
   }

schema_registry_client = SchemaRegistryClient(srconfig)

json_serializer = JSONSerializer(
       schema_str, schema_registry_client, serialize_custom_data
   )

async def quote_data_handler(stockname, data):
   producer.produce(
       topic=stockname,
       key=stockname,
       value=json_serializer(
           data, SerializationContext(stockname, MessageField.VALUE)
       ),
       on_delivery=delivery_report,
   )

   producer.flush()
```

```sql
bid_timestamp
```

```sql
{
  "bid_timestamp": "2024-04-01 15:56:48.427360+00:00",
  "price": 521,
  "symbol": "SPY"
}
```

```sql
bid_timestamp
```

```sql
schema_str = """{
 "$id": "http://confluent.io/myURI.schema.json",
 "$schema": "http://json-schema.org/draft-07/schema#",
 "additionalProperties": false,
 "description": "JSON schema for stock price topic",
 "properties": {
   "bid_timestamp": {
     "description": "The string type is used for strings of text describing the timestamp of the bid.",
     "type": "string"
   },
   "price": {
     "description": "JSON number type denoting the price of the stock.",
     "type": "number"
   },
   "symbol": {
     "description": "The string type is used for strings of text describing the stock symbol sold.",
     "type": "string"
   }
 },
 "title": "StockRecord",
 "type": "object"
}"""
```

```sql
schema_registry_client = SchemaRegistryClient(srconfig)

   json_serializer = JSONSerializer(
       schema_str, schema_registry_client, serialize_custom_data
   )
   producer.produce(
       topic=stockname,
       key=stockname,
       value=json_serializer(
           data, SerializationContext(stockname, MessageField.VALUE)
       ),
       on_delivery=delivery_report,
   )
```

```sql
[1] INSERT INTO tumble_interval_SPY
[2] SELECT symbol, DATE_FORMAT(window_start,'yyyy-MM-dd hh:mm:ss.SSS'), 
DATE_FORMAT(window_end,'yyyy-MM-dd hh:mm:ss.SSS'), AVG(price)
[3] FROM TABLE(
        TUMBLE(TABLE SPY, DESCRIPTOR($rowtime), INTERVAL '5' SECONDS))
[4] GROUP BY
    symbol,
    window_start,
    window_end;
```

```sql
window_start
```

```sql
window_start
```

```sql
window_end columns
```

```sql
tumble_interval_SPY
```

```sql
{
  "window_start": "2024-04-01 03:57:05.000",
  "window_end": "2024-04-01 03:57:10.000",
  "price": 521
}
```

---

### Part 2: How to Use Flink SQL, Streamlit, and Kafka
Source: https://www.confluent.io/blog/how-use-flinksql-streamlit-kafka-part-2/

Join us at Current New Orleans! Save $500 with early bird pricing until August 15 | Register NowLogin Contact UsIn part one of this series, we walked through how to use Streamlit, Apache Kafka®, and Apache Flink® to create a live data-driven user interface for a market data application to select a stock (e.g., SPY) and discussed the structure of the app at a high level. First, data with information on stock bid prices is moved via an Alpaca websocket, then, it’s produced to a Kafka topic in Confluent Cloud where it is also processed with Flink SQL. Now comes the tricky part: running the Kafka consumer and producer in the same application.async.ioWe will use async.io to help manage multiple threads in our Streamlit application. One of async.io’s functions is to allow developers to run multiple coroutines concurrently. A coroutine is a type of subroutine that can be entered, accessed, and resumed at many different points. A subroutine is a function or a block of code that you can call. This is what is needed to run multiple coroutines concurrently: one coroutine implements a Kafka producer, and the other implements a Kafka consumer. The producer needs to produce data to Confluent Cloud at the same time that the consumer reads from it. Before my co-worker, Gilles Philippart, introduced async.io to my application, I encountered an issue: when I ran a consumer, the producer wouldn’t work. This was the behavior of the single-threaded application. How does async.io help run multiple threads? By virtue of its event loop, or scheduler, async.io can help you run these threads. For example, say a developer writes a piece of Python code like the following:async def printHello: print(“Hello from within the event loop”)Because of the async declaration, the code block is a coroutine.I wrote two top-level coroutines: one named on_select in kafkaproducer.py, containing subscribe_quotes, which calls back to fn, which in turn calls back to quote_data_handler, which produces data:async def on_select(stockname): fn = partial(quote_data_handler, stockname) print(f"Subscribing to quote for {stockname}") wss_client.subscribe_quotes(fn, stockname) await wss_client._run_forever(),And the other named display_quotes in app.py, which starts the consumer:async def display_quotes(component): component.empty() price_history = [] window_history = [] topic_name = option topic = f"tumble_interval_{topic_name}" consumer.subscribe(topic) while True: try: msg = consumer.poll(0.1) await asyncio.sleep(0.5) print("Received message: {}".format(msg)) if msg is None: continue elif msg.error(): print("Consumer error: {}".format(msg.error())) with component: [CODE TRUNCATED FOR CLARITY] chart = ( alt.Chart(data) .mark_line() .encode( x="window_end", y=alt.Y( "price_in_USD", scale=alt.Scale(domain=[domain_start, domain_end]), ), ) .transform_window( rank="rank()", sort=[alt.SortField("window_end", order="descending")], ) .transform_filter((alt.datum.rank < 20)) ) st.altair_chart(chart, theme=None, use_container_width=True) except KeyboardInterrupt: print("Canceled by user.") consumer.close()Both of these coroutines are scheduled together, using the async.gather() method:async def main(): if isinstance(option, str): # ordering the coroutines await asyncio.gather(on_select(option), display_quotes(placeholder))As the event loop runs, it schedules the coroutines to operate concurrently. Note the order of the coroutines here: the order of their results will arrive in the same order as they are listed in the .gather method, although it is not guaranteed that they’ll be executed in this order. There’s another place in the code where async.io is used, right after polling the consumer: msg = consumer.poll(0.1) await asyncio.sleep(0.5)asyncio.sleep() is not the same as time.sleep(). While time.sleep() pauses an entire application, asyncio.sleep pauses only the coroutine while the rest of the application runs.You can try out .sleep() for yourself using the Python REPL:$ python3 -m asyncio asyncio REPL 3.10.13 (main, Jan 29 2024, 10:26:28) [Clang 14.0.0 (clang-1400.0.29.202)] on darwin >>> await asyncio.sleep(.5, result='five milliseconds have passed') 'five milliseconds have passed'Why is this needed after polling the consumer? consumer.poll() specifies the number of milliseconds the records spend waiting—by default, it sends them immediately, although it was set to .1 milliseconds. In an async.io program, multiple coroutines are executed by the event loop. asyncio.sleep() allows other coroutines to run while one coroutine is paused, facilitating cooperative multitasking.Creating a single partitionThere was another issue while running Flink SQL. The original query was written like this:CREATE TABLE tumble_interval_SPY (`symbol` STRING, `window_start` STRING,`window_end` STRING,`price` DOUBLE, PRIMARY KEY (`symbol`) NOT ENFORCED) WITH ('value.format' = 'json-registry');In using Flink SQL in Confluent Cloud to create tables, the corresponding topic here would be created with six partitions by default. Normally, you’d want to take advantage of multiple partitions, but this simplified demo only required one. You can create the Flink table with one partition like so (distribution into a single bucket ensures the Kafka topic has a single partition), and then just subscribe to the topic. You can use this same syntax to create a topic with more partitions for production use cases. CREATE TABLE tumble_interval_SPY (`symbol` STRING, `window_start` STRING,`window_end` STRING,`price` DOUBLE, PRIMARY KEY (`symbol`) NOT ENFORCED) DISTRIBUTED BY (symbol) INTO 1 BUCKETS WITH ('value.format' = 'json-registry');Creating the live diagramStreamlit has a large array of frontend components in its library. In order to use them, you must import Streamlit after installation:import streamlit as stAnd then use call methods to create the components. Here’s what I wrote to title my page:st.title("Stock Price Averages")In order to create a stock visualization, st.altair_chart was used, which displays a chart using the Altair library, a declarative visualization library for Python. The basis of the Altair chart is a pandas DataFrame, a tabular data structure from the pandas library with rows and columns. Here’s the code that declares the chart before it is passed to Altair: chart = ( alt.Chart(data) .mark_line() .encode( x="window_end", y=alt.Y( "price_in_USD", scale=alt.Scale(domain=[domain_start, domain_end]), ), ) .transform_window( rank="rank()", sort=[alt.SortField("window_end", order="descending")], ) .transform_filter((alt.datum.rank < 20)) )There are a few different things going on here. The .mark_line() method ensures that the end result will be a line chart. The .encode method sets the x and y axes. Here, the price is determined by the variables domain_start and domain_end, which is calculated as the largest and smallest price in the price history. Next, .transform_window introduces a sliding window (note: the window used in Flink SQL is tumbling, but visually, this is going to be sliding). This sorts and ranks the objects in descending order by window end. Last, .transform_filter makes sure there are only 20 items in the window at a time.Notes on deploying the websiteOnce all of this was running locally, I was ready to deploy my project to the outside world. You can view it here! Deploying this app was not a complex process, however, there were a couple gotchas in the development process. All I had to do was point Streamlit to my GitHub repository, but if I changed the name of the repository, or changed the visibility, the deployment no longer worked.I stored my keys in an environment variable. For deployment, I moved them. Locally, they are kept in a .streamlit/secrets.toml file, and in deployment, they’re stored in a settings/secrets file that I set up during the deployment process.Upon loading the webpage and selecting the stockname, the code for the app runs.This means that the websocket gets a new connection. I used the Alpaca API free account so my app wouldn’t work with more than one connection. That’s why I created the website as a simulation rather than a live application. I captured some data from the behavior of SPY in a Kafka topic, then assigned the consumer to start at a specific offset. The tricky partsWriting this demo was both a challenging and rewarding experience because it pushed my understanding of Flink SQL, Python, and data visualization. In reflecting on some of the trickiest parts, a few stand out. The first was my misunderstanding of Flink tables—thinking that they provide a description of how to view data stored in a Kafka topic but instead unlocked the basic functionality of the application. The second was using the async.io library. It was a challenge to figure out which pieces of code were fundamentally on different threads. I’m really glad I worked it out, though, because I’m certain I’ll continue to use the library in future Python apps that use Kafka. ConclusionThere’s no better way to learn technology than to get your hands dirty and build something. Streamlit is available to you whenever you want to quickly build and deploy a Confluent Cloud-powered data visualization application. You can also experiment with threads. If you want to learn data streaming by building more projects yourself, here are some recommended resources:GitHub README: see this project in its context and run it for yourself! The website where this simulation is deployedThe Confluent Developer Demos page: build more data streaming projects like this one Vega-Altair: data visualization with PythonTutorials: learn how to use Flink SQL for your use caseConfluent Cloud signupApache Flink® 101: Complete guides, courses, and video tutorials →Learn more →Written ByLucia CerchieSenior Software EngineerLucia Cerchie is a Senior Software Engineer at Confluent. She believes in a human-centered developer experience and in the joy of learning. She blogs at her personal site, https://luciacerchie.dev/blog/, and here on Confluent, https://www.confluent.io/blog/author/lucia-cerchie/Apache Flink® 101: Complete guides, courses, and video tutorials →Learn more →Did you like this blog post? Share it nowSubscribe to the Confluent blogSubscribeHow to Use Flink SQL, Streamlit, and Kafka: Part 1Jun 4, 2024In part 1 of this series, we’ll make an app, powered by Kafka and FlinkSQL in Confluent Cloud and visualized with Streamlit, that allows a user to select a stock, in this case SPY, or the SPDR S&P 500 ETF Trust. Upon selection, a live chart of the stock’s bid prices, calculated every five seconds...Lucia CerchieStream Processing Simplified: An Inside Look at Flink for Kafka UsersAug 15, 2023Learn why stream processing is such a critical component of the data streaming stack, why developers are choosing Apache Flink as their stream processing framework of choice, and how to use Flink with Kafka.Konstantin KnaufFeedback

#### Code Examples

```sql
async def printHello:
  print(“Hello from within the event loop”)
```

```sql
quote_data_handler
```

```sql
async def on_select(stockname):
   fn = partial(quote_data_handler, stockname)

   print(f"Subscribing to quote for {stockname}")

   wss_client.subscribe_quotes(fn, stockname)

   await wss_client._run_forever(),
```

```sql
display_quotes
```

```sql
async def display_quotes(component):
   component.empty()
   price_history = []
   window_history = []
   topic_name = option

   topic = f"tumble_interval_{topic_name}"
   consumer.subscribe(topic)

   while True:
       try:
           msg = consumer.poll(0.1)
           await asyncio.sleep(0.5)
           print("Received message: {}".format(msg))
           if msg is None:
               continue
           elif msg.error():
               print("Consumer error: {}".format(msg.error()))

           with component: 
[CODE TRUNCATED FOR CLARITY]
             chart = (
                   alt.Chart(data)
                   .mark_line()
                   .encode(
                       x="window_end",
                       y=alt.Y(
                           "price_in_USD",
                           scale=alt.Scale(domain=[domain_start, domain_end]),
                       ),
                   )
                   .transform_window(
                       rank="rank()",
                       sort=[alt.SortField("window_end", order="descending")],
                   )
                   .transform_filter((alt.datum.rank < 20))
               )

               st.altair_chart(chart, theme=None, use_container_width=True)

       except KeyboardInterrupt:
           print("Canceled by user.")
           consumer.close()
```

```sql
async.gather()
```

```sql
async def main():
   if isinstance(option, str):
       # ordering the coroutines
       await asyncio.gather(on_select(option), display_quotes(placeholder))
```

```sql
msg = consumer.poll(0.1)

           await asyncio.sleep(0.5)
```

```sql
asyncio.sleep()
```

```sql
time.sleep()
```

```sql
time.sleep()
```

```sql
asyncio.sleep
```

```sql
$ python3 -m asyncio
asyncio REPL 3.10.13 (main, Jan 29 2024, 10:26:28) [Clang 14.0.0 (clang-1400.0.29.202)] on darwin
>>> await asyncio.sleep(.5, result='five milliseconds have passed')
'five milliseconds have passed'
```

```sql
consumer.poll()
```

```sql
CREATE TABLE tumble_interval_SPY
(`symbol` STRING, `window_start` STRING,`window_end` STRING,`price` DOUBLE, PRIMARY KEY (`symbol`) NOT ENFORCED)
WITH ('value.format' = 'json-registry');
```

```sql
CREATE TABLE tumble_interval_SPY
(`symbol` STRING, `window_start` STRING,`window_end` STRING,`price` DOUBLE, PRIMARY KEY (`symbol`) NOT ENFORCED)
DISTRIBUTED BY (symbol) INTO 1 BUCKETS
WITH ('value.format' = 'json-registry');
```

```sql
import streamlit as st
```

```sql
st.title("Stock Price Averages")
```

```sql
chart = (
                   alt.Chart(data)
                   .mark_line()
                   .encode(
                       x="window_end",
                       y=alt.Y(
                           "price_in_USD",
                           scale=alt.Scale(domain=[domain_start, domain_end]),
                       ),
                   )
                   .transform_window(
                       rank="rank()",
                       sort=[alt.SortField("window_end", order="descending")],
                   )
                   .transform_filter((alt.datum.rank < 20))
               )
```

```sql
.mark_line()
```

```sql
domain_start
```

```sql
.transform_window
```

```sql
.transform_filter
```

```sql
.streamlit/secrets.toml
```

```sql
settings/secrets
```

---

### Apache Flink® SQL
Source: https://developer.confluent.io/courses/flink-sql/overview/

‹ Back to coursescourse: Apache Flink® SQLApache Flink® SQLApache Flink® SQL This is a course about Flink SQL, which is part of the Apache Flink project. Exploring what Flink SQL can do is a great way to get started with Apache Flink and stream processing. What you’ll learn in this course This course includes videos, readings, and hands-on exercises. Topics include: What Flink SQL is, and why you might use it Getting started with Confluent Cloud for Apache Flink (exercise) How streaming SQL uses watermarks Time and watermarks (exercise) Window aggregations with Table Valued Functions OVER windows Streaming analytics (exercise) Streaming joins The Flink SQL Runtime Additional modules are being developed, and will be added incrementally. Intended Audience Anyone who wants to learn about Apache Flink and Flink SQL. Prerequisites This course doesn't assume you know anything about Apache Flink or SQL. The exercises assume you have access to a computer on which you can install the Confluent CLI. Length Approximately 3 hours. Staff David Anderson (Course Author) David has been working as a data engineer since long before that job title was invented. He has worked on recommender systems, search engines, machine learning pipelines, and BI tools, and has been helping companies adopt stream processing and Apache Flink since 2016. David is an Apache Flink committer, and works at Confluent as a Software Practice Lead. LinkedInDo you have questions or comments? Join us in the #confluent-developer community Slack channel to engage in discussions with the creators of this content.PreviousNextWe've got promo codes for you! Use CONFLUENTDEV1 to skip credit card entry during signup, and FLINKSQL25 for $25 of additional usage credit.Modules: Start from lesson 1Total 14Apache Flink® SQLWhat is Flink SQL? Is it a database?15 minExercise: Get started with Confluent Cloud for Apache Flink30 minHow streaming SQL uses watermarks10 minExercise: Hands-on with watermarks30 minWindow aggregations6 minOVER aggregations5 minExercise: Streaming analytics30 minStreaming JOINs in Flink SQL20 minThe Flink SQL Runtime10 minPattern Recognition and Complex Event Processing with MATCH_RECOGNIZE19 minExercise: Match RecognizeChangelog processing and troubleshooting errors about change updates10 minExercise: Stream enrichment45 minRelated ResourcesDocsStream Processing and SQL with Confluent Cloud for Apache Flink®ExamplesFlink SQL ExamplesBe the first to get updates and new contentWe will only share developer content and updates, including notifications when new content is added. We will never send you sales emails. 🙂 By subscribing, you understand we will process your personal information in accordance with our Privacy Statement.Feedback

---

### Apache Flink - A Complete Introduction
Source: https://developer.confluent.io/courses/apache-flink/intro/

‹ Back to coursesView Transcriptcourse: Apache Flink® 101Introduction3 minApache Flink® 101 About This Course This course is an introduction to Apache Flink, focusing on its core concepts and architecture. Learn what makes Flink tick, and how it handles some common use cases. What is Flink? Today's consumers have come to expect timely and accurate information from the companies they do business with. Whether it's being alerted that someone just used your credit card to rent a car in Prague, or checking on the balance of your mobile data plan, it's not good enough to learn about yesterday's information today. We all expect the companies managing our data to provide fully up-to-the-moment reporting. Apache Flink is a battle-hardened stream processor widely used for demanding applications like these. Its performance and robustness are the result of a handful of core design principles, including a share-nothing architecture with local state, event-time processing, and state snapshots (for recovery). Through a combination of videos and hands-on exercises, this course brings these core principles to life. Common Flink Use Cases Common use cases include data analytics, fraud detection, billing, business process monitoring, rule-based alerting, etc. How Flink Works Flink is a powerful system with many components, but once you understand the fundamentals presented in this course, and how they fit together – streams, state, time, and snapshots – learning the details will become much easier. The hands-on exercises in this course use Flink SQL to illustrate and clarify how Flink works. The focus is on learning about Flink, using the SQL you already know. What You’ll Learn in This Course What Apache Flink is, and why you might use it What stream processing is, and how it differs from batch processing Flink’s runtime architecture How to use Flink and Kafka together How to use Flink SQL: tables, windows, event time, watermarks, and more Stateful stream processing How watermarks support event time operations How Flink uses snapshots (checkpoints) for fault tolerance Intended Audience Anyone who knows the basics of Kafka and SQL who wants to understand what Flink is and how it works. Prerequisites Required knowledge: This course assumes some basic familiarity with Kafka and SQL. If you understand what producers and consumers are, and can explain what GROUP BY does, that’s good enough. Required setup: A local Docker installation. Resources To learn more about Kafka, see Kafka 101. Building Flink Applications in Java is a companion course to this one, and a great way to learn more about the practical side of Flink application development. Length Approximately 2-3 hours Staff David Anderson (Course Author) David has been working as a data engineer since long before that job title was invented. He has worked on recommender systems, search engines, machine learning pipelines, and BI tools, and has been helping companies adopt stream processing and Apache Flink since 2016. David is an Apache Flink committer, and works at Confluent as a Software Practice Lead. LinkedInDo you have questions or comments? Join us in the #confluent-developer community Slack channel to engage in discussions with the creators of this content.PreviousNextUse the promo codes FLINK101 & CONFLUENTDEV1 to get $25 of free Confluent Cloud usage and skip credit card entry.Get StartedModules: Start from lesson 1Total 15Introduction3 minIntro to Stream Processing with Apache Flink8 minIntro to Flink SQL5 minBatch and Stream Processing with Flink SQL (Exercise)The Flink Runtime5 minUsing the Flink Web UI (Exercise)Using Kafka with Flink5 minDeploying an ETL Pipeline using Flink SQL (Exercise)Stateful Stream Processing with Flink SQL5 minStreaming Analytics with Flink SQL (Exercise)Event Time and Watermarks12 minImplementing and Troubleshooting Watermarks (Exercise)Checkpoints and Recovery5 minExperiencing Failure Recovery (Exercise)Conclusion4 minRelated ResourcesDocsStream Processing and SQL with Confluent Cloud for Apache Flink®DocsApache Flink DocumentationCourseBuilding Apache Flink Applications in JavaIntroductionHi, I'm David Anderson with Confluent, here to tell you all about Apache Flink. We'll look together at why Flink is interesting, and how you can use it to build real-time data products. Along the way, I'll explain the big ideas on which Flink is based, and show you around under the hood so you'll understand how Flink works. Apache Flink is a battle-hardened stream processor widely used for demanding real-time applications. Its performance and robustness are the result of a handful of core design principles, including a share-nothing architecture featuring local state, event-time processing, and state snapshots for recovery. Through a combination of videos and hands-on exercises, this course brings these core principles to life. The focus of this course is going to be on the 4 Big Ideas that form the foundation for Apache Flink, which are streaming, state, time, and the use of state snapshots for fault tolerance and failure recovery. Understanding how Flink's runtime is organized around these four concepts, and how they are interrelated, is the key that unlocks Apache Flink. In the first four sections, we'll look together at streams and stream processing, and do so from several different perspectives. In the second half, we will look at each of the other three big ideas. By the end of the course you'll know enough to be able to implement some common use cases, and you'll understand what's going on inside of Flink when it is running your applications. Most of the modules in this course have a hands-on exercise that reinforces and expands upon the information in these videos. You'll find these exercises and other materials for this course on developer.confluent.io. These hands-on exercises all use Flink SQL. The focus will always be on learning about the concepts and architecture of Flink, while taking advantage of the SQL that you already know. And don't worry, you're not expected to be a SQL expert for this course. We're not going to do anything more complicated than aggregating data with GROUP BY. In several of the hands-on exercises, you will be using Flink SQL together with Apache Kafka to produce and consume data on Confluent Cloud. If you haven't already signed up for Confluent Cloud, sign up now so when you need it for the exercises, you'll be ready. And be sure to use the promo code in the description: it provides enough free credit to do all of the exercises for this course.Be the first to get updates and new contentWe will only share developer content and updates, including notifications when new content is added. We will never send you sales emails. 🙂 By subscribing, you understand we will process your personal information in accordance with our Privacy Statement.Feedback

---

### Apache Flink with Java - An Introduction
Source: https://developer.confluent.io/courses/flink-java/overview/

‹ Back to coursesView Transcriptcourse: Building Apache Flink® Applications in JavaApache Flink with Java - An Introduction2 minBuilding Apache Flink Applications in Java About This Course Peel away the surface of a modern system and you’ll often find a mountain of data being processed. It wasn’t always this way. Not long ago applications were smaller and the data tended to be static. Processing was performed on demand whenever a query was made. If the application required upfront computation, it was done with a batch job running against a relatively small data set. Today, the data sets have grown to staggering sizes. They are too large for a simple batch job to handle. Meanwhile, users are no longer content to wait hours or even minutes for a batch job to process their data. They want results now. As a result, developers are increasingly turning to distributed streaming solutions to process data in real time. Apache Flink is a powerful engine built for processing streaming data flows in a distributed environment. Rather than accumulating data into batches to be processed later, Apache Flink allows us to process the data as it happens, applying stateful transformations along the way. This makes it an invaluable tool for today’s streaming needs. This course will introduce students to Apache Flink through a series of hands-on exercises. Students will build a basic application in Java that will consume a collection of Apache Kafka data streams. The data will be transformed using Flink and pushed back into new Kafka topics. This simple use case will give students many of the tools they need to start building production-grade Apache Flink applications. Intended Audience You are an experienced Java developer who is new to Apache Flink. You are curious about real-time data streaming systems. Course Outline Overview Datastream Programming Setup your Flink environment (Exercise) The Flink Job Lifecycle Running a Flink Job (Exercise) Anatomy of a Stream Flink Data Sources Creating a Flink Data Source (Exercise) Serializers & Deserializers Deserializing Messages in Flink (Exercise) Transforming Data in Flink Flink Data Transformations (Exercise) Flink Data Sinks Creating a Flink Data Sink (Exercise) Closing Remarks Prerequisites You will need a decent knowledge of the Java programming language (Java 11), including classes, functions, lambdas. You will need a Java development environment. You can use your local development environment if you have one. A Gitpod configuration will be supplied to create a temporary environment. You will need a Confluent Cloud account. If you don't already have one, you will be instructed to create it early in the course. Length 2-3 hours Staff Wade Waldron (Course Author) Wade has been a Software Developer since 2005. He has worked on video games, backend microservices, ETL Pipelines, IoT systems, and more. He is an advocate for Test-Driven Development, Domain-Driven Design, Microservice Architecture, and Event-Driven Systems. Today, Wade works as a Staff Software Practice Lead at Confluent, showing people how to build modern data streaming applications. LinkedInDo you have questions or comments? Join us in the #confluent-developer community Slack channel to engage in discussions with the creators of this content.PreviousNextUse the promo codes FLINKJAVA101 & CONFLUENTDEV1 to get $25 of free Confluent Cloud usage and skip credit card entry.Get StartedGet started with Confluent CloudTry freeModules: Start from lesson 1Total 21Apache Flink with Java - An Introduction2 minDatastream Programming4 minHow to Start Flink and Get Setup (Exercise)The Flink Job Lifecycle6 minRunning a Flink Job (Exercise)Anatomy of a Stream4 minFlink Data Sources5 minCreating a Flink Data Source (Exercise)Serializers & Deserializers5 minDeserializing Messages in Flink (Exercise)Transforming Data in Flink6 minFlink Data Transformations (Exercise)Flink Data Sinks4 minCreating a Flink Data Sink (Exercise)Creating Branching Data Streams in Flink7 minMerging Flink Data Streams (Exercise)Windowing and Watermarks in Flink7 minAggregating Flink Data using Windowing (Exercise)Working with Keyed State in Flink5 minManaging State in Flink (Exercise)Closing Remarks2 minRelated ResourcesCourseApache Flink 101DocsIntro to the DataStream APIApache Flink with Java - An IntroductionHi, I'm Wade from Confluent. In this course, I'm going to show you how to create a series of small Flink Jobs using Java. Flink can be a very powerful tool for helping you build real-time data streams. But, if you aren't sure where to begin, it can seem scary at first. This course will take some of the guesswork out of the process by walking you through building Flink applications from the ground up. We'll be taking an iterative approach so that each step feels small and manageable. By the end, you will have three fully functioning Flink jobs that cover many of the basics you will need. Throughout this course, we'll be teaching you how to: Run a job Consume data from a Kafka topic Serialize and deserialize Java objects, or POJOs as they are often called. Transform and filter data using a variety of operators. Produce data to a Kafka topic. Combine multiple incoming data sources Aggregate data streams using time windows And manage stateful operations. The course will introduce you to the Flink Data Stream API in Java through a series of hands-on exercises. These exercises will allow you to produce and consume data through Confluent Cloud using Flink. If you haven't already signed up for Confluent Cloud, sign up now so when your first exercise asks you to log in, you're ready. Be sure to use the promo code when signing up to get the free usage it provides. If you aren't already on Confluent Developer, head there now using the link in the video description to access the rest of this course and its hands-on exercises.Be the first to get updates and new contentWe will only share developer content and updates, including notifications when new content is added. We will never send you sales emails. 🙂 By subscribing, you understand we will process your personal information in accordance with our Privacy Statement.Feedback

---

### Exercise: Connecting the Flink Table API to Confluent Cloud
Source: https://developer.confluent.io/courses/flink-table-api-java/exercise-connecting-to-confluent-cloud/

‹ Back to coursescourse: Apache Flink® Table API: Processing Data Streams In JavaExercise: Connecting the Flink Table API to Confluent Cloud30 minConnecting the Apache Flink Table API to Confluent Cloud In this exercise, we will set up Confluent Cloud and establish a connection from a Flink application to the cluster. In future exercises, we'll expand on this framework to produce queries focused on an online marketplace. Download the code These exercises use the code provided by the following Github repo. To start, clone the repo. Review the README.md file Inside the cloned repository, locate the README and review it before continuing. Setup your development environment Dev Container If you are using the Dev Container, you can skip this step. You will need a suitable Java development environment including: Java 21 Maven An IDE such as IntelliJ, Eclipse, or VS Code. To easily switch between Java versions, you can use SDKMAN. NOTE This project already has many of the settings required to work with the table API. For details on how to setup your own project, check out the Confluent Flink Table API Documentation. Create a Confluent Cloud account and log in NOTE If you already have a Confluent Cloud account, you can skip this step. Go to the Confluent Cloud signup page and create a new account. Watch your inbox for a confirmation email and follow the link to proceed. You will be asked to create a cluster. Feel free to ignore this. We'll create one shortly. Install the Confluent CLI Dev Container If you are using the Dev Container, you can skip this step. The easiest way to create an environment for this course is using the Confluent CLI. If you use Homebrew, the CLI can be installed using the following command: brew install confluentinc/tap/cli Otherwise, review the following instructions to install the CLI on your machine. Install the Confluent CLI Install the Flink quickstart plugin Dev Container If you are using the Dev Container, you can skip this step. The confluent-flink-quickstart plugin will create the required resources for the course. Install the plugin using the following command: confluent plugin install confluent-flink-quickstart Log in with the CLI You will need to log in to your Confluent Cloud account with the CLI. You can do that using the following command: confluent login If you are working in the Dev Container, it may not be able to open a browser to do the login. In that case, you can use the following command instead: confluent login --no-browser Create an environment For this course, you need the following resources: A Confluent Cloud Environment named flink-table-api-java A Kafka Cluster named marketplace A Flink Compute Pool A Flink API Key These resources can all be created by running the Confluent CLI command below. NOTE Feel free to alter the cloud and region to something more appropriate for your location. However, make sure that if you change it, you use the same settings throughout the course. confluent flink quickstart \ --name flink-table-api-java \ --environment-name flink-table-api-java \ --kafka-cluster-name marketplace \ --max-cfu 10 \ --region us-central1 \ --cloud gcp \ --table-api-client-config-file ./cloud.properties When you execute the command it will generate a cloud.properties file with the necessary parameters to connect your application to the cluster. Hold on to that file. Once you have finished creating your environment, you might want to take a moment to explore it. In Confluent Cloud you should be able to see your environment named flink-table-api-java. It should contain a Kafka cluster named marketplace and a Flink compute pool named flink-table-api-java. Stage the exercise Stage the exercise by executing: $ cd exercises $ ./exercise.sh stage 01 Import the project Dev Container If you are using the Dev Container, you can skip this step. Import the project (Maven POM file) from the exercises folder into your IDE. Copy configuration settings Copy the cloud.properties file you created above into src/main/resources/cloud.properties. A cloud-template.properties file has been provided for reference. Build the application Confluent Cloud includes a read-only set of Flink tables in a sandbox-link environment. These tables can be used for experimentation and testing. For a simple test of the connection parameters, we can ask the Table API to list those tables. In the src/main/java/marketplace/Marketplace class, implement the main method as follows: Use the ConfluentSettings class to load the configuration from the cloud.properties file: EnvironmentSettings settings = ConfluentSettings.fromResource("/YOUR.PROPERTIES.FILE"); HINT You must prefix your properties file with the / as shown above. Create a new table environment using the settings: TableEnvironment env = TableEnvironment.create(settings); Set the catalog to examples and the database to marketplace. env.useCatalog(<Catalog Name>); env.useDatabase(<Database Name>); Use env.listTables() to produce a list of tables in the database and print the results. Run the application Finally, we'll run the application to verify it works as expected. In a terminal, execute the application by running the commands: mvn clean package java -jar target/flink-table-api-marketplace-0.1.jar Assuming you have done everything correctly you should see the following tables printed: clicks customers orders products Finish This brings us to the end of this exercise.Do you have questions or comments? Join us in the #confluent-developer community Slack channel to engage in discussions with the creators of this content.PreviousNextUse the promo codes FLINKTABLEAPIJAVA & CONFLUENTDEV1 to get $25 of free Confluent Cloud usage and skip credit card entry.Get started with Confluent CloudTry freeModules: Start from lesson 1Total 17Overview2 minWhat is the Apache Flink® Table API?8 minUsing Apache Flink® in Confluent Cloud6 minExercise: Connecting the Flink Table API to Confluent Cloud30 minQuerying Apache Flink® Tables with the Select Statement6 minFiltering Data with the Apache Flink® Table API7 minExercise: Querying Flink Tables with the Table API30 minCreating Apache Flink® Tables with the Table API7 minWriting to a Table with the Apache Flink® Table API6 minExercise: Building a Data Streaming Pipeline with the Flink Table API20 minAutomatic Watermarking in Confluent Cloud8 minAggregating Data with the Apache Flink® Table API7 minUsing Windows to Aggregate Data with the Apache Flink® Table API9 minExercise: Aggregating Data Over Time with the Flink Table API20 minJoining Flink Tables using the Apache Flink® Table API6 minExercise: Combining Data from Multiple Streams with the Flink Table API20 minClosing Remarks1 minRelated ResourcesDocsTable API on Confluent Cloud for Apache Flink | Confluent DocumentationDocsStream Processing and SQL with Confluent Cloud for Apache Flink®Be the first to get updates and new contentWe will only share developer content and updates, including notifications when new content is added. We will never send you sales emails. 🙂 By subscribing, you understand we will process your personal information in accordance with our Privacy Statement.Feedback

#### Code Examples

```sql
brew install confluentinc/tap/cli
```

```sql
confluent plugin install confluent-flink-quickstart
```

```sql
confluent login
```

```sql
confluent login --no-browser
```

```sql
confluent flink quickstart \
    --name flink-table-api-java \
    --environment-name flink-table-api-java \
    --kafka-cluster-name marketplace \
    --max-cfu 10 \
    --region us-central1 \
    --cloud gcp \
    --table-api-client-config-file ./cloud.properties
```

```sql
$ cd exercises
$ ./exercise.sh stage 01
```

```sql
EnvironmentSettings settings = ConfluentSettings.fromResource("/YOUR.PROPERTIES.FILE");
```

```sql
TableEnvironment env = TableEnvironment.create(settings);
```

```sql
env.useCatalog(<Catalog Name>);
env.useDatabase(<Database Name>);
```

```sql
mvn clean package
java -jar target/flink-table-api-marketplace-0.1.jar
```

---

### Apache Flink: Stream Processing and Analytics | Confluent
Source: https://www.confluent.io/product/flink/

Join us at Current New Orleans! Save $500 with early bird pricing until August 15 | Register NowLogin Contact UsSimple, Serverless Stream ProcessingEasily build high-quality, reusable data streams with the industry’s only serverless Apache Flink® service, fully integrated with Apache Kafka® on Confluent Cloud across all three major clouds. Confluent Platform for Apache Flink® is also now generally available for on-prem and private cloud workloads – see details here.Start for FreeWatch VideoReal-time Use Cases Need Real-time ProcessingEnable real-time data handling by transforming, analyzing and acting on data as it flows across your enterprise: Shift Left Data Pipelines: Clean and shape data as it's created, delivering fresh, reliable and reusable data to your data warehouse or data lake. Real-Time Materialized Views: Keep downstream systems and applications in sync with incrementally updated real-time universal data products. Classical Event Processing: Capture and process events as they occur, aggregating and analyzing them over time to detect trends, patterns and anomalies. Learn MoreAccelerate Real-Time AI PipelinesUnify data processing and AI workflows by working directly with models and vector databases in Flink to enable accurate, real-time AI-driven decisions based on the most relevant streaming data.AI Model InferenceSeamlessly integrate LLMs into your stream processing jobs using familiar SQL syntaxReal-time EmbeddingsGenerate vector embeddings for RAG in real time, from any model to any vector database, across any cloud platformBuilt-in ML FunctionsSimplify complex data science tasks into Flink SQL with out-of-the-box anomaly detection and forecastingFilter. Join. Enrich.Effortlessly filter, join, and enrich your data streams with Flink, the de facto standard for stream processing.Real-time ProcessingPower low-latency applications and pipelines that react to real-time events and provide timely insightsData ReusabilityShare consistent and reusable data streams widely with downstream applications and systemsData EnrichmentCurate, filter, and augment data on-the-fly with additional context to improve completeness, accuracy, and complianceEfficiencyImprove resource utilization and cost-effectiveness by avoiding redundant processing across silosCloud-native ExperienceEnable high-performance and efficient stream processing at any scale, without the complexities of infrastructure management.Fully ManagedEasily develop Flink applications with a serverless, SaaS-based experience instantly available and without ops burdenElastic ScalabilityAutomatically scale up or down to meet the demands of the most complex workloads without overprovisioningInfrastructure-as-codeAutomate Flink deployment on Kubernetes for private cloud and on-prem workloadsContinuous UpdatesBuild using always up-to-date platform with automated upgradesKafka + Flink, UnifiedExperience Kafka and Flink, unified as a complete enterprise-grade data streaming platform.Enterprise-grade SecuritySecure stream processing with built-in identity and access management, RBAC, and detailed audit logsStream GovernanceDefine and enforce universal data standards that enable scalable data compatibilityMonitoringEnsure Flink applications run reliably, efficiently, and are easy to maintainConnectorsConnect to and from any app and system with 120+ pre-built connectors“Stream processing is critical for identifying and protecting against security risks in real time. With Confluent’s fully managed Flink offering, we can access, aggregate, and enrich data from IoT sensors, smart cameras, and Wi-Fi analytics, to swiftly take action on potential threats in real time, such as intrusion detection. This enables us to process sensor data as soon as the events occur, allowing for faster detection and response to security incidents without any added operational burden.”Vinay Krishna PatnanaEngineering Manager at Cisco Meraki"To meet rising customer demands in a volatile energy market, we need to deliver near real-time data to our client-facing applications. Relying on batch processing can cause performance issues and result in poor decision-making based on outdated data. By using Kafka and Flink together in a unified platform, our teams will be able to easily build intelligent streaming data pipelines that can extract data from various sources, process it in real time, and feed it to our downstream consumers for timely analysis without any operational challenges. We’re excited about Confluent’s fully managed Flink service, because it will help make stream processing accessible to everyone by creating high quality, reusable data streams to fuel innovation and data exploration across our lines of business.”Sami AlAshabiSolutions Architect at Essent"Conditions in the automotive logistics industry can change rapidly, requiring immediate action to address delays, reroute vehicles, and update systems and customers. Confluent's serverless Flink service will enable us to instantly and efficiently transform, integrate, and enrich massive volumes of data in our transportation management system, providing real-time visibility into the status and location of vehicles for both systems and customers."Jeffrey JenningsSr. Consultant, Data and Integration Services at ACERTUS"Apache Flink is becoming a prominent stream processing framework in this shift towards real-time insights. Flink and Apache Kafka are commonly used together for real-time data processing, but differing data formats and inconsistent schemas can cause integration challenges and hinder the quality of streaming data for downstream systems and consumers. A fully managed, unified Kafka and Flink platform with integrated monitoring, security, and governance capabilities can provide organizations with a seamless and efficient way to ensure high-quality and consistent data streams to fuel real-time applications and use cases, while reducing operational burdens and costs."Stewart BondResearch VP, Data Integration and Data Intelligence Software at IDC"Stream processing involves real time integration and transformation of streams of event data, powering systems across industries ranging from fraud detection in finance to personalized recommendations in e-commerce. When used in combination, Apache Flink and Apache Kafka can enable data reusability and avoid redundant downstream processing. The delivery of Flink and Kafka as fully managed services delivers stream processing without the complexities of infrastructure management, enabling teams to focus on building real-time streaming applications and pipelines that differentiate the business."Matt AslettVice President and Research DirectorResourcesGetting started with FlinkFlink DatasheetConfluent Cloud: Flink Launch BlogConfluent Platform: Flink Launch BlogCourse: Table APICourse: Flink SQLNEW Comic Book: The Force of Kafka + Flink Awakens

---

